{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "PyCharm (VHB-ProDok-Internal)",
      "language": "python",
      "name": "pycharm-e3448bea"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "4_ex_data_prep.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmIQkT_sZTzN"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/exercises/4_ex_data_prep.ipynb) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf5bQw9lZTzN"
      },
      "source": [
        "# BADS Exercise 4 on data preparation\n",
        "This exercise revisits some of the concepts covered in [Tutorial 4 on data preparation](https://github.com/Humboldt-WI/bads/blob/master/tutorials/4_nb_data_preparation.ipynb). That tutorial was rather comprehensive and provided a lot of materials and codes associated with typical tasks in the wide scope of data preparation. Therefore, the exercises will not go beyond [Tutorial 4](https://github.com/Humboldt-WI/bads/blob/master/tutorials/4_nb_data_preparation.ipynb). Rather, we will consider a different data set and repeat some standard data prep. tasks for that data set.   \n",
        "\n",
        "## 1 Loading the data \n",
        "The data set for this tutorial comes from the classic Book Credit Scoring and its Applications by Lyn C. Thomas, David B. Edelman, and Jonathan N. Crook. You can obtain the data, called *loan_data* from our [GitHub repository](https://github.com/Humboldt-WI/bads/tree/master/data). The data folder of the repository also provides a file *loan_data_dictionary*, which offers a brief description of the features in this data set. In a nutshell, the data represents yet another vanilla credit scoring task with a binary target variable, indicating whether bank customers repaid their debt or defaulted, and a few features characterizing credit applicants. Your first task is to load the data into a `Pandas DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54Ud3XtqZTzN"
      },
      "source": [
        "# Load the data (either from disk or directly from the web)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlHauiUhZTzO"
      },
      "source": [
        "By now, you have run through the process of getting a first idea about a new data set many times. You have seen examples in previous tutorials and have written your own codes in, e.g., the third exercise on predictive analytics. Nonetheless, draw once more on your experience and take a quick look into the data.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSvtKE1hZTzO"
      },
      "source": [
        "# Some space for any code you want to write to take a first look\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86a0L4iMZTzO"
      },
      "source": [
        "## 2 Data types\n",
        "You can tell from the data dictionary that the loan data includes numeric and categorical variables. Draw on the examples from [Tutorial 4](https://github.com/Humboldt-WI/bads/blob/master/tutorials/4_nb_data_preparation.ipynb) and make sure that all numeric features are stored as `float32` and all categorical features are stored as categories in your DataFame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reOczUdlZTzO"
      },
      "source": [
        "# Conversion of data types where needed\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBHdKZhVZTzO"
      },
      "source": [
        "## EDA\n",
        "### 3.1 Histogram\n",
        "The data includes a feature dINC_A, which captures the income of a credit applicant. We would expect that this feature is related to our target variable, which is called BAD in the data set. \n",
        "\n",
        "Create a histogram plot of the income feature and examine its distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOrcl7obZTzO"
      },
      "source": [
        "# Histogram of dINC_A \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA2E41Q-ZTzO"
      },
      "source": [
        "The distribution reveals some potentially important insights. However, the histogram alone does not suffice to check our intuition that income and credit risk are related. To that end, let's examine the income distribution across good and bad credit applications.\n",
        "\n",
        "### 3.2 Analysis of the dependency between applicants' income and credit risk\n",
        "We begin with a manual approach, which also allows us to revisit logical indexing in Python and Pandas. Calculate the average income of a credit applicant for good risks and for bad risks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozLSHlVOZTzO"
      },
      "source": [
        "# Calculate the group-wise mean of dINC_A for good and bad risks using logical indexing\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BGB_zEbZTzO"
      },
      "source": [
        "Remember that the Pandas function `groupby()` allows you to perform an analysis similar to your above calculation of the group-wise means. Replicate the previous calculation using `groupby()`.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw0DEqzbZTzO"
      },
      "source": [
        "# Function groupby()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX1sCedMZTzO"
      },
      "source": [
        "Next, we perform a graphical analysis. Depict the distribution of the income of customers with a good and bad risk, respectively, by means of a box-plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0InCH1Z_ZTzP"
      },
      "source": [
        "# Box plot\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqci-rUtZTzP"
      },
      "source": [
        "### 3.3 Statistical testing\n",
        "Identify an appropriate statistical test to verify whether the observed income difference between good and bad applicants is statistically significant. Perform the test and display its results. Hint: A web-search similar to “statistical test difference in means python” will help."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-tpjC8SZTzP"
      },
      "source": [
        "# Statistical testing of mean differences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYi1_apnZTzP"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoStoQjjZTzP"
      },
      "source": [
        "### 3.4 Categorical variables\n",
        "The data set comprises three categorical features. The feature PHON is binary and will not cause any issues. The features EMPS_A and RES are more interesting. Remember to check the data dictionary to understand what information the features encode. \n",
        "\n",
        "In the lecture, we explained that categorical features are typically encoded using dummy variables prior to applying an analytical model. Python supports dummy coding in several ways.  Pandas offers a function `get_dummies()` and sklearn offer a class `OneHotEncoder()`. The Pandas approach is maybe a bit easier to use. The more prevalent approach in practice is to rely on sklearn. \n",
        "\n",
        "Check the documentation of one or both of the above functions. Then create dummy variables for the feature RES and add them to your DataFrame.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGRN72W0ZTzP"
      },
      "source": [
        "# Dummy coding of RES\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO38EkoKZTzP"
      },
      "source": [
        "The feature EMPS_A has more distinct levels. Considering the previous task, it is obvious that dummy coding the feature will increase dimensionality substantially. To avoid this, it makes sense to **regroup** the category levels prior to dummy coding. \n",
        "\n",
        "In the lecture on data preparation, we argued that a pivot table helps to identify category levels that we can merge. Specifically, we were recommending merging category levels for which the odds-ratio (i.e., the ratio of goods to bads) is similar. Write code to calculate the odds ratio for each level of the feature EMPS_A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agPvI8ejZTzP"
      },
      "source": [
        "# Calculation of the odds-ratio for EMPS_A\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93iA4QJkZTzP"
      },
      "source": [
        "Now merge some category levels based on your solution to the previous task.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeKKvupOZTzP"
      },
      "source": [
        "# Merge category levels from EMPS_A\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR6iSiTPZTzP"
      },
      "source": [
        "The advantage of merging category levels is that we need less dummy variables for encoding the feature. On the other hand, by reducing category levels, we run the risk of losing information. It would make sense to check that our previous merging of category levels did not hurt, e.g., was not too aggressive. Why aggressive? Well, imagine you merge all category levels into one level. This would render the feature useless. So there is a trade-off between having few levels, to not increase dimensionality, and not having too few levels, to sustain the information in the feature for distinguishing good and bad applicants. To find a healthy balance between these conflicting objectives, we need a measure that tells us whether a grouping is informative. It turns out that a well-known statistical test, namely the $\\chi^2$ test, provides this functionality. \n",
        "- Run a quick web search to revisit the $\\chi^2$ test and understand how it is useful for judging a grouping of EMPS_A in our context.\n",
        "- Identify a way to calculate the $\\chi^2$ test statistic in Python\n",
        "- Calculate the test statistic for the original version of EMPS_A with 11 levels and the new version with less levels (i.e., solution to previous task)\n",
        "- Interpret the results and conclude whether your encoding of EMPS_A is suitable\n",
        "\n",
        "*HINT:* When merging categories, you should expect $\\chi^2$ test statistic to decrease unless the merged categories have exactly the same odds ratio. When deciding how many (and which) categories to merge, we should balance two conflicting objectives and try reducing the number of categories such that not too much information is lost. When fixing the number of categories, you can use the $\\chi^2$ test to compare competing encodings to select the one that preserves the most information. Play around with different ways to merge categories and try to select the one that achieves a good balance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1zQGSStZTzP"
      },
      "source": [
        "# Chi^2 testing\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDLUEmYaZTzP"
      },
      "source": [
        "# Well done. You did great in solving all the exercises!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqxmKIBHZTzP"
      },
      "source": [
        "**Optional**\n",
        "By solving the previous task, you have created a rather powerful mechanism to regroup categorical variables and judge the predictive power of the encoding prior to applying dummy coding. Write a function that wraps-up this functionality. In particular, your function should:\n",
        "- receive a categorical variable as input\n",
        "- check that the variable is actually a category\n",
        "- determine the number of unique levels\n",
        "- iteratively reduce the number of levels by:\n",
        "  - calculating the odds-ratio of all current levels\n",
        "  - merging the two levels whose odds ratio is most similar\n",
        "  - calculating the $\\chi^2$ statistic for the current grouping and store its value\n",
        "- plot the elbow curve for the $\\chi^2$ statistic against the number of levels\n",
        "- display the optimal encoding for each number of levels\n",
        "\n",
        "*HINT:* recall the elbow curve showing relationship between the distance and the number of clusters that we plotted in [Tutorial 2](https://github.com/Humboldt-WI/bads/blob/master/tutorials/2_nb_descriptive_analytics.ipynb). Your function should output a similar graph.\n",
        "\n",
        "Finally, check your implementation by applying the function to EMPS_A and interpreting the results.\n",
        "\n",
        "Note that the described procedure mimics the logic behind the [CHAID tree](https://towardsdatascience.com/clearly-explained-top-2-types-of-decision-trees-chaid-cart-8695e441e73e), which is a tree-based supervised learning algorithm that finds feature splits based on the $\\chi^2$ statistic. When pruning the CHAID tree, feature splits with the lowest value of the $\\chi^2$ statistic are removed, which is similar to what we do when merging categories such that the drop in the $\\chi^2$ statistic value is as small as possible. In the next turorials, you will learn more about tree-based algorithms and implement a decision tree from scratch. Solving this exercise will help you to make a step towards a better understanding of the principles of these learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgTqNdOGZTzP"
      },
      "source": [
        "# Solution to the optional task\n",
        "def optimize_grouping(cat_feature):    \n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}