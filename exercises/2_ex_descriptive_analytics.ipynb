{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/exercises/2_ex_descriptive_analytics.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BADS Exercise 2 on the foundations of descriptive analytics\n",
    "The second exercise comprises one more task to advance your skills in Python programming. It's main focus is, of course, cluster analysis. However, to warm up, we start with some more exercises on Python programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Further exercises on Python programming\n",
    "The following exercise tasks revisit some concepts covered in [Demo notebook 1 on Python programming](https://github.com/Humboldt-WI/bads/blob/master/demo_notebooks/1_nb_python_intro.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using inbuilt functions and libraries\n",
    "In Python and other programming language, we use inbuilt functions all the time. Libraries like `NumPy` and `Pandas`, which offer a ton of functionality for handling and analyzing data, are the main reason why the Python language is such a good fit for data science. Let's practice our ability to access libraries and use their functions with some concrete tasks.\n",
    "\n",
    "The density of the normal distribution with mean $\\mu$ and variance $\\sigma$ is given as\n",
    "$$f(x | \\mu ,\\sigma ^{2}) = {\\frac {1}{\\sqrt {2\\sigma ^{2}\\pi}}}e^{-{\\frac {(x-\\mu )^{2}}{2\\sigma ^{2}}}}$$\n",
    "\n",
    "Let's create a nice plot of the bell curve that is so famous and characteristic for the normal distribution. Below, we already made sure that relevant libraries are imported. First, define two variables that store the two parameters of the normal distribution; no need to spill out these parameters, right? Next, generate some values $x$. Say you want to plot the bell curve for $x \\in \\{-3, 3\\} $. Use the `NumPy` function `linspace()` for this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# let's get the data for x\n",
    "x = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for each value of $x$, compute the probability that a normally distributed random variable would be arbitrarily close to that value. To calculate the probability density of the normal distribution, you can use the function `norm.pdf`. The function is part of the *stats models library*, which we import below. So you can write something like `stats.norm.pdf(...)` where ... stands for the arguments that the function requires. Make sure to store the results of the computation in a variable with name **nvValues**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the desity of the normal distribution at x\n",
    "nvValues ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to plot. Create a simple graph of **nvValues** against **x** using the `plot()`function. Let's say you want your line to be in red color. Use the help and web search to find out how to plot a red line. Also make sure to label your axes; remember: never create a plot without axis labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exercises on descriptive analytics\n",
    "The following exercise tasks revisit some concepts covered in [Demo notebook 2 on descriptive analytics](https://github.com/Humboldt-WI/bads/blob/master/demo_notebooks/2_nb_descriptive_analytics.ipynb). \n",
    "\n",
    "### 2.1 Data generation\n",
    "We want to revisit kMeans and need some data for this purpose. Make use of the function `make_blobs()`, which is part of the `sklearn` library to generate some artificial data. Say we want to **create data with 4 clusters**. Make sure to configure the `make_blobs()` function appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data for clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 kMeans\n",
    "Our GitHub repository provides a *from scratch* implementation of the kMeans algorithm. Below you find the code that we will need for this exercise. For more details visit the corresponding [kMeans from scratch notebook](https://github.com/Humboldt-WI/bads/blob/master/algorithms_from_scratch/kmeans.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need to copy the function `KMeans_from_scratch` from the previous tutorial\n",
    "# as well as the helper functions required by it\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    \"\"\"Computes Euclidean distance between two arrays\"\"\"\n",
    "    return np.sqrt(np.sum((a - b)**2))\n",
    "\n",
    "\n",
    "def label_cluster(n_samples, clusters):\n",
    "    \"\"\"each sample will get the label of the cluster it was assigned to\"\"\"\n",
    "    # creates empty array as long as samples for future labels\n",
    "    labels = np.empty(n_samples)  \n",
    "\n",
    "    for cluster_idx, cluster in enumerate(clusters):\n",
    "        # assign labels to each sample\n",
    "        for sample_index in cluster:\n",
    "            labels[sample_index] = cluster_idx  \n",
    "    return labels\n",
    "\n",
    "\n",
    "def create_clusters(K, X, centroids):\n",
    "    \"\"\"Assign the samples to the closest centroids to create clusters\"\"\"\n",
    "    clusters = [[] for _ in range(K)]  # creates a list of K number of lists\n",
    "    for idx, sample in enumerate(X):\n",
    "        # find closest centroid for each sample\n",
    "        centroid_idx = find_closest_centroid(sample, centroids)\n",
    "        # create index list of closest centroids\n",
    "        clusters[centroid_idx].append(idx)\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def find_closest_centroid(sample, centroids):\n",
    "    \"\"\"Distance from each sample to every centroid\"\"\"\n",
    "    # calculate distance from each sample to each centroid\n",
    "    distances = [euclidean_distance(sample, point) for point in centroids]  \n",
    "    # take closest centroid (one with minimal distance)\n",
    "    closest_index = np.argmin(distances)\n",
    "    return closest_index\n",
    "\n",
    "\n",
    "def update_centroids(X, K, n_features, clusters):\n",
    "    \"\"\"Assign mean value of cluster features to each centroid\"\"\"\n",
    "    centroids = np.zeros((K, n_features))\n",
    "    for cluster_idx, cluster in enumerate(clusters):\n",
    "        # find new centroid by finding mean of all points assigned to centroid\n",
    "        cluster_mean = np.mean(X[cluster, :], axis=0)\n",
    "        centroids[cluster_idx] = cluster_mean  # collect all centroids\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def is_converged(centroids_old, centroids, K):\n",
    "    \"\"\"Check if centroids have changed since last iteration\"\"\"\n",
    "    # check distance between old and new centroids\n",
    "    distances = [euclidean_distance(centroids_old[i], centroids[i]) for i in range(K)]\n",
    "    # return Boolean indicating whether centroids are the same as before or not\n",
    "    return sum(distances) == 0\n",
    "\n",
    "\n",
    "def KMeans_single(X, K=5, max_iters=100):\n",
    "    \"\"\"Choose a random set of centroids then optimise using above functions\"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # Initialization\n",
    "    # take random sample points to be initial clusters\n",
    "    random_sample_idx = np.random.choice(n_samples, K, replace=False)\n",
    "    # label these points as centroids\n",
    "    centroids = [X[idx] for idx in random_sample_idx]\n",
    "\n",
    "    iteration_num = 0  # initialize iteration tracker\n",
    "\n",
    "    # Optimization\n",
    "    for iteration in range(max_iters):\n",
    "\n",
    "        iteration_num += 1  # track iterations required\n",
    "\n",
    "        # assignment to closest centroids (cluster creation)\n",
    "        clusters = create_clusters(K, X, centroids)\n",
    "\n",
    "        centroids_old = centroids  # archive previous centroids\n",
    "        centroids = update_centroids(\n",
    "            X, K, n_features, clusters)  # Updating centroids\n",
    "\n",
    "       # Convergence Confirmation\n",
    "        # check if last iteration's centroids were the same as current iteration\n",
    "        if is_converged(centroids_old, centroids, K):\n",
    "            break  # exit loop since there was no change since last iteration\n",
    "\n",
    "    # Clustering has converged or we have hit the max number of iteration:\n",
    "    # Determine current cluster solution\n",
    "    # classify samples as the index of their clusters\n",
    "    labels = label_cluster(n_samples, clusters)\n",
    "    # get distance from each point to its centroid\n",
    "    dist = [euclidean_distance(X[row_num], centroids[int(labels[row_num])])\n",
    "            for row_num in range(X.shape[0])]\n",
    "    # total distance calculated as sum of squares\n",
    "    total_dist = np.sum(np.square(dist))\n",
    "\n",
    "    # Classify samples as the index of their clusters\n",
    "    return labels, iteration_num, centroids, total_dist\n",
    "\n",
    "\n",
    "def KMeans_from_scratch(data, k_clusters, n_iter=10):\n",
    "\n",
    "    for i in range(n_iter):\n",
    "\n",
    "        # run KMeans once\n",
    "        labels, iteration_num, centroids, dist = KMeans_single(\n",
    "            data, K=k_clusters)\n",
    "\n",
    "        # save results if they are the best so far\n",
    "        if 'best_dist' not in locals() or dist < best_dist:\n",
    "\n",
    "            best_labels = labels\n",
    "            best_iter = iteration_num\n",
    "            best_centroids = centroids\n",
    "            best_dist = dist\n",
    "\n",
    "    return best_labels, best_iter, best_centroids, best_dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the implementation by clustering the data created in 2.1 above. We know the data has 4 clusters, so feel free to set $k=4$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering of the artificial data\n",
    "k = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Distance function\n",
    "In the lecture, we argued that clustering methods like kMeans are versatile in that they support various distance measures. Let's convince ourself that this is true. Your task is to write a custom function that calculates **cosine similarity**. You can look up the formula of the cosine similarity in the lecture slides of chapter 2, or from the Internet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that computes cosine similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next thing to do is to change our from scratch implementation of the kMeans algorithm such that it uses cosine similarity for clustering the data. To achieve this, write a new custom function `KMeans_with_cosine_similarity`. You can copy/paste the above implementation of the function `KMeans_from_scratch` to a large extent, and then add adjustments to exchange Euclidian distance with cosine distance. \n",
    "\n",
    "**Hint:** We aware of the fact that a similarity function and a distance function are two sides of the same coin but not identical. Once you are able to compute cosine *similarity* you need to adjust your *similarity* measure to obtain a *distance* measure. Afterwards, you can use conside *distance* within kMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kMeans with cosine similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 New clustering solution\n",
    "The last task on the list is to apply the altered kMeans with cosine similarity to your synthetic data set. Try to come up with a suitable way to compare the results of the two versions of kMeans. It is natural to ask how the cluster solutions differ when using Euclidean distance or cosine similarity. How would you answer that question? Make use of your Python skills to come up with an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cluster solution with the modified kMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write some code to compare the two cluster solutions from using Euclidean distance and cosine similarity \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Alternative approach\n",
    "The kMeans algorithm with cosine similarity can also be implemented more elegantly by normalizing the rows of $X$ to be of unit length. The reason being that if $X_1$ and $X_2$ are unit vectors, the term inside the brackets in the last line of the following equation is equivalent to the cosine distance.\n",
    "\n",
    "\\begin{align*}\n",
    "\\vert\\vert X_1 - X_2 \\vert\\vert_2^2 &= X_1^T X_1 + X_2^T X_2 - 2 X_1^T X_2 \\\\\n",
    "&= 2 - 2 X_1^T X_2 \\\\\n",
    "&= 2 (1 - X_1^T X_2)\n",
    "\\end{align*}\n",
    "\n",
    "Normalize $X$ before computing kMeans clusters with Euclidian distances. Compare your solution to that from task 2.4 and briefly note your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize X and create cluster solution using kMeans with Euclidian distiance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write some code to compare the cluster solution with the one from task 2.4 (cosine similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Further tasks\n",
    "Still not enough? Ok, that is great! Here are a few ideas for some more tasks associated with kMeans and the scope of this exercise.\n",
    "- Write a custom function calculating the city-block or Manhatten metric\n",
    "- Improve the from scratch implementation of kMeans such that it supports the caller to specify the desired distance function as an argument\n",
    "- The above changes should enable you to flexibly run kMeans with Euclidean, cosine, and city-block distance. Try that out using your synthetic data\n",
    "- Use the `sklearn` function `make_classification()` to generate a more challenging data and apply kMeans to it. You can use your customer implementation of kMeans or the one available in `sklearn`\n",
    "- Run a web-search for the **IRIS data set**. It is a very well known data set. Quickly familiarize yourself with the data. Afterwards, load it using the function `sklearn.datasets.load_iris()`. Check whether kMeans is able to identify the three types of iris flowers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done... puh, that was a lot of work. And you did it! Congratulations!!! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "0d59063d68e7723de5ed538f052e9a0cf5060d8feb35a125775eb36a31b4dca6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
