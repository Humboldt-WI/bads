{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "PyCharm (VHB-ProDok-Internal)",
      "language": "python",
      "name": "pycharm-e3448bea"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "8_ex_ensemble_learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XstQcr6KZbTV"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/exercises/8_ex_ensemble_learning.ipynb) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhjDMKBTZbTV"
      },
      "source": [
        "# BADS Exercise 8 on ensemble learning\n",
        "This exercise revisits some of the concepts covered in [Tutorial 8 on ensemble learning](https://github.com/Humboldt-WI/bads/blob/master/tutorials/8_nb_ensemble_learning.ipynb). We will take a close look at bagging and analyze its impact on the predictive accuracy, and implement one of the boosting algorithms, Adaboost.\n",
        "\n",
        "## Loading the data \n",
        "Fo this tutorial, we will use HMEQ credit risk data available at [our GitHub repo](https://github.com/Humboldt-WI/bads/blob/master/data/hmeq_prepared.csv). By now, you have imported different data sets multiple times in previous tutorials, but this step is always necessary when working with data. Your preliminary task is to load the HMEQ data set into a `pandas DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpr9o9HxbT5v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqw_wcW2ZbTW"
      },
      "source": [
        "Now we can proceed to the tasks on ensemble learning!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoW4Z3kIPnZk"
      },
      "source": [
        "## Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3thD4BpPvqW"
      },
      "source": [
        "### Task 1\n",
        "\n",
        "Ensemble learning works by reducing bias and/or variance. We begin with examining the variance component. \n",
        "\n",
        "In the first task, you will write code that trains and tests a classifier multiple times on different subsets of HMEQ data and examines the classifier preformance. We prepared two versions of this task: *simple* and *for the experts*. Read the task description below and proceed with the version you feel ready to tackle!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT8GKYaYSp9y"
      },
      "source": [
        "*Simple version:* set up a loop to sample the data, calling the sklearn function `train_test_split()` multiple times in a loop. You can use either logistic regression or a decision tree as a classifier. Train and test a new classifier on the sampled data in each iteration of the loop and compute its AUC on the test set. Run your code for 100 iterations and visualize the variation in AUC performance by means of a boxplot. Briefly discuss your findings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I86l0smaQSfm"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2GkdCKHQGa4"
      },
      "source": [
        "*For the experts:* perform the same task as above but wrap your code in a function `examine_variance()` that:\n",
        "- supports both logit and decision tree as a classifier\n",
        "- allows to specify the number of iterations and test sample size\n",
        "- facilitates controlled sampling of the data such that you randomize either the training or the test set or both sets in each iteration. The idea is that your code should let you study the isolated effect of randomizing only the training data while always predicting the same test data, or the isolated effect of applying the same model to multiple randomized test sets, or the overall effect of sampling the data just as in the simple version\n",
        "- returns a list of AUC values from each iteration\n",
        "\n",
        "Run your function for 100 iterations and visualize the AUC values using a boxplot. Briefly discuss differences between the AUC variance when randomizing the training data, the test data or both. \n",
        "\n",
        "*Hint:* you can rely on the function `bootstrapping()` from the ensemble learning tutorial to randomize the sampling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdO6A7TxbI_5"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuacyFZIQTTb"
      },
      "source": [
        "### Task 2\n",
        "\n",
        "Implement a bagged logistic regression classifier from scratch. You can use the `sklearn` class `LogisticRegressionClassifier` for implementing the base model. The actual bagging step, however, should be implemented from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC2PKhiBNZfp"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8B1ql4cQXYM"
      },
      "source": [
        "### Task 3\n",
        "\n",
        "Theory predicts that bagging should work better for unstable base models like trees than for stable base models like logistic regression. Use the custom bagging algorithm developed in Task 2 to verify this assertion for the HMEQ loan dataset. Specifically:\n",
        "  - chose a proper experimental design to compare models (split-sample or cross-validation)\n",
        "  - train two simple classifiers: \n",
        "    - logistic regression\n",
        "    - decision tree\n",
        "  - train two bagging classifiers:\n",
        "      - bagged logistic regression\n",
        "      - bagged decision tree\n",
        "  - both bagging classifiers should use your custom bagging function from Task 2\n",
        "  - compare the predictive performance of the bagging ensembles on the test data and briefly discuss your findings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3Vl67GuNZRn"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOXR9MN3QaxJ"
      },
      "source": [
        "### Task 4 [optional]\n",
        "\n",
        "#### 4.1. Further enhance the analysis from Task 3 as follows:\n",
        "  - repeat the comparison of bagged logit versus bagged trees multiple times with different training and testing data sets\n",
        "  - depict the results (predictive performance) as a boxplot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1sd1D6o5gkt"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7LVHGBro09H"
      },
      "source": [
        "### 4.2. Investigate the impact of the ensemble size:\n",
        "- try out different settings for the hyperparameter *ensemble size* (number of bagging iterations)\n",
        "- produce a line plot of predictive performance versus ensemble size for bagged logit and bagged tree\n",
        "- identify the suitable ensemble size for both classifiers  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vla-mENcoyAN"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nanPbDybSGpO"
      },
      "source": [
        "### Task 5\n",
        "Write a custom Python function that implements the *Adaboost* algorithm. Follow the pseudo-code of the algorithm, as shown in the lecture materials. Design your function such that it accepts a `sklearn` model object as argument and than runs Adaboost using the corresponding base classifier. Test your function on the HMEQ data and evaluate performance in terms of AUC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZnu5cFibKyS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0v5WATBZbTX"
      },
      "source": [
        "# Well done! Your ensembles performed great!"
      ]
    }
  ]
}