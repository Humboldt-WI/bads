{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swSaDjWvNDnY"
   },
   "source": [
    "# Chapter 5 - Algorithms for supervised learning \n",
    "The following exercises test your familiarity with the content of chapter 5. The corresponding [tutorial notebook](https://github.com/Humboldt-WI/bads/blob/master/tutorials/5_nb_supervised_learning.ipynb) was quite heavy including a walkthrough of building logit and tree models from scratch. Given the algorithm-heavy tutorial, we opted for a somewhat light exercise session that basically allows you to verify some of the claims made in the lecture. Specifically, the exercise revisits the lecture part on linear and logistic regression for classification. After completing the exercises, you will have gained even more confidence in logistic regression being the better approach for classification and, and this is maybe the main point of the tasks, further improved your Python programming skills. \n",
    "\n",
    "Have fun with the following tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: \n",
    "We established in the lecture, that the linear regression model is not suitable for classification problems. Let's test this claim empirically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Synthetic data\n",
    "Create a synthetic data set for binary classification using the sklearn function `make_classification()`. We want to plot the data later. So use only two input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Linear regression\n",
    "We discussed the analytical solution to the least-squares loss minimization problem in the lecture. To recall, using our standard notation, we can compute the regression parameters $\\beta$ by \n",
    "$$ \\beta^* = \\left( X^{\\top} X \\right)^{-1}  X^{\\top}  y $$\n",
    "\n",
    "Calculate $\\beta^*$ using plain numpy. Recall that you need to augment $X$ with a column of ones to ensure that your regression model includes an intercept.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Plotting\n",
    "Create a chart that depicts your data set. Use different colors for the data points of the two classes. Also plot the decision surface that corresponds to your regression model from task 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Residuals\n",
    "Compute and plot the residuals of your linear regression model. Inspecting the graph, how do you judge the adequacy of the linear regression model for the data at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Logistic regression\n",
    "Now estimate a logistic regression model for your synthetic data set using standard sklearn functionality. Reproduce the plot from task 1.3 and incorporate the class boundary from the logistic model in the plot. It should also include the result from the linear regression. By visual inspection, does the logit model give a better fit? Briefly state your opinion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (adams)",
   "language": "python",
   "name": "pycharm-feb95198"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
