{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iGz0qnmS-UK"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/demo_notebooks/pandas_data_prep_eda.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHScrtl_S-UK"
   },
   "source": [
    "# Self-Study Notebook on Pandas, Data Preparation, and EDA. \n",
    "\n",
    "In this self-study notebook, you will further deepen your understanding of **Pandas**, the goto library for working with tabular data in Python. We will draw on our [Python Data Science 1o1 Notebook 1](https://github.com/Humboldt-WI/bads/blob/master/python_data_science_1o1.ipynb), which introduced the two core Pandas classes *data series* and *dataframe*, and demonstrate how to use dataframes for loading and manipulating data. To that end, the demo notebook introduces a real-world data associated with credit scoring. \n",
    "\n",
    "The second part of the demo notebook revisits our fourth lecture on **data preparation**. We will see how Pandas dataframes natively performs basic data preparation operations such as replacing missing values, and how this functionality can be extended to handle outliers and categorical data, amongst others. \n",
    "\n",
    "Finally, the demo notebook demonstrate basic and advanced ways to chart data for **explanatory data analysis**. \n",
    "\n",
    "The outline of the demo notebook is as follows:\n",
    "- The HMEQ data set\n",
    "- Pandas reloaded ...\n",
    "- Data preparation\n",
    "- Explanatory data analysis\n",
    "\n",
    "Before moving on, let's import some of our standard library so that we have them ready when we need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33ZdGrYCS-UK"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Some configuration of the plots we will create later\n",
    "%matplotlib inline  \n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BKS1jVVS-UK"
   },
   "source": [
    "## The HMEQ data set\n",
    "Our data set, called the  \"Home Equity\" or, in brief, HMEQ data set, is provided by www.creditriskanalytics.net. It comprises  information about a set of borrowers, which are categorized along demographic variables and variables concerning their business relationship with the lender. A binary target variable called 'BAD' is  provided and indicates whether a borrower has repaid her/his debt. You can think of the data as a standard use case of binary classification.\n",
    "\n",
    "You obtain the data, together with other interesting finance data sets, directly from www.creditriskanalytics.net. The website also provides a brief description of the data set. Specifically, the data set consists of 5,960 observations and 13 features including the target variable. The variables are defined as follows:\n",
    "\n",
    "- BAD: the target variable, 1=default; 0=non-default \n",
    "- LOAN: amount of the loan request\n",
    "- MORTDUE: amount due on an existing mortgage\n",
    "- VALUE: value of current property\n",
    "- REASON: DebtCon=debt consolidation; HomeImp=home improvement\n",
    "- JOB: occupational categories\n",
    "- YOJ: years at present job\n",
    "- DEROG: number of major derogatory reports\n",
    "- DELINQ: number of delinquent credit lines\n",
    "- CLAGE: age of oldest credit line in months\n",
    "- NINQ: number of recent credit inquiries\n",
    "- CLNO: number of credit lines\n",
    "- DEBTINC: debt-to-income ratio\n",
    "\n",
    "As you can see, the features aim at describing the financial situation of a borrower. We will keep using the data set for many modeling tasks in this demo notebook and future demo notebook. So it makes sense to familiarize yourself with the above features. Make sure you understand what type of information they provide and what this information might reveal about the risk of defaulting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFgoPwxcS-UL"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dhX-K3MS-UL"
   },
   "source": [
    "## Pandas reloaded\n",
    "You have already seen the Pandas library in action in [Demo Notebook 2](https://github.com/Humboldt-WI/bads/blob/master/demo_notebooks/2_nb_descriptive_analytics.ipynb). There, however, we used data series and frames to store synthetic data that we created for the purpose of demonstration. The much more common use case is to load data that already exists on disk, a cloud drive, on the web, etc.\n",
    "\n",
    "Loading such data into a `DataFrame` means that you load it into the main memory of your computer and can subsequently work with it. Note that loading larger data sets can take a long time and that they will consume a sizeable part of your main computer memory. More on that later. The HMEQ data should not challenge your PC ;)\n",
    "\n",
    "Remember that we have to load the Pandas library prior to using its classes. Let's do so and then examine a few options to load data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CddV9ByWS-UL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuEYYNrVS-UL"
   },
   "source": [
    "### Loading data from disk\n",
    "Say you have downloaded the data to your hard disk. It will be stored in some format in some folder somewhere on your computer. A common format for small data sets is the **csv** format, meaning comma separated values. This is also the format of our data. Before moving on, use a text editor to open the data and take a look at the format of the file. This is clarify what is meant by csv.\n",
    "\n",
    "Pandas supports a variety of standard data formats including csv. To load the data, you will need to specify the full path to data file on your hard disk. The easiest way to do so is to put the data into your current working directory. Roughly speaking, this is the directory in which the Python interpreted is looking for files. You can identify the working directory as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUiqjcfcS-UL",
    "outputId": "3e3f2985-ca27-45ba-b21f-00e5fa951955"
   },
   "outputs": [],
   "source": [
    "import os  # operating system interface \n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJLQ6kYYS-UM"
   },
   "source": [
    "Once we know where to find the file, we can load the data into a Pandas `DataFrame`. Recall that a data frame is basically a container for data. It is stored in memory and facilitates querying and manipulating tabular data. Note that the following code would also work if the data were not stored as a .csv file but as a zip archive. That is useful as it allows you to store larger data set in a compressed format (e.g., zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElnrvgSMS-UM"
   },
   "outputs": [],
   "source": [
    "# You have to update the code such that the variable file includes the correct path to the csv file on your computer\n",
    "file = '../data/hmeq.csv'\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cm1FGNRvS-UM"
   },
   "source": [
    "To convince yourself that data loading was successful, you can simply make Python outputting a summary of the data by typing the variable name of the data frame object; that is `df` in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qeg4z-acS-UM",
    "outputId": "e806cdfc-fb42-4e30-abd9-e98436d83932"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ut9UvTmMS-UN"
   },
   "source": [
    "Before discussing the above output, let's first showcase another way to load the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0t-qUdgDS-UN"
   },
   "source": [
    "### Loading data from the web\n",
    "Surprisingly, loading data stored somewhere on the web is even easier compared to loading data from disk because we do not have to worry so much where exactly the data is stored. All we need to do is to give Pandas an URL. To demonstrate this, we keep a copy of the HMEQ data set in the data folder of the [BADS Github repo](https://github.com/Humboldt-WI/bads/). The following code loads the data directly from the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CFQPyjiS-UN",
    "outputId": "17a8c028-8539-4a22-9c1c-359146d2761a"
   },
   "outputs": [],
   "source": [
    "# Load the data directly from GitHub\n",
    "data_url = 'https://raw.githubusercontent.com/Humboldt-WI/bads/master/data/hmeq.csv'\n",
    "df = pd.read_csv(data_url)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mj0wk9gzS-UN"
   },
   "source": [
    "### Eyeballing data \n",
    "The Pandas data frame provides a ton of useful functions for data handling. We begin with showcasing some standard functions that one needs every time when working with data. This also allows us to re-visit some of the functions that we have seen in the Pandas part of the Python introduction (i.e., [Demo Notebook 1](1_nb_python_intro)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PH9g48kDS-UN",
    "outputId": "4d778539-f9ee-4e39-9bbe-22063bc53025"
   },
   "outputs": [],
   "source": [
    "# Query some properties of the data\n",
    "print('Dimensionality of the data is {}'.format(df.shape))  # .shape returns a tupel\n",
    "print('The data set has {} cases.'.format(df.shape[0]))     # we can also index the elements of that tupel\n",
    "print('The total number of elements is {}.'.format(df.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dd4TMhnyS-UN",
    "outputId": "7ea3a4a9-a9da-41ab-b15e-ce71106e5975"
   },
   "outputs": [],
   "source": [
    "# Preview of the first n rows (similar to just typing the variable name as above but with more control)\n",
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvsd7RIeS-UN",
    "outputId": "dc056c5a-a44c-4897-c2c4-addf228414fb"
   },
   "outputs": [],
   "source": [
    "# Similarily, you can inspect the last rows\n",
    "df.tail(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DdYdSXezS-UN",
    "outputId": "964fceae-a2a3-4a05-f5fb-5a4cf04755f1"
   },
   "outputs": [],
   "source": [
    "# Obtain a more technical overview of the data \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnNVr6oXS-UO"
   },
   "source": [
    "The above output displays some useful information how exactly data is stored. We learn about the data type of each feature (i.e, each *data series* object), missing values, and the total amount of memory that the data consumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3_MZA2rS-UO",
    "outputId": "1a0b20f0-3173-427c-a9bb-4cf55500223e"
   },
   "outputs": [],
   "source": [
    "# Produce summary statistics (to R-programmers: this is equivalent to the famous R function summary())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5Y4xUmnS-UP"
   },
   "source": [
    "The previous demos gave as an overview of the data. However, if you compare the output to the `describe()` function list of features given on the www.creditriskanalytics.net website (see above), you will notice that we are missing some features. For example, we lack a summary of the feature *REASON*; same with *JOB*. If you think about it, that actually makes sense. The result from the function `info()` showed how these features are stored as data type *object*. They are not stored as numeric variables. Consequently, statistical / mathematical operations like computing a mean or quantile are undefined and cannot be computed for these variables. That said, you can still force the `describe()` function to consider all features in its output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4CMz8ilS-UP",
    "outputId": "822b8c81-aa6e-4bcd-a00c-d9f0b9bf18cc"
   },
   "outputs": [],
   "source": [
    "# The argument include all ensures that non-numeric variables are also shown\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJIDTC8sS-UP"
   },
   "source": [
    "You can also calculate a summary statistic for each column individually by using the corresponding method, all methods can be found in the [Pandas reference](https://pandas.pydata.org/docs/reference/frame.html). In the below example, do not worry about the `sep` argument in the `print()` function. It just inserts two empty lines to make the output more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdRVLD__S-UP",
    "outputId": "35cd1c5a-3c3e-4813-82d7-91b7f123d60b"
   },
   "outputs": [],
   "source": [
    "print(df.select_dtypes(exclude='O').mean(), df.select_dtypes(exclude='O').median(), sep=\"\\n\\nFeature medians\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVqCk2tbS-UP"
   },
   "source": [
    "### Aggregation and grouping\n",
    "The `describe()` function already introduces functionality that aggregates the data in a data frame. Aggregations are extremely useful in EDA and for feature engineering. One of the function that you need to know when it comes to aggregations is `value_counts()`. Our data represents a classification problem. So you would want to know the relative frequencies of the target variable values. This is only one of the many useful ways in which you can leverage the function.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeDmEt8NS-UP",
    "outputId": "a377da2c-64df-4e42-9fbb-c66941e0e8c9"
   },
   "outputs": [],
   "source": [
    "# Count the frequency of good and bad credit risks\n",
    "df.BAD.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVrCrwS9S-UP"
   },
   "source": [
    "Ok, knowing the good to bad ratio is clearly useful. Another standard question for classification problems would be how feature values distribute across the classes (i.e., good and bad borrowers). Time to introduce yet another important Python function: `group_by()`. You might remember the *group by* clause from SQL. Data frames offer similar functionality. The function is quite powerful. We could have a whole demo notebook only on `group_by()`. Fortunately, such demo notebook already exists (see, e.g., [here](https://realpython.com/pandas-groupby/) or check out the [official documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html)). For now, we leave it with a simple demo to sketch what you can do with `group_by()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gihNrZ5AS-UP",
    "outputId": "d07a3b06-5503-460d-f8a0-22327e8859f7"
   },
   "outputs": [],
   "source": [
    "df.groupby(\"BAD\")[\"LOAN\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wp2Ids8BS-UQ"
   },
   "source": [
    "We just calculated the mean loan amount using the target variable for grouping. We find that bad clients borrow less money than good clients, on average. The lender might be pleased to know that. Better than the opposite result, right? But let's not dive too deep into financial implications. Our point was to showcase grouping. Let's extend the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0DaxyUwS-UQ",
    "outputId": "a7ccaa4e-4fe2-4719-fccd-29ea14780caf"
   },
   "outputs": [],
   "source": [
    "# We can group multiple features in one go\n",
    "df.groupby(\"BAD\")[[\"LOAN\", \"MORTDUE\", \"VALUE\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OpFPfPnuS-UQ",
    "outputId": "503676e8-b5b5-4b3c-f66f-48ef8cf0ae37"
   },
   "outputs": [],
   "source": [
    "# We can use any aggregation function that Pandas supports, including the median\n",
    "df.groupby(\"BAD\")[[\"LOAN\", \"MORTDUE\", \"VALUE\"]].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-d0MyUTS-UQ",
    "outputId": "88b85688-448b-4006-a319-673b5bc591c8"
   },
   "outputs": [],
   "source": [
    "# or computing quantiles\n",
    "df.groupby(\"BAD\")[[\"LOAN\", \"MORTDUE\", \"VALUE\"]].quantile(q=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mO4zjD9bS-UQ",
    "outputId": "dd2a5211-45f4-41e6-dc90-3d145f23b407"
   },
   "outputs": [],
   "source": [
    "# and in case you wonder, the output that we received from the above demos is just a Pandas series object\n",
    "type(df.groupby(\"BAD\")[\"LOAN\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6r1Tkf8S-UQ"
   },
   "source": [
    "### Navigating data\n",
    "We discussed indexing numpy arrays and Pandas data frames in [Demo Notebook 1](https://github.com/Humboldt-WI/bads/blob/master/demo_notebooks/1_nb_python_intro.ipynb). However, indexing is such a crucial part of working with data that it make sense to refresh our minds by including a few more demos. We will only illustrate some more popular options. A web search for \"pandas data frame indexing\" will provide many additional insights if you are interested. Likewise, feel free to skip this part if you already feel comfortable with data frame indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIYUWcVJS-UQ"
   },
   "source": [
    "#### Basic indexing of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "st_ZSynMS-UQ",
    "outputId": "a86ba3a3-2b63-4a9f-db73-6df8aa8c0e6f"
   },
   "outputs": [],
   "source": [
    "# Accessing a single column by name\n",
    "df['BAD']\n",
    "# Alternatively\n",
    "df.BAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zozXICo0S-UR",
    "outputId": "5990c7d1-b5e8-416d-be60-e866d591b0ff"
   },
   "outputs": [],
   "source": [
    "# R-style indexing of selected rows and columns\n",
    "df.loc[0:4, [\"BAD\", \"LOAN\"]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8P8I_9E2S-UR",
    "outputId": "9b7ab9d9-c8df-4c88-9e21-ad148acbba25"
   },
   "outputs": [],
   "source": [
    "# Access columns by a numerical index\n",
    "df.iloc[0:4, 0]\n",
    "df.iloc[0:4, [0, 3, 5]]\n",
    "df.iloc[0:4, np.arange(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xiq3fI9qS-UR"
   },
   "source": [
    "A few cautionary notes on numerical indexing in Python. The function `iloc()` considers the index of the data frame. In the above output, this is the left-most column without header. We have not defined a custom row index and Python uses consecutive integer numbers by default. However, a data frame could also have a custom index. In such a case, calls to `iloc()` need to refer to the custom index. It is good practice to eyeball a data frame and verify the way in which rows are indexed prior to using `iloc()`.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WS9tBMunS-UR"
   },
   "source": [
    "#### Other common forms of indexing and subset selection\n",
    "It is also common practice to select rows based on comparisons of feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_o4y-7aS-UR",
    "outputId": "cfaa733c-7ea2-439e-e71d-793e9844d55c"
   },
   "outputs": [],
   "source": [
    "q = \"NINQ>10\"  # our query using the variable NINQ\n",
    "tmp = df.query(q)\n",
    "print('Number of applicants with {} is {}.'.format(q, tmp.shape[0]))\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9jM6QM9S-UR"
   },
   "source": [
    "Alternatively, we can also use the function `loc()` for logical indexing. Here, we query all bad applicants and output the values of three features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jh4xuEKWS-UR",
    "outputId": "13cebe56-c8a2-436b-82f4-f5c66a15b33b"
   },
   "outputs": [],
   "source": [
    "df.loc[df.BAD == 1, ['LOAN', 'JOB', 'YOJ']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9KB26FrS-UR"
   },
   "source": [
    "### Manipulating data\n",
    "Data preparation (see next) will require us to change the data stored in a data frame. Let's introduce some functionality and concepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5EaoWGmS-UR"
   },
   "source": [
    "#### The inplace Argument\n",
    "When altering values in a data frame, you should know the argument `inplace`. Many function that result in changes support this argument. If set to `True`, whatever change was made to the data, it will be reflected right in the data frame. If set to `false`, on the other hand, the data frame to which the function was applied will not change. Instead, the function will only return a new data frame, which incorporates the change. Ok, that sounds rather theoretical. Let's look at an example. Consider the following call of the `sort_values` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylQHYoQCS-US",
    "outputId": "ccbf8143-cf23-431d-acd9-e815797e09bf"
   },
   "outputs": [],
   "source": [
    "df.sort_values(by=\"VALUE\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlQ9W9kxS-US"
   },
   "source": [
    "The purpose of `sort_values` is fairly obvious. Good to know this function exists. However, the point of the demo was to show that we obtain a changed data frame. Inspect the index (left-most column). We have changed the rows in the data frame by sorting, right. But have we really? Let's output the data frame one more time as we usefully do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVyTWjM1S-US",
    "outputId": "64f8cee7-8872-4817-99d9-a4c707689c52"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d30OeG8XS-US"
   },
   "source": [
    "Ok, this is the original row oder. Consequently, we **have not** changed the data frame by calling `sort_values`. Rather, the function has created a new data frame with the same content of the original one but altered row order. Let's make this more explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYAQrCBXS-US"
   },
   "outputs": [],
   "source": [
    "df_changed = df.sort_values(by=\"VALUE\", ascending=False)  # this statement does not produce any output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8o7ynxSqS-US",
    "outputId": "7a1df8f7-5dd7-453e-eb1b-a744f3145839"
   },
   "outputs": [],
   "source": [
    "df_changed  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7g94_C3S-US"
   },
   "source": [
    "We create a variable, `df_changed` that points to the altered data frame. In this variable, the changes are persistent. What you will see quite often in Python is a statement like this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bmnkH3hJS-US"
   },
   "outputs": [],
   "source": [
    "df_changed = df_changed.sort_values(by=\"VALUE\", ascending=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoX8cKP-S-US"
   },
   "source": [
    "Here, we apply a function that changes our data and we overwrite the original data frame with the changed version. That works well. A possible pitfall is to make a change to the data and to expect that change to affect the original data without the assignment. We have seen above that this does not happen. However, many functions including `sort_values` support the argument `inplace`. Consider this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRDdhZO4S-US",
    "outputId": "359ef8ad-5b46-4d81-bf96-fb37d27d5dbe"
   },
   "outputs": [],
   "source": [
    "df_demo = df.copy()  # we do not want to really change our data so let's make a copy\n",
    "df_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-lt-vPnS-UT",
    "outputId": "93e22b04-6125-47cd-db79-bdfae0170183"
   },
   "outputs": [],
   "source": [
    "# This is the interesting part\n",
    "df_demo.sort_values(\"VALUE\", ascending=True, inplace=True)  # the changes from sorting will directly affect our data\n",
    "df_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSqTOKxZS-UT"
   },
   "source": [
    "I can imagine that you found the demo a little theoretical. So much to know about Pandas, not to mention other libraries... Let's agree on the following, whenever you manipulate data in a data frame, remember there was this weird thing of whether changes affect the data frame or not. That would already by enough. `inplace` is one way of enacting a change of the data but more importantly I want you to remember that an operation might or might not alter a data frame. Remembering this bit will help you a lot when progressing with Python; trust me. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NW4D58fUS-UT"
   },
   "source": [
    "#### The apply function\n",
    "More functionality that you need to know... It's getting a lot, right. Well, the good thing is that you can always come back to this notebook to look-up certain functions. That is the main reason why the notebook is so comprehensive, or long.\n",
    "<br>\n",
    "If you have used R, you will know the `apply()` function. It kinda does what the name suggests. It let's you define a function, either a custom one or one that already exist in Python, and apply that function to every element in a data frame. Combine that with indexing and you obtain a powerful way to selectively alter your data. \n",
    "<br>\n",
    "We provide some demos in the following. To avoid corrupting our data frame, which we will need later, all demos use a copy. And for simplicity, we consider only the numerical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlC_-KNbS-UT"
   },
   "outputs": [],
   "source": [
    "df_demo = df.select_dtypes(float) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-itB_qwS-UT"
   },
   "source": [
    "Say you want to square the values of all your features. Why you would want to do that? No idea, this is a demo so bear with it. Specifically, we showcase different forms of using `apply`. All three examples perform the same task, squaring all features in the data; which, you are right, is a pointless operation. Did I mention that this is a demo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AiQxmyzlS-UT",
    "outputId": "d93ce20b-14d3-4b37-fad4-512fb65aae02"
   },
   "outputs": [],
   "source": [
    "# Using apply together with an existing function\n",
    "df_demo.apply(np.square) # you can define a function directly like here, we have a square function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyeqQpm8S-UT",
    "outputId": "2b01ac81-627d-48bc-81d1-b3d68d7a03a6"
   },
   "outputs": [],
   "source": [
    "# Using apply together with a customer function\n",
    "def my_square(x):\n",
    "    return x*x\n",
    "\n",
    "df_demo.apply(my_square) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXKMvBw4S-UT",
    "outputId": "e55b5583-d1d1-4234-e98b-b21e1e78f076"
   },
   "outputs": [],
   "source": [
    "# Using apply together with a lamda function\n",
    "df_demo.apply(lambda x: x * x) # you can define a function directly like here, we have a square function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFnWNqP1S-UU"
   },
   "source": [
    "So this was apply in action. By the way, do you think any of the above examples did actually change the data frame? \n",
    "<br>\n",
    "Hopefully, the examples have sufficed to let you appreciate the beauty of `apply`. Writing your own custom function and then feeding every column of a data frame or a selection thereof - by indexing - let you perform some powerful operations. Again, for now, know that `apply()` exist. We will see more meaningful use cases as we go along (spoiler alert: we use `apply()` for outlier handling below) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBhKmhguS-UU"
   },
   "source": [
    "### Saving data to disk\n",
    "Saving is the natural complement to reading data. So for the sake of completeness, let's briefly exemplify one approach to store data using the `to_csv()` function.\n",
    "<br>\n",
    "The use of the function is straightforward. It receives a file name, possibly together with a specific path, and that is pretty much it. You can decide whether you want to index column to be stored together with the data. Since our index is just a consecutive list of integer numbers, we do not store the index and save a little bit of space.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-qz-wxvS-UU"
   },
   "outputs": [],
   "source": [
    "# Store the data in the present, pre-processed format\n",
    "df.to_csv('./hmeq_prepared.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-c-L0rdS-UU"
   },
   "source": [
    "Storing data in csv format might not be the best idea. Explore the Pandas library to check out other functions `to_xyz()` that facilitate writing data. `Pickle` and `hdf` are popular formats / modes of writing data in a binary format. Storing data in binary format will substantially reduce the size of the data file on disk and is also more efficient (e.g., faster reading and writing). We won't detail these formats here as we do not need them for the BADS lecture. Still, feel free to web search for, e.g., \"Pandas data storage\", to find resources [like this one](https://towardsdatascience.com/the-best-format-to-save-pandas-data-414dca023e0d), which offer more information.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AjUrqd_S-UU"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7uvFSUsS-UU"
   },
   "source": [
    "## Data preparation\n",
    "Data preparation is a mega-topic. It will accompany us in one way or the other throughout the whole course. At this point, we will address some typical issues in our data and, in doing so, demonstrate how to perform standard data prep tasks using Python and Pandas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZL4c5HyS-UU"
   },
   "source": [
    "### Altering data types\n",
    "We start with a rather technical bit, data types. Remember the way our data is stored at the moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6zC5rYPS-UU",
    "outputId": "623dc9d4-fbc1-49ed-df5a-a8af6c4fc3ce"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QP1iSYXDS-UU"
   },
   "source": [
    "The features *JOB* and *REASON* are stored as data type `object`. This is the most general data type in Python. A variable of this type can store pretty much any piece of data, numbers, text, dates, times, ... This generality has a price. The data type consumes a lot of memory. Further using a more specialized data type unlocks specific functionality, which is available only for that data type. Functions to manipulate text are an example. These are available for data of type `string`. \n",
    "<br>\n",
    "In our case, the two features that Pandas stores as objects are actually categorical variables. We can easily verify this using `value_counts`, amongst others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbBgoi3uS-UU",
    "outputId": "3fc66b34-74e2-472b-a185-0c929b7edc33"
   },
   "outputs": [],
   "source": [
    "df.REASON.value_counts()  # so REASON is a binary variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3FeAT1dS-UU",
    "outputId": "382ae29c-898f-4505-f9b2-7ddd148bb057"
   },
   "outputs": [],
   "source": [
    "df.JOB.value_counts()  # whereas JOB is a multinomial variable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4JsilV3S-UU"
   },
   "source": [
    "Knowing our two \"object features\" are categories, we should alter their data type accordingly. To that end, we make use of the function `astype`, which facilitates converting one data type into another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MO6IxmTS-UU",
    "outputId": "8f52d60c-034d-4ecb-9fad-873995bbc710"
   },
   "outputs": [],
   "source": [
    "# Code categories properly \n",
    "df['REASON'] = df['REASON'].astype('category')\n",
    "df['JOB'] = df['JOB'].astype('category')\n",
    "df.info()  # verify the conversion was successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXA9BTp_S-UV"
   },
   "source": [
    "Although it does not really matter for this tiny data set, note that the conversion from object to category has reduced the amount of memory that the data frame consumes. On my machine, we need 524.2 KB after the translation, whereas we needed more than 600 KB for the original data frame. If you work with millions of observations the above conversion will result in a significant reduction of memory consumption.\n",
    "<br>\n",
    "Let's change some more data types. The target variable is stored as an integer but we know that it has only two states. So we can convert the target to a boolean. Further, we might not need the numeric precision that the data type float64 provides. Downcasting to 32 bit precision should not affect modeling results too much. \n",
    "<br>If you feel uncomfortable with programming jargon (i.e., *downcasting*, number precision, ...) just accept the below part as another demo of Pandas functionality. Specifically, watch out for uses of the function `select_dtypes` in the below examples. A common use case when working with Pandas data frames is that you want to access all features (columns) of a certain data type and perform some operations with these columns; data type conversion being one example for the many operations you might want to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-K5ln75FS-UV"
   },
   "outputs": [],
   "source": [
    "# The target variable has only two states so that we can store it as a boolean\n",
    "df['BAD'] = df['BAD'].astype('bool')\n",
    "\n",
    "# For simplicity, we also convert LOAN to a float so that all numeric variables are of type float\n",
    "df['LOAN'] = df['LOAN'].astype(np.float64)\n",
    "\n",
    "# Last, let's change all numeric variables from float64 to float32 to reduce memory consumption\n",
    "num_vars = df.select_dtypes(include=np.float64).columns\n",
    "df[num_vars] = df[num_vars].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhqJ1Ty0S-UV"
   },
   "source": [
    "Invest some time to understand the above codes. We are getting more and more advanced with Python. Our coding examples start to combine multiple pieces of functionality. For example, the above demo uses indexing, functions, function arguments to perform a task. Keep practicing and you will become familiar with the syntax. As always, if you do not understand certain bits, note down your questions so that we can discuss them in our discussion sessions.\n",
    "<br>\n",
    "Finally, let's verify our changes once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cuBwfDSS-UV",
    "outputId": "e07680f7-dac9-423e-aa82-ee14b05f690f"
   },
   "outputs": [],
   "source": [
    "# Check memory consumption after the converstions\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8t60vNRS-UV"
   },
   "source": [
    "Wow, in total, our data type conversions reduced memory consumption by more than a half. You might want to bear this potential in mind when using your computer to process larger data sets ;) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLAnv1TUS-UV"
   },
   "source": [
    "---\n",
    "**Skip this part if you are new to Python and do not want to bother with efficiency issues yet.**\n",
    "##### Excursion on efficiency\n",
    "Since we speak about efficiency, note that the function `.select_dtypes`, which we have used above may be slow when working with larger data. This is because it performs some copying of the data internally. Should you ever experience issues, consider using *list comprehension* to get a list of column names for indexing a data frame. We try to sketch the performance differences between list comprehension and `select_dtypes` below. However, note that the example may fail if you work on a powerful machine where the difference in runtime might not be noticeable with this small data set. Both blocks of code implement the same logic and simply query all the numeric columns in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9gFj-inS-UV",
    "outputId": "4f81dd6b-d1df-4b68-c569-22d92e442501"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "filter = ['float16', 'float32', 'float64']  # say we are interested in these data types \n",
    "start = time.time()\n",
    "# Option 1: using select_dtypes\n",
    "num_vars = df.select_dtypes(include=filter).columns.tolist()\n",
    "end = time.time()\n",
    "t1 = end-start\n",
    "print('That took {:f} sec'.format(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXejQ2bwS-UV",
    "outputId": "44c44a03-9fdf-4797-d81b-b301a4f70aeb"
   },
   "outputs": [],
   "source": [
    "# Option 2: using list comprehension\n",
    "start = time.time()\n",
    "num_vars2 = [x for x in df.columns if df[x].dtype in filter]\n",
    "end = time.time()\n",
    "t2 = end-start\n",
    "print('That took {:f} sec'.format(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dU64Y7GXS-UV",
    "outputId": "b8e4684a-56ab-45b6-d3b8-9898e697d1cf"
   },
   "outputs": [],
   "source": [
    "# and yes, the result is the same\n",
    "print(num_vars)\n",
    "print(num_vars2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iGJBEi4S-UW"
   },
   "source": [
    "Should you be interested in some more information on memory efficiency, have a look at this post at [TowardDataScience.com](https://towardsdatascience.com/pandas-save-memory-with-these-simple-tricks-943841f8c32). Likewise, list comprehension is used a lot in Python, so you might want to make your peace with it. Here is a great post explaining the concept with awesome visuals: [Parul Pandey (2019): Comprehending the ‘Comprehensions’ in Python](https://towardsdatascience.com/comprehending-the-concept-of-comprehensions-in-python-c9dafce5111). Here is just another very useful [post including some background information](https://realpython.com/list-comprehension-python/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_RTCP2sS-UW"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ypE7GuOS-UW"
   },
   "source": [
    "### Missing values\n",
    "You might have already noticed that our data contains many missing values. This is common when working with real data. Likewise, handling missing values is a standard task in data preparation. Let's learn how Pandas supports handling missing values.  The function `.isnull()` is the entry point to the corresponding functionality and helps with identifying the relevant cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeahGMfES-UW",
    "outputId": "9141fa4f-01b3-4f3f-fa63-5e744bcb1c5f"
   },
   "outputs": [],
   "source": [
    "# Boolean mask to access missing values\n",
    "df.isnull()\n",
    "# Note that the above is equivalent to\n",
    "df.isna()\n",
    "# Both versions are available in Python to sustain backward compatibility. We suggest you select one and stick to it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aBde7TBS-UW"
   },
   "source": [
    "We can easily count the total number of missing values as shown below. Pause here and make sure you fully understand why we call the `sum()` function twice. I strongly recommend debugging the code. For example, inspect the output after removing the second call to `sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_DgQ7QHS-UW",
    "outputId": "b931d0ae-fece-4d25-d8d9-0a352b4da582"
   },
   "outputs": [],
   "source": [
    "# count the total number of missing values\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_9jyQY_S-UW"
   },
   "source": [
    "The plotting library *seaborn* allows us to create an easy to digest overview of the *missingness* in a data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggtNOCJKS-UW",
    "outputId": "c66c163d-4ae4-46ec-978a-49cca891d75d"
   },
   "outputs": [],
   "source": [
    "sns.heatmap(df.isnull(), cbar=False);  # quick visualization of the missing values in our data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vnw8ySRgS-UW"
   },
   "source": [
    "### Categorical features\n",
    "Let's start with the two categorical features. The heatmap suggests that `REASON` exhibits more missing values than `JOB`. We will treat them differently for the sake of illustration. Now that we start altering our data frame more seriously, it is a good idea to make a copy of the data so that we can easily go back to a previous state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuSqLNgzS-UW"
   },
   "outputs": [],
   "source": [
    "# copy data\n",
    "df_orig = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7nqgRZuS-UX"
   },
   "source": [
    "One way to treat missing values in a categorical feature is to simply introduce a new category level. We will demonstrate this approach for the feature *REASON*. One feature of the category data type in Pandas is that category levels are managed. In a nutshell, we cannot change data or enter new data arbitrarily. Before assigning the missing values our new category level *IsMissing*, we first need to introduce this level. We basically tell our data frame that *IsMissing* is another suitable entry for *REASON* next to the levels that already exist in the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdBWbWtfS-UX",
    "outputId": "b0338de2-ff64-4ff7-d922-1142f7f92879"
   },
   "outputs": [],
   "source": [
    "# Variable REASON: we treat missing values as a new category level.\n",
    "# First we need to add a new level\n",
    "df.REASON = df.REASON.cat.add_categories(['IsMissing'])\n",
    "\n",
    "# Now we can do the replacement\n",
    "df.REASON[df.REASON.isnull() ] = \"IsMissing\"\n",
    "df.REASON.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXAjFQFtS-UX",
    "outputId": "b5d3bf61-c04e-4d4f-a5f0-d345f8d3b184"
   },
   "outputs": [],
   "source": [
    "df.REASON.isna().sum()  # verify that no more missing values exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tf_LOVmPS-UX"
   },
   "source": [
    "Ok, that worked. On to the next category. \n",
    "<br>\n",
    "For the feature *JOB*, which is multinomial, we will use a different approach for illustration. We replace missing values with the mode (most frequent category level; just in case). That is actually a crude way to handle missing values but its used quite a lot. I'm not endorsing it! But you should have at least seen a demo. Here it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTNUN63yS-UX",
    "outputId": "a5ecc720-55f7-48b2-967a-27b387f50ad1"
   },
   "outputs": [],
   "source": [
    "# Determine the mode\n",
    "df.JOB.mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0l3WmkpS-UY",
    "outputId": "7956786a-76a5-4a6b-d5b9-3d0da18952c6"
   },
   "outputs": [],
   "source": [
    "# Alternatively\n",
    "df.JOB.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4M32XJxQS-UY",
    "outputId": "a6d86590-c2d8-4716-adcf-9fa8e258b3ac"
   },
   "outputs": [],
   "source": [
    "# replace missing values with the mode\n",
    "df.JOB[df.JOB.isnull() ] = df.JOB.mode()[0]  # the index [0] ensures that we only extract the value from the result of calling mode()\n",
    "df.JOB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4SvhbFnS-UY",
    "outputId": "ecb4b0ef-ca74-4890-cc24-0b46e0738747"
   },
   "outputs": [],
   "source": [
    "# Verify mising value replacement was successful\n",
    "if df.REASON.isnull().any() == False and df.JOB.isnull().any() == False:\n",
    "    print('well done!')\n",
    "else:\n",
    "    print('ups')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXZaNFpGS-UZ"
   },
   "source": [
    "### Numerical features\n",
    "We have a lot of numerical features. To keep things simple, we simply replace all missing values with the median. Again, this is  a crude approach that should be applied with utmost care if at all. However,  it nicely shows how we can process several columns at once using a loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjlpGozNS-UZ"
   },
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='float32').columns:  # loop over all numeric columns\n",
    "    if df[col].isna().sum() > 0:                         # check if there are any missing values in the current feature\n",
    "        m = df[col].median(skipna=True)                  # compute the median of that feature\n",
    "        df[col].fillna(m, inplace=True)                  # replace missing values with the median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should you wonder whether it is necessary to write a loop to perform this rather standard operation, impute missing values across all numerical features using the median, the answer is no. You could achieve the same result with the following approach that combines the `fillna()` method with a call to the method `transform()`. \n",
    "\n",
    "Specifically, `transform()` applies a function to each column of the DataFrame. The lambda function takes each column, fills the missing values with the median of that column, and returns the transformed column. This way, you avoid looping over each column manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach to impute missing values with the feature median\n",
    "cols = df.select_dtypes(include='float32').columns \n",
    "\n",
    "df[cols] = df[cols].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dukR_VzGS-UZ",
    "outputId": "13f49cee-b983-4e17-ddb3-ebef2477dff7"
   },
   "outputs": [],
   "source": [
    "# see if it worked, count again\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73xkUP7lS-Uc"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqvuqkp6S-Uc"
   },
   "source": [
    "## Explanatory data analysis\n",
    "Now that we are familiar with Pandas and have prepared out data, at least rudimentary, we can explore some of the plotting capabilities in Python. To that end, we go through an EDA pipeline and try to improve our understanding of the data along the way.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRBDvKXvS-Uc"
   },
   "source": [
    "### Univariate analysis of categorical variables\n",
    "In this part, we will examine  our target variable 'BAD', as well as the two categorical variables 'REASON' and 'JOB' individually. Firstly, we will count how many observations belong to each category of a variable. The function `value_counts()`, which we already saw above is probably the best choice to do that. We can also use it to process all categories in one go, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5q0yY6RS-Uc",
    "outputId": "8f514d55-0460-44bd-b007-f351e0f2d2f4"
   },
   "outputs": [],
   "source": [
    "df.select_dtypes(exclude='float32').apply(pd.Series.value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNCeqECfS-Uc"
   },
   "source": [
    "<br>\n",
    "While tables are a useful way to inspect data, graphs are often easier to interpret and more appealing. For categories, count plots and stacked count plots are common vehicles for data exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCTarhjMS-Uc",
    "outputId": "f7573dd2-436f-41e3-f66e-7a5d073356ee"
   },
   "outputs": [],
   "source": [
    "# exluding data type float leaves us with the target variable and both categorical variables\n",
    "for i, col in enumerate(df.select_dtypes(exclude='float32').columns):\n",
    "    plt.figure(i)\n",
    "    sns.countplot(x=col, data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4lMtwdLS-Ud"
   },
   "source": [
    "### Univariate analysis of numeric variables\n",
    "Let us now take a closer look at the numeric variables and their distribution by means of histograms. Creating a histogram is easily achieved using the `hist()` function, which Pandas offers. We will use it below.\n",
    "Dedicated plotting libraries offer a bit more flexibility. For start, we showcase functionality of the `Seaborn` library. We recommend this version when you examine a single variable at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eipAF4cxS-Ud",
    "outputId": "d303ce35-716d-4529-9b8f-22a235931bb9"
   },
   "outputs": [],
   "source": [
    "# Demo of a histogram using Seaborn\n",
    "sns.displot(df['CLAGE']);  # Explore the arguments that the function supports to discober variants of the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRw9FxtLS-Ud"
   },
   "source": [
    "Next, we consider a basic histogram and use Pandas functionality to produce an overview of all the numeric variables in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7zOw6tbS-Ud",
    "outputId": "801aae5a-4558-4faf-e175-deaf50d26953"
   },
   "outputs": [],
   "source": [
    "# We create one histogram for each numeric variable and illustrate how to set the number of bins\n",
    "df.select_dtypes(include='float32').hist(bins=20, figsize=(12,8))\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NU1s-CMqS-Ud"
   },
   "source": [
    "We observe many variables to have a long tail. This is very common in real data. Outliers also seem to be an issue in this data set. To get a clearer view on outliers let's inspect boxplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuVfCzxrS-Ud",
    "outputId": "f7379889-e7be-451e-ad56-63b5668b752d"
   },
   "outputs": [],
   "source": [
    "# Nice way to do it quickly\n",
    "num_vars = df.select_dtypes(include=\"float32\")\n",
    "df.boxplot(column=list(num_vars), figsize=(12,8))\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the little amount of code in the above example is clearly nice, it is immediately clear that the above plot suffers from the fact that the numerical variables have different scale. Standardization could be a way to remedy this issue and we will look at it later. For now, we can extend our code to create one boxplot for each variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUaL4xDqS-Ud",
    "outputId": "2cdb7cbf-3ede-4b49-948a-1d37e40257dc"
   },
   "outputs": [],
   "source": [
    "# One boxplot for each variable\n",
    "num_vars = df.select_dtypes(include=\"float32\")\n",
    "half = np.ceil(num_vars.shape[1] / 2).astype(int)  # for cosmetic reasons we split the plots into two rows\n",
    "num_vars.iloc[:, 0:half].plot(kind = 'box', subplots=True);\n",
    "num_vars.iloc[:, half:num_vars.shape[1]].plot(kind = 'box', subplots=True)\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8A7eMOpDS-Ud"
   },
   "source": [
    "#### Outlier handling\n",
    "Ok, so outliers are indeed a problem. We will truncate them based on Tukey's rule of thumb. At this point, we can finally come back to the `.apply()` function and provide a more meaningful example of how to use it. Specifically, we implement a function that takes care of outlier truncation. We then `.apply()` this function to our data. In comparison to a loop, which facilitates the same type of operation, `.apply()` may be more and leads to more readable code. It is fair to say that `apply()` is often preferred over loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oay-SE6S-Ue"
   },
   "outputs": [],
   "source": [
    "# we first need our function to handle outliers \n",
    "def outlier_truncation(x, factor=1.5):\n",
    "    '''\n",
    "    Identifies outlier values based on the inter-quartile range IQR. \n",
    "    Corresponding outliers are truncated and set to a contant value equal to the IQR\n",
    "    times a factor, which, following Tuckey's rule, we set to 1.5 by default\n",
    "    \n",
    "        Parameters:\n",
    "            x (Pandas Series): A data frame column to scan for outliers\n",
    "            factor (float): An outlier is a value this many times the IQR above q3/below q1\n",
    "            \n",
    "        Returns:\n",
    "            Adjusted variable in which outliers are truncated\n",
    "    '''\n",
    "    x_new = x.copy()\n",
    "    \n",
    "    # Calculate IQR\n",
    "    IQR = x.quantile(0.75) - x.quantile(0.25) \n",
    "    \n",
    "    # Define upper/lower bound\n",
    "    upper = x.quantile(0.75) + factor*IQR\n",
    "    lower = x.quantile(0.25) - factor*IQR\n",
    "    \n",
    "    # Truncation\n",
    "    x_new[x < lower] = lower.astype(np.float32)  # downcasting to float32 is needed to ensure\n",
    "    x_new[x > upper] = upper.astype(np.float32)  # compatibility with how we store the data in our data frame \n",
    "    \n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBvJahV5S-Ue"
   },
   "source": [
    "We could simply apply the above function to every numeric variable. This would work. However, if you go back to the boxplots, and look carefully, you may notice that some variables need special treatment. Consider the variable `DEROG`. If you simply truncate outliers in this variable, the result will be a constant. Make sure you understand why this is the case. We will leave such special cases for later and process the other less troublesome variables using our outlier truncation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isVQNt3MS-Ue",
    "outputId": "87397a11-6c1f-43c7-ba01-2e30dbe6948e"
   },
   "outputs": [],
   "source": [
    "# Select numeric variables for outlier treatment. We could also use select_dtypes(). Do so if you prefer.\n",
    "# The programming construct we use is called list comprehension and quite common in Python. Think of it\n",
    "# as a more efficient way of writing loops\n",
    "num_cols = [x for x in df.columns if df[x].dtype == 'float32' and x not in ['DEROG', 'DELINQ']]  \n",
    "\n",
    "# Process every selected column using apply\n",
    "# Updated 10.06.20 to show passing arguments to the 'applied' functions. Just send a tupel with arguments in the order as specified\n",
    "# by the called function leaving out the first argument (see, https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.apply.html)\n",
    "df[num_cols] = df[num_cols].apply(outlier_truncation, axis=0, args=(3,))  \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWGZ60BqS-Ue"
   },
   "source": [
    "Note how the maximum values have changed. You can also re-run the above code to create the boxplots and convince yourself that we have mitigated the outlier problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H93dSkr5S-Ue"
   },
   "source": [
    "#### Discretization of Numeric Variables\n",
    "\n",
    "As seen in the boxplots above, we are facing some issues with the distribution of `DEROG` and `DELINQ`. Given their distribution, it seems appropriate to discretize these variables.  Discretization is the process of converting a numeric variable into a discrete variable, i.e., a category. Since both, `DEROG` and `DELINQ` display a large number of zeros, we could, for example, consider one category level *isZero* and another *IsGreaterThenZero*. This would give a binary variable. We can also introduce more category levels to obtain a fine-grained categorical representation of the original numbers. Normally, the function `qcut()` is a good choice to discretize a variable based on quantiles. We cannot demonstrate this with `DEROG` and `DELINQ` since their distributions do not warrant using quantiles as split points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6XPDiCsS-Ue",
    "outputId": "fad363c3-c9d8-4a76-a764-b34a1e4a5334"
   },
   "outputs": [],
   "source": [
    "df.DEROG.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxadmoWtS-Ue",
    "outputId": "4b786225-1ee7-41a4-a56f-0b9396fc610c"
   },
   "outputs": [],
   "source": [
    "# Just an example to show the inappropriatness of using qcut for DEROG\n",
    "pd.qcut(df.DEROG, q=1)  # any value for q > 1 will raise an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYTsvL_4S-Ue",
    "outputId": "0e5e695e-36e2-404f-9be0-3214b4461112"
   },
   "outputs": [],
   "source": [
    "# Whereas with more distinct values, qcut would work just fine, e.g.\n",
    "pd.qcut(df.CLAGE, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnlyB9iYS-Ue"
   },
   "source": [
    "Since quantile-based discretization is not an option, we go for a manual approach and start by a close look at the value counts for our two \"problem variables\". Category levels that occur very rarely can not help distinguish the good and bad payers. There are just too few observations to learn from. So, we inspect the frequency counts and decide which levels to merge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naggM0uGS-Ue",
    "outputId": "ebb68d63-1664-40c9-8547-d9dc5ca56984"
   },
   "outputs": [],
   "source": [
    "df.DELINQ.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VscfWYodS-Uf",
    "outputId": "61e438c8-7935-4365-ff95-395acb7688a7"
   },
   "outputs": [],
   "source": [
    "df.DEROG.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzQOOSgoS-Uf"
   },
   "source": [
    "How you proceed from here is based on judgment. We will exemplify two different techniques that could be considered in the focal case. For one variable we will create a dummy, indicating whether or not the value of the variable is 0. For the other variable we will group into three categories. As \"DELINQ\" shows fewer observations for the value 0, we will use this variable to divide into three groups: 0, 1 & >1. If you wish, this is a manual, or expert-based version of discretization in which we pick the boundaries of the buckets manually instead of picking them by looking at quantiles.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8h-5OQxlS-Uf",
    "outputId": "5c9976ff-da8c-416f-af07-51d634df6e73"
   },
   "outputs": [],
   "source": [
    "df['DELINQcat'] = '1+' #set default value to +1 for new variable \n",
    "df.loc[(df['DELINQ'] == 1), 'DELINQcat'] = '1' # change this value to 1, if value of DELINQ is 1\n",
    "df.loc[(df['DELINQ'] == 0), 'DELINQcat'] = '0'\n",
    "df['DELINQcat'] = df['DELINQcat'].astype('category')  # convert to categorical\n",
    "df.DELINQcat.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQ_m0iuIS-Uf"
   },
   "source": [
    "We will proceed similarly with the variable \"DEROG\". We will create a dummy variable where every observation has the value 1 (true) if their value for the \"DEROG\" variable was 0. Every other value will be assigned the value 0 (false)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAHjbl7GS-Uf",
    "outputId": "fcf8f961-8cd3-4412-ceb5-89df7a8c5916"
   },
   "outputs": [],
   "source": [
    "df['DEROGzero'] = 0 #set default to 0\n",
    "df.loc[(df['DEROG'] == 0), 'DEROGzero'] = 1 #change to 1 if value of \"DEROG\" is 0 \n",
    "df['DEROGzero'] = df['DEROGzero'].astype('bool')\n",
    "df.DEROGzero.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sOJcgT6S-Uf"
   },
   "source": [
    "We are not done with our EDA workflow. However, we will not make any more changes to the data. Therefore, it is a good time to save a copy of the prepared data so that we can re-use it in subsequent coding sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "raz9DoOYS-Uf"
   },
   "outputs": [],
   "source": [
    "# Store the data in the present, pre-processed format\n",
    "df.to_csv('./hmeq_prepared.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9hGA2X6S-Uf"
   },
   "source": [
    "### Multivariate analysis of numeric variables\n",
    "\n",
    "After gaining more knowledge about the variables individually, it is important to examine their relationships more closely. In data science, this is a good way of identifying redundant information as well as variable interactions. \n",
    "Next, we will plot a heatmap. It shows the correlation for all numeric variables. Highly correlated variables are redundant as they convey the same pieces of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QljTR_vgS-Uf",
    "outputId": "22cffb3c-7649-4245-8886-041ae36879cb"
   },
   "outputs": [],
   "source": [
    "corr= df.select_dtypes(include=np.float32).corr()\n",
    "f,ax = plt.subplots(figsize=(18, 15))\n",
    "sns.heatmap(corr ,annot=True,linewidth=.5,fmt='1f');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDaPNcK2S-Ug"
   },
   "source": [
    "Due to the amount of variables, it can be hard to make sense of the plot and spot the important variable relationships. We can solve this by filtering from a specific threshold. For example, we consider a threshold of $\\rho=0.3$ below, and focus the plot to those variables whose pairwise correlation exceeds this threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Twsigh5dS-Ug",
    "outputId": "ba6fcd1c-392c-43c2-f872-14be638ae628"
   },
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(18, 15))\n",
    "sns.heatmap(corr[(corr >= 0.30) | (corr <= -0.30)],\n",
    "            annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiqtYpksS-Ug"
   },
   "source": [
    "If you have trust in the threshold, the above chart makes variable selection easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sDH0mo-S-Ug"
   },
   "source": [
    "### Multivariate analysis of categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZj8d8iCS-Ug"
   },
   "source": [
    "Categories and the binary target in particular are also useful to examine sub-groups. For example, we could calculate the mean of a/all numeric variables for good and bad borrowers. Enter `.groupby()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeVyOzUIS-Ug",
    "outputId": "a0f8a2d1-6b7b-4130-d638-67d9aad09b2d"
   },
   "outputs": [],
   "source": [
    "df.groupby(\"BAD\").mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAewG6Y5S-Ug"
   },
   "source": [
    "Another standard operation when exploring categorical variables is to check cross-tabulations. Considering, for example, the variables `Reason`and `Job`, we can create a cross-tab as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwHH1-CUS-Ug",
    "outputId": "74aed1b0-1ecd-4712-a8cf-77439e214a8d"
   },
   "outputs": [],
   "source": [
    "pd.crosstab(df.REASON, df.JOB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SARAxO_S-Ug"
   },
   "source": [
    "A cross-tab can be particularly informative when it includes the target variable. This way, we can spot whether certain category levels of the other (independent) variable are especially prominent with good or bad borrowers. In this use case, we would also want to switch from showing counts (as above) to showing relative frequencies. We achieve this by augmenting our call to `crosstab()` with the argument `normalize='index'`. Have a look into the [documentation of the function](https://pandas.pydata.org/docs/reference/api/pandas.crosstab.html) to understand why we select the option `'index'` for the function argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yICXsDpS-Ug",
    "outputId": "464f9f13-ea31-4460-dcef-2032e3d862b9"
   },
   "outputs": [],
   "source": [
    "job = pd.crosstab(df.JOB, df.BAD, normalize='index')\n",
    "job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kj5VCMO3S-Uh"
   },
   "source": [
    "Once again, we can also report the same information in a graphical way. A common way to display categorical variables is the stacked count plot. Let us analyze the variables `REASON` and `JOB` and how they are linked to the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsJ9-EPkS-Uh",
    "outputId": "18e8ac61-bfd6-4efe-d535-d21ef0f9f429"
   },
   "outputs": [],
   "source": [
    "reason = df.groupby(['BAD', 'REASON'], observed=True).size().reset_index().pivot(columns='BAD', index='REASON', values=0)\n",
    "                                                                       \n",
    "reason.plot(kind='bar', stacked=True); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAh2YJoiS-Uj"
   },
   "source": [
    "Let's do it one more time, this time showing the relative frequencies instead of the absolute counts. This might provide more insight into the variable distribution and how it differs across the two target groups (i.e., goods and bads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oosCIfBdS-Uj",
    "outputId": "8838b92d-a81b-4710-c527-a827a1c04a95"
   },
   "outputs": [],
   "source": [
    "job.div(job.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\n",
    "reason.div(reason.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAb1s-9SS-Uk"
   },
   "source": [
    "### Interactions between numeric and categorical variables \n",
    "Next, we can have a look at the distribution of our categories across the numerical variables. Violin plots are a great way to do so. The *seaborn* library makes creating these plots very easy. Below, we illustrate two options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CsH-GbUWS-Uk",
    "outputId": "d5c15520-45ba-46b6-b7f1-89fcdfddef33"
   },
   "outputs": [],
   "source": [
    "# We use the category 'REASON' and create one plot for each numeric variable\n",
    "for col  in df.select_dtypes(include='float32').columns:\n",
    "    plt.figure()\n",
    "    sns.violinplot(x='REASON', y=col, hue='BAD',\n",
    "                   split=True, inner=\"quart\",\n",
    "                   data= df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FSC9grnS-Uk"
   },
   "source": [
    "Let's repeat this for the variable 'JOB' but using a different approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BB0RalBlS-Uk",
    "outputId": "b69a3320-7334-4f8e-804f-60288494d90e"
   },
   "outputs": [],
   "source": [
    "# Repeat for category JOB\n",
    "fig, axs = plt.subplots(3,3, figsize=(15, 10))\n",
    "plt.tight_layout(pad=0.5, w_pad=4, h_pad=1.0)  \n",
    "x = df.JOB\n",
    "\n",
    "sns.violinplot(x=x, y=\"LOAN\",  data=df,ax=axs[0,0])\n",
    "sns.violinplot(x=x, y=\"MORTDUE\", data=df,ax=axs[0,1])\n",
    "sns.violinplot(x=x, y=\"VALUE\", data=df,ax=axs[0,2])\n",
    "sns.violinplot(x=x, y=\"YOJ\", data=df,ax=axs[1,0])\n",
    "sns.violinplot(x=x, y=\"DEROG\", data=df,ax=axs[1,1])\n",
    "sns.violinplot(x=x, y=\"CLAGE\", data=df,ax=axs[1,2])\n",
    "sns.violinplot(x=x, y=\"NINQ\", data=df,ax=axs[2,0])\n",
    "sns.violinplot(x=x, y=\"CLNO\", data=df,ax=axs[2,1])\n",
    "sns.violinplot(x=x, y=\"DEBTINC\", data=df,ax=axs[2,2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UY0vPyHxvsyD"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Learning how to display data in a meaningful and engaging way is an art as is good data pre-processing. You have to practice to become good at it. There are many more functionalities in the Python world which may come in handy. Keep up with updates on libraries and find some blogs whose graphics you find very easy to understand. You can practice implementing their code for your purposes. The possibilities are endless!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdaOAlZTTEZT"
   },
   "source": [
    "## Table of Useful Tricks\n",
    "\n",
    "Many useful tricks with `pandas` (here `df` is a pandas DataFrame and `col` is one of its columns):\n",
    "\n",
    "| Goal | Possible Code |\n",
    "| --- | --- |\n",
    "| Get df column (column name must have no spaces) | `df.col` |\n",
    "| Get df column | `df[\"col\"]` |\n",
    "| Example condition: only select rows where `col1 > 1` | `df[\"col\"] > 1` |\n",
    "| Use index names to select rows and columns | `df.loc[row_list, col_list]` |\n",
    "| Use index numbers to select rows and columns | `df.iloc[row_list, col_list]` |\n",
    "| Get df column based on a condition | `df.loc[condition, ['col2','col3',...]]`|\n",
    "| Group df by values of `col` | `df.groupby(\"col\")` |\n",
    "| Perform function on `col2` for each group of `col1` | `df.groupby(\"col1\")[\"col2\"].fun()` |\n",
    "| Find value counts of each value in `col` | `df.groupby(['col']).size()`| \n",
    "| Get column mean and ignore null values | `df[\"col\"].mean(skipna=True)` |\n",
    "| Get column mode | `df[\"col\"].mode()` |\n",
    "| Get column median | `df[\"col\"].median()` |\n",
    "| Get rows of the 95th quantile of `col` | `df[\"col\"].quantile(q=0.95)` |\n",
    "| Filter `df` with a boolean condition | `df.query(condition)` |\n",
    "| Create tally of `col2` by values of `col1` | `pd.crosstab(df['col1'], df['col2']`) |\n",
    "| Pivot rows and columns | `df.pivot(index='col1', columns='col2', values='col3')` | \n",
    "| Sort values by `col` and save `df` in this order | `df.sort_values(by='col', inplace=True)` |\n",
    "| Apply function to each column of `df` | `df.apply(fun)` |\n",
    "| Save `df` as CSV in working directory | `df.to_csv('./file_name.csv', index=False)` |\n",
    "| Count the number of times each value occurs | `df['col'].value_counts()` |\n",
    "| Change column's data type | `df['col'] = df['col'].astype('type')` |\n",
    "| Create boolean matrix of `df` where `True` indicates null value | `df.isnull()` | \n",
    "| Create boolean matrix of `df` where `True` indicates null value | `df.isna()` | \n",
    "| Create copy of df | `df_copy = df.copy()` |\n",
    "| Add new category to categorical variable | `df.col.cat.add_categories(['New C'], inplace=True)` |\n",
    "| Replace null values with `\"IsMissing\"` | `df.col[df.col.isnull()] = \"IsMissing\"` |\n",
    "| Fill missing values with median and save `df` | `df['col'].fillna(median_value, inplace=True)` |\n",
    "| Calculate time at execution (must import `time` library) | `time.time()` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots for interesting visualizations:\n",
    "\n",
    "| Goal | Possible Function |\n",
    "| --- | --- |\n",
    "| Heatmap of null values | `sns.heatmap(df.isnull(), cbar=False)` |\n",
    "| Colored violin plot | `sns.violinplot(x='col1', y=col2, hue='col3', data= df)` |\n",
    "| Density of two variables | `sns.jointplot(x='col1',y='col2',data=df, kind='kde')` |\n",
    "| Swarmplot | `sns.swarmplot(df['col1'], df['col2'])` |\n",
    "| Scatterplot shaded by third variable | `df.plot.scatter(x='col1',y='col2',c='col3')` |\n",
    "\n",
    "* `sns` is the most common `seaborn` library abbreviation. It must be imported before calling its functions."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "4_nb_data_preparation (2).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "bads2526",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
