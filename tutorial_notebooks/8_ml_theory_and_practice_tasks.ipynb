{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97f60c7",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorial_notebooks/8_ml_theory_and_practice_tasks.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd632d8e",
   "metadata": {},
   "source": [
    "# Tutorial 8 - Machine Learning Theory & Practice\n",
    "In this tutorial, we revisit the ML Theory & Practice session of our BADS lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f867e0",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d210c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set size (samples x features): 5960 x 29.\n",
      "Logistic Regression test set AUC (MSE): 0.906(0.081)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load preprocessed HMEQ data from GitHub\n",
    "data_url = 'https://raw.githubusercontent.com/Humboldt-WI/bads/master/data/hmeq_prepared.csv'\n",
    "df = pd.read_csv(data_url)\n",
    "X = df.copy() # Separate features and target\n",
    "y = X.pop(\"BAD\")\n",
    "\n",
    "# Data partitioning using the holdout method\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Train a simple logistic regression model as benchmark\n",
    "lr = LogisticRegression(max_iter=1000, random_state=123).fit(X_train, y_train)\n",
    "yhat_lr = lr.predict_proba(X_test)[:, 1]\n",
    "auc_lr = roc_auc_score(y_test, yhat_lr)\n",
    "mse_lr = mean_squared_error(y_test, yhat_lr)\n",
    "print(f\"Data set size (samples x features): {X.shape[0]} x {X.shape[1]}.\")\n",
    "print(f\"Logistic Regression test set AUC (MSE): {auc_lr:.3f} ({mse_lr:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91904e69",
   "metadata": {},
   "source": [
    "# Bias, Variance and Overfitting\n",
    "<p align=\"left\">\n",
    "  <img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/overfitting.png\" alt=\"Bias, variance, and overfitting\" width=\"640\" />\n",
    "</p>\n",
    "\n",
    "Image source: [Geeks for Geeks](https://www.geeksforgeeks.org/machine-learning/ml-bias-variance-trade-off/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb489532",
   "metadata": {},
   "source": [
    "## Overfitting in neural networks\n",
    "\n",
    "### An arbitrary neural network model\n",
    "- Train a neural network and assess its performance on training and test data. \n",
    "- Compare the results to those of the logistic regression benchmark model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb570b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "674087f8",
   "metadata": {},
   "source": [
    "\n",
    "### Training error evolution\n",
    "\n",
    "#### The influence of training epochs\n",
    "In this part, we try to reproduce the above illustration, approximating the complexity of the NN by the **number of training iterations**. \n",
    "\n",
    "- Vary the number of training epochs of the neural network from 1 to 1000 in steps of 50.\n",
    "- For each configuration, train the model on the training data and evaluate its performance (AUC and MSE) on both training and test data.\n",
    "- Plot the training and test performance against the number of training iterations.\n",
    "  - Display your results in 1 x 2 grid of two charts:\n",
    "  - Let the first chart measure model performance by MSE\n",
    "  - Let the second chart measure model performance by 1-AUC (i.e., to transform AUC as an error measure) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de651be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24878184",
   "metadata": {},
   "source": [
    "#### The influence of model complexity\n",
    "In this part, we try to reproduce the above illustration, approximating the complexity of the NN by the **number of weights**, which are a function of the number of layers and the size of those layers. So we will have to train multiple NNs with different architecture.\n",
    "\n",
    "Apart from this modification, the steps are similar to the previous task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d3cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "B0rxPs4QEGtz",
    "27sCENzmoGcX"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "bads310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
