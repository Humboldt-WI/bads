{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97f60c7",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorial_notebooks/8_ml_theory_and_practice_tasks.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd632d8e",
   "metadata": {},
   "source": [
    "# Tutorial 8 - Machine Learning Theory & Practice\n",
    "In this tutorial, we revisit the ML Theory & Practice session of our BADS lecture, beginning with revisiting the bias-variance trade-off and how the holdout method and cross-validation help us detect overfitting. Afterwards, we look into regularization as a technique to address overfitting and discuss hyperparameter optimization. For these tasks, we use our HMEQ datasets and neural networks as our primary learning algorithm. \n",
    "\n",
    "Here is the outline of this tutorial:\n",
    "1. [Bias, Variance and Overfitting](#bias-variance-and-overfitting)\n",
    "2. [Regularization](#regularization)\n",
    "3. [Hyperparameter Optimization](#hyperparameter-optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f867e0",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Switch off convergence warnings from the NN for cleaner output\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Load preprocessed HMEQ data from GitHub\n",
    "data_url = 'https://raw.githubusercontent.com/Humboldt-WI/bads/master/data/hmeq_prepared.csv'\n",
    "df = pd.read_csv(data_url)\n",
    "X = df.copy() # Separate features and target\n",
    "y = X.pop(\"BAD\")\n",
    "\n",
    "# Define the initialization value for random numbers (for reproducibility)\n",
    "seed = 888\n",
    "\n",
    "# Data partitioning using the holdout method\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Train a simple logistic regression model as benchmark\n",
    "lr = LogisticRegression(max_iter=100, random_state=seed).fit(X_train, y_train)\n",
    "yhat = lr.predict_proba(X_test)[:, 1]  # test set predictions\n",
    "yhat_tr = lr.predict_proba(X_train)[:, 1]  # train set predictions\n",
    "\n",
    "# Store performance as a nested dictionary\n",
    "error_lr = {'Train': {'AUC': roc_auc_score(y_train, yhat_tr),\n",
    "                     'MSE': mean_squared_error(y_train, yhat_tr) },\n",
    "            'Test': {'AUC': roc_auc_score(y_test, yhat),\n",
    "                     'MSE': mean_squared_error(y_test, yhat) }}\n",
    "\n",
    "# Some output\n",
    "print(f\"Data set size (samples x features): {X.shape[0]} x {X.shape[1]}.\")\n",
    "print(f\"Logistic Regression train set AUC (MSE): {error_lr['Train']['AUC']:.3f} ({error_lr['Train']['MSE']:.3f})\")\n",
    "print(f\"Logistic Regression test  set AUC (MSE): {error_lr['Test']['AUC']:.3f} ({error_lr['Test']['MSE']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91904e69",
   "metadata": {},
   "source": [
    "# [Bias, Variance and Overfitting](#bias-variance-and-overfitting)\n",
    "The *well-known* illustration of the bias-variance trade-off and overfitting is shown below. As discussed, it shows the theoretical development of bias, variance, and various errors. In this part, we strive to reproduce the figure using our HMEQ data and a neural network using the class `MLPClassifier` from  `sklearn.neural_network`.\n",
    "  \n",
    "<p align=\"left\">\n",
    "  <img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/overfitting.png\" alt=\"Bias, variance, and overfitting\" width=\"640\" />\n",
    "</p>\n",
    "\n",
    "Image source: [Geeks for Geeks](https://www.geeksforgeeks.org/machine-learning/ml-bias-variance-trade-off/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0bfb3f",
   "metadata": {},
   "source": [
    "## MLP Classifier\n",
    "We use the `MLPClassifier` class from the `sklearn.neural_network` module to create and train our neural network, as demononstrated in the [Deep Learning tutorial](https://github.com/Humboldt-WI/bads/blob/master/tutorial_notebooks/7_deep_learning_tasks.ipynb). Below, we reproduce the relevant steps as a starting point for subsequent exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce7c416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a NN\n",
    "nn = MLPClassifier(random_state=888, max_iter=100)\n",
    "nn.fit(X=X_train, y=y_train)\n",
    "# Compute predictions\n",
    "yhat_nn = nn.predict_proba(X=X_test)[:, 1]  # get probability of class 1 only\n",
    "# Compute test set performance in, e.g., AUC\n",
    "auc_nn = roc_auc_score(y_true=y_test, y_score=yhat_nn)\n",
    "\n",
    "# Print performance of NN versus logistic regression (-> see preliminaries)\n",
    "print(f\"Test set AUC of the neural network is: {auc_nn:.3f}\")\n",
    "print(f\"Test set AUC of logistic regression baseline is: {error_lr['Test']['AUC']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb489532",
   "metadata": {},
   "source": [
    "## Overfitting in neural networks\n",
    "\n",
    "### The influence of training iterations\n",
    "In this part, we try to reproduce the above illustration, approximating the complexity of the NN by the **number of training iterations**. To that end, you need to solve the following programming tasks:\n",
    "\n",
    "#### Exercise 1\n",
    "- Vary the number of training epochs of the neural network from 50 to 1000 in steps of 50.\n",
    "- For each configuration, train the model on the training data and evaluate its performance in AUC and MSE on both training and test data.\n",
    "- Plot the training and test performance against the number of training iterations.\n",
    "  - Display your results in 1 x 2 grid of two charts:\n",
    "  - Let the first chart measure model performance by MSE\n",
    "  - Let the second chart measure model performance by 1-AUC (i.e., to transform AUC as an error measure) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de651be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 1: Varying the number of training iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c255d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24878184",
   "metadata": {},
   "source": [
    "### The influence of model complexity\n",
    "In this part, we try to reproduce the above illustration, approximating the complexity of the NN by the **number of weights**, which are a function of the number of layers and the size of those layers. So we will have to train multiple NNs with different architecture. Apart from this modification, the steps are similar to the previous task.\n",
    "\n",
    "\n",
    "#### Exercise 2\n",
    "- Vary the architecture of the neural network by changing the number of hidden layers and the number of neurons per layer.\n",
    "  - Recall that `MLPClassifier` expects a *Tuple* as input for the parameter `hidden_layer_sizes`. For example, you can configure a NN with 5 and 3 nodes in the first and second hidden layer, respectively, by setting `hidden_layer_sizes=(5,3)`. \n",
    "  - Design the NN in such a way that the total number of nodes increases across the architectures you consider.\n",
    "- For each NN architecture, train the model on the training data and evaluate its performance in AUC and MSE on both training and test data.\n",
    "- Collect the results (i.e., MSE and AUC) from each run.\n",
    "- Plot the training and test performance against the total number of nodes in your NN in 1 x 2 grid of two charts:\n",
    "  - Let the first chart measure model performance by MSE\n",
    "  - Let the second chart measure model performance by 1-AUC (i.e., to transform AUC as an error measure) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcfa4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 2: Varying the NN architecutre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80326e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd95a98f",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "Regularization is a technique to reduce overfitting by adding a penalty term to the loss function. In the context of the *bias-variance trade-off*, you can think of regularization as a way to introduce bias to reduce variance and ultimately achieve a better balance between the two. \n",
    "\n",
    "How to implement this idea? The answer depends on the type of learning algorithm. For regression-type models the answer is: add a complexity penalty to the loss. Large values of the regression coefficients are indicators of complex, unstable models. Possible causes include high dimensionality and multicollinearity. The regularized loss function as the form: \n",
    "\n",
    "$$ \\min\\limits_{\\beta} ‚Å°\\mathcal{L} (\\beta)+\\lambda|(\\beta)| $$\n",
    "\n",
    "This penalty produces sparser models as it forces the coefficients to zero. Furthermore, we also have a new hyperparameter $\\lambda$, which governs the strength of regularization. Simply put, $\\lambda$ embodies our preference for models that fit the training data more accurately (low $\\lambda$) or models that are less complex (high $\\lambda$). It is difficult to know suitable settings of $\\lambda$ a priori. Thus, we typically tune this *hyperparameter* for each data set. We will get to hyperparamater tuning later in the notebook.\n",
    "\n",
    "We have discussed common forms of regularization. Let's revisit those in more detail.\n",
    "\n",
    "## Lasso or L1 Regularization\n",
    "The Least Absolute Shrinkage and Selection Operator or LASSO set the penalty to equal to the sum of the absolute coefficients multiplied by some *shrinkage* value, denoted by $\\lambda$. The LASSO optimization problem can be written as:\n",
    "\n",
    "$$ \\arg\\min_{\\beta_j} \\sum_{i=1}^{n}(y_i - \\sum_{j}x_{ij} \\beta_j)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j | $$\n",
    "\n",
    "Note that it is **crucial to standardize numerical features when using Lasso**. Standardization is crucial to ensure that every feature, independent of the magnitude of common feature values (e.g., several thousands for a feature *salary* as opposed to small int value for a feature *years of education*) has the same chances of *surviving* lasso regularization. Here, survive means receiving a non-zero coefficient. Intuitively, shrinking the coefficient of a feature with large values (e.g., salary) will reduce the penalty more than shrinking the coefficient of a feature with smaller feature values (e.g., years of education). Lasso performs **embedded feature selection** meaning that it is likely to set some coefficients to zero, which is equivalent to discarding the feature. If two or more variables are highly correlated, lasso will arbitrarily select one of them and assign a non-zero coefficient, while the other correlated features are likely to receive a coefficient value of zero. This behavior is sometimes desirable, for example to remove *redundant features* but also cause problems, for example if the features are dummy codes originating from the same categorical variable.\n",
    "\n",
    "## Ridge or L2 Regularization\n",
    "Ridge regression is similar to lasso regression with the difference of the penalty being equal to the sum of the **square of the coefficients** multipled by some shrinkage value. Here is the objective function for L2 regularization:\n",
    "\n",
    "$$ \\arg\\min_{\\beta_j} \\sum_{i=1}^{n}(y_i - \\sum_{j}x_{ij} \\beta_j)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j^2 | $$\n",
    "\n",
    "Again, it is crucial to **standardize all continuous variables** before performing ridge regression. The reason is the same as for Lasso. Ridge regression never really sets coefficient values to zero due to the square. So, if all parameters are believed to hold some importance, it may be better to use this method of regularization. Note that ridge regression only works if the number of coefficients is less than the number of observations.\n",
    "\n",
    "## Elastic Net\n",
    "Elastic Net (enet) is a combination of L1 and L2 regularization. It is best used when an analyst is unsure whether ridge or lasso is more suitable for a given data set. The enet penalty specifies two separate shrinkage values. \n",
    "\n",
    "$$ \\arg\\min_{\\beta_j} \\sum_{i=1}^{n}(y_i - \\sum_{j}x_{ij} \\beta_j)^2 + \\lambda_1 \\sum_{j=1}^{p} |\\beta_j | + \\lambda_2 \\sum_{j=1}^{p} |\\beta_j^2 | $$\n",
    "\n",
    "One of the bigger disadvantages of enet is the computational cost required to find the best combination of the two shrinkage values. It is also important to check if lasso alone might have done a better job at preventing overfitting.\n",
    "\n",
    "## Working examples of regularized logistic regression\n",
    "The *sklearn* implementation of logistic regression supports L1, L2, and elastic net regularization. The relevant class is `LogisticRegression` from the module `sklearn.linear_model`. The type of regularization is specified via the parameter `penalty`, which can take the values `'l1'`, `'l2'`, and `'elasticnet'`. The strength of regularization is controlled by the parameter `C`, which is the inverse of the shrinkage value $\\lambda$. Thus, smaller values of `C` imply stronger regularization. When using elastic net regularization, you also need to specify the parameter `l1_ratio`, which controls the relative weight of L1 and L2 regularization. A value of `l1_ratio=0` implies pure L2 regularization, while a value of `l1_ratio=1` implies pure L1 regularization.\n",
    "\n",
    "Below, we train and evaluate a logistic regression model with L2 regularization on our HMEQ data, and compare the result to the baseline model, our logistic regression model `lr`, which was trained without was adding a regularization penalty to the loss function (see, [Preliminaries](#Preliminaries)). Note that not all solvers that *sklearn* offers for logistic regression support all types of regularization. For L1 regularization, we use the solver `'saga'`, which supports both L1 and elastic net regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc532798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression with L1 penalty\n",
    "lr_l1 = LogisticRegression(penalty='l1', solver='saga', C=.25, random_state=888) \n",
    "lr_l1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d1c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the estimated coefficients using a bar plot\n",
    "coef_df = pd.DataFrame({'Feature': X.columns,\n",
    "                        'Logistic Regression Coef.': lr.coef_.flatten(),\n",
    "                        'L1-Regularized Logistic Regression Coef.': lr_l1.coef_.flatten()})\n",
    "\n",
    "coef_df.set_index('Feature').plot(kind='bar', figsize=(12, 6))\n",
    "plt.title(\"Comparison of Coefficients: Logistic Regression vs. L1-Regularized Logistic Regression\")\n",
    "plt.ylabel(\"Coefficient Value\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1e6ec",
   "metadata": {},
   "source": [
    "We clearly see how the average magnitude of the coefficients is reduced when using L1 regularization. Furthermore, some coefficients are set to zero, indicating that those features were discarded by the regularization process. To get a better feeling for the effect of the hyperparameter `C`, it is useful to rerun the previous code multiple times with different settings of `C`. Recall that smaller values imply stronger regularization. \n",
    "\n",
    "### Exercise 3 Benchmarking regularized vs. non-regularized logistic regression\n",
    "This exercises involves systematically benchmarking different regression models to each other. Although the notebook already trained some models, it is best to start from scratch to have a self-contained exercise solution in the end. To achieve this: \n",
    "- Train four logistic regression models:\n",
    "  - A baseline model without regularization\n",
    "  - A model with L1 regularization  \n",
    "  - A model with L2 regularization\n",
    "  - A model with elastic net regularization\n",
    "  - Set relevant hyperparameters to reasonable values. You may want to experiment with different settings of `C` and `l1_ratio` (for elastic net).\n",
    "- Evaluate all four models on the test data using AUC and MSE as performance measures.\n",
    "- Create a ROC chart showing all four models to facilitate a comparision. We suggest you use the class `RocCurveDisplay` from the module `sklearn.metrics` for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbe3610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 3: Benchmarking regularized vs. non-regularized logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d47bede",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization\n",
    "A limitation of the benchmarking exercise above is that we had to set hyperparameters manually. In practice, it is often unclear which hyperparameter settings are best for a given data set. Thus, we need systematic ways to find good hyperparameter configurations. Hyperparameter optimization (HPO) is the process of finding the best hyperparameter configuration for a given learning algorithm and data set. Hyperparameters are parameters that are not learned from the data but are set before training. Examples include the learning rate, the number of hidden layers in a neural network, or the regularization strength in regression models. The goal of HPO is to find the hyperparameter values that lead to the best model performance on unseen data.\n",
    "\n",
    "## Grid search\n",
    "The lecture introduced grid search as a simple yet effective method for hyperparameter optimization. The idea is to define a grid of possible hyperparameter values and evaluate the model's performance for each combination of these values. The combination that yields the best performance on a validation set is selected as the optimal hyperparameter configuration.\n",
    "\n",
    "<p align=\"left\">\n",
    "  <img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/gridsearch.png\" alt=\"Grid search\" width=\"640\" />\n",
    "</p>\n",
    "\n",
    "The key class in *sklearn* for hyperparameter optimization using grid search is `GridSearchCV` from the module `sklearn.model_selection`. This class implements grid search with cross-validation to systematically explore a predefined set of hyperparameter values. Below, we demonstrate how to use `GridSearchCV` to optimize the hyperparameter `C` and the type of penalty for a regularized logistic regression model. Given that HPO is about finding a *best* model, we need to select a performance metric to optimize. Here, we use AUC as our performance metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b47ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "# grid of hyperparameter values for tuning\n",
    "C_values = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "penalties = ['l1', 'l2']\n",
    "\n",
    "gcv = GridSearchCV(\n",
    "    estimator=LogisticRegression(solver='saga', random_state=888),\n",
    "    param_grid={'C': C_values, 'penalty': penalties},  # the grid of hyperparameters to search\n",
    "    scoring='roc_auc',  # metric to optimize\n",
    "    cv=5,  # number of cross-validation folds\n",
    "    n_jobs=-1  # use all available CPU cores\n",
    "\n",
    ")\n",
    "# Start optimization\n",
    "gcv.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameter setting\n",
    "best_params = gcv.best_params_\n",
    "print(f\"Best hyperparameters found: {best_params}\")\n",
    "# Evaluate the best model on the test set\n",
    "best_model = gcv.best_estimator_\n",
    "\n",
    "RocCurveDisplay.from_estimator(best_model, X_test, y_test, name=\"Best Model\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10e5755",
   "metadata": {},
   "source": [
    "### Exercise 4 NN hyperparameter optimization\n",
    "Considering all previous results, the performance of the logistic regression models did not vary much. Your last exercise is to extend the hyperparameter optimization example above to include neural networks. More specifically, perform grid-search to identify a high-performing NN architecture by searching over the following hyperparameters: \n",
    "- Number of hidden layers: consider architectures with 1, 2, and 3 hidden layers.\n",
    "- Number of neurons per hidden layer: consider 5, 20, and 50 neurons per hidden layer.\n",
    "- Degree of L2 regularization: \n",
    "  - Note that MLPClassifier supports only L2 regularization via the argument `alpha` of `MLPClassifier`\n",
    "  - consider values of 0.001, 0.01, and 0.1.\n",
    "- Learning rate: consider learning rates of 0.001, 0.01, and 0.1.   \n",
    "- Use AUC as the performance metric to optimize.\n",
    "- PLot the ROC curve of the best model and that of a NN with default hyperparameters using `ROCurveDisplay`.\n",
    "\n",
    "Note that the search over this comprehensive hyperparameter space may take a while. Set the argument `verbose` of `GridSearchCV` to obtain some status updates while the search is running.\n",
    "\n",
    "And don't be too disappointed when the performance gain over the default model is less than expected ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd2756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to exercise 4: NN hyperparameter optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6383d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ROC curve\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "B0rxPs4QEGtz",
    "27sCENzmoGcX"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "bads310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
