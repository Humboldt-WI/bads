{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97f60c7",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorial_notebooks/6_classification_tasks.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd632d8e",
   "metadata": {},
   "source": [
    "# Tutorial 6 - Classification for Credit Risk Prediction\n",
    "In this demo notebook, we will revisit our lecture on classification models. To that end, we consider the logistic regression model and study how it allows us to approach a probability-to-default prediction task. The notebook comprises ready-to-use demo codes and small programming tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f867e0",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "Before moving on, let's import some of our standard library so that we have them ready when we need them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91904e69",
   "metadata": {},
   "source": [
    "### The HMEQ data set\n",
    "We continue using the \"Home Equity\" data set (HMEQ), which was introduced in your [last session](https://github.com/Humboldt-WI/bads/blob/master/tutorial_notebooks/5_data_prep_tasks.ipynb).\n",
    "To keep this notebook self-contained, we re-introduce the set of features below: \n",
    "\n",
    "- BAD: the target variable, 1=default; 0=non-default \n",
    "- LOAN: amount of the loan request\n",
    "- MORTDUE: amount due on an existing mortgage\n",
    "- VALUE: value of current property\n",
    "- REASON: DebtCon=debt consolidation; HomeImp=home improvement\n",
    "- JOB: occupational categories\n",
    "- YOJ: years at present job\n",
    "- DEROG: number of major derogatory reports\n",
    "- DELINQ: number of delinquent credit lines\n",
    "- CLAGE: age of oldest credit line in months\n",
    "- NINQ: number of recent credit inquiries\n",
    "- CLNO: number of credit lines\n",
    "- DEBTINC: debt-to-income ratio\n",
    "\n",
    "As you can see, the features aim at describing the financial situation of a borrower. We will keep using the data set for many modeling tasks in this and future notebooks. So it makes sense to familiarize yourself with the above features. Make sure you understand what type of information they provide and what this information might reveal about the risk of defaulting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ece5652",
   "metadata": {},
   "source": [
    "### Pandas reloaded\n",
    "You have already seen the Pandas library in action. We assume you have familiarized yourself with the demos and exercises of [Tutorial 5](https://github.com/Humboldt-WI/bads/blob/master/tutorial_notebooks/5_data_prep_tasks.ipynb). Below, we load the data and revisit some standard operations to eyeball the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1242423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data directly from GitHub\n",
    "data_url = 'https://raw.githubusercontent.com/Humboldt-WI/bads/master/data/hmeq.csv'\n",
    "df = pd.read_csv(data_url)\n",
    "\n",
    "# To show that everything worked out, we can print the first few rows of the data frame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73f29e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a more technical overview of the data \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79652819",
   "metadata": {},
   "source": [
    "The above output displays some useful information how exactly data is stored. We learn about the data type of each feature (i.e, each *data series* object), missing values, and the total amount of memory that the data consumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce summary statistics (to R-programmers: this is equivalent to the famous R function summary())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93cec23",
   "metadata": {},
   "source": [
    "### Preparing the data for analysis\n",
    "The data exhibits several standard data quality issues including missing values. We discussed in previous tutorials and exercises to address such issues and prepare the data for analysis. Take a little time to review the following codes. They do not comprise novel concepts. Rather, we combine known programming steps to ready our data for classification modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0018559d",
   "metadata": {},
   "source": [
    "#### Altering data types\n",
    "See [Tutorial 5](https://github.com/Humboldt-WI/bads/blob/master/tutorial_notebooks/5_data_prep_tasks.ipynb) for details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dd937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target variable has only two states so that we can store it as a boolean\n",
    "df['BAD'] = df['BAD'].astype('bool')\n",
    "\n",
    "# Code categories properly \n",
    "df['REASON'] = df['REASON'].astype('category')\n",
    "df['JOB'] = df['JOB'].astype('category')\n",
    "\n",
    "# Verify that the data types have been changed\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184f5516",
   "metadata": {},
   "source": [
    "#### Missing values\n",
    "See [Tutorial 5](https://github.com/Humboldt-WI/bads/blob/master/tutorial_notebooks/5_data_prep_tasks.ipynb) for details. \n",
    "\n",
    "##### Imputation in categorical features\n",
    "We replace missing values in categorical features with the most frequent value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183ce652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an index of the categorical columns\n",
    "ix_cat = df.select_dtypes(include=['category']).columns\n",
    "\n",
    "# Process each category\n",
    "for c in ix_cat:\n",
    "    df.loc[df[c].isna(), c ] = df[c].mode()[0]  # the index [0] is necessary as the result of calling mode() is a Pandas Series\n",
    "\n",
    "# Verify that there are no missing values left\n",
    "df[ix_cat].isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57647cc8",
   "metadata": {},
   "source": [
    "##### Missing values in numerical features\n",
    "We use median imputation to replace missing values in numerical features. While we could use a loop, just as before, a more elegant way is to use the `SimpleImputer` class from the `sklearn.impute` module. Recall that [Tutorial 5](https://github.com/Humboldt-WI/bads/blob/master/tutorial_notebooks/5_data_prep_tasks.ipynb) demonstrated further options to achieve the same. It is common that there is no single *best* way of programming things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8034f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create an imputer object with the strategy 'median'\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Select only numerical columns\n",
    "ix_num = df.select_dtypes(include=np.number).columns\n",
    "\n",
    "# Apply the imputer to the numerical columns\n",
    "df[ix_num] = imputer.fit_transform(df[ix_num])\n",
    "\n",
    "df[ix_num].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6731a26e",
   "metadata": {},
   "source": [
    "#### Outliers\n",
    "Given that our data includes some outliers, we apply our standard outlier truncation mechanism. This time, we focus on extreme outliers (i.e., 3 times inter-quartile-range above/below the 75th/25th percentile). Further, we exclude two features from outlier treatment: `DEROG` and `DELINQ`. The reason is that standard outlier truncation would completely destroy these features due to their very skewed distribution. We will come back to this point later in the notebook and propose a more suitable treatment for these features. \n",
    "\n",
    "We also exclude the `DEBTINC` feature from outlier treatment. This is for didactic reasons. The feature is more informative without standard outlier treatment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b07479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to handle outliers\n",
    "def truncate_outliers(df, factor=1.5):\n",
    "    excluded_columns = ['DEROG', 'DELINQ', 'DEBTINC']\n",
    "    ix_num  = [c for c in df.select_dtypes(include=np.number).columns if c not in excluded_columns]   \n",
    "    for col in ix_num:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - factor * IQR\n",
    "        upper_bound = Q3 + factor * IQR\n",
    "        df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "    return df\n",
    "\n",
    "# Apply the function to the data and inspect the results\n",
    "df = truncate_outliers(df, factor=3)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f9563",
   "metadata": {},
   "source": [
    "# Explanatory data analysis\n",
    "Now that we are familiar with Pandas and have prepared out data, at least rudimentary, we can explore some of the plotting capabilities in Python. To that end, we go through an EDA pipeline and try to improve our understanding of the data along the way.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427ced0c",
   "metadata": {},
   "source": [
    "### Univariate analysis of categorical variables\n",
    "In this part, we will examine  our target variable 'BAD', as well as the two categorical variables 'REASON' and 'JOB' individually. Firstly, we will count how many observations belong to each category of a variable. The function `value_counts()` is a good choice to process all categories in one go, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(exclude=np.number).apply(pd.Series.value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffc58de",
   "metadata": {},
   "source": [
    "<br>\n",
    "While tables are a useful way to inspect data, graphs are often easier to interpret and more appealing. For categories, count plots and stacked count plots are common vehicles for data exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49063a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exluding data type float leaves us with the target variable and both categorical variables\n",
    "for i, col in enumerate(df.select_dtypes(exclude=np.number).columns):\n",
    "    plt.figure(i)\n",
    "    sns.countplot(x=col, data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f417bc21",
   "metadata": {},
   "source": [
    "### Univariate analysis of numeric variables\n",
    "Let us now take a closer look at the numeric variables and their distribution by means of histograms. Creating a histogram is easily achieved using the `hist()` function, which Pandas offers. Dedicated plotting libraries offer a bit more flexibility. For start, we showcase functionality of the `Seaborn` library. We recommend this version when you examine a single variable at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c6a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo of a histogram using Seaborn\n",
    "sns.displot(df['CLAGE']);  # Explore the arguments that the function supports to discober variants of the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e706c6a7",
   "metadata": {},
   "source": [
    "Next, we consider a basic histogram and use Pandas functionality to produce an overview of all the numeric variables in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2051a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create one histogram for each numeric variable and illustrate how to set the number of bins\n",
    "df.select_dtypes(include=np.number).hist(bins=20, figsize=(12,8))\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3400636d",
   "metadata": {},
   "source": [
    "Despite our outlier treatment, we stil observe many variables to have a long tail. This is plausible since we only treated extreme outliers above. To emphasize this, let's proceed with the next key plot, the **boxplot**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dfc1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One boxplot for each variable\n",
    "ix_num = df.select_dtypes(include=np.number)\n",
    "half = np.ceil(ix_num.shape[1] / 2).astype(int)  # for cosmetic reasons we split the plots into two rows\n",
    "ix_num.iloc[:, 0:half].plot(kind = 'box', subplots=True);\n",
    "ix_num.iloc[:, half:ix_num.shape[1]].plot(kind = 'box', subplots=True)\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af376a7e",
   "metadata": {},
   "source": [
    "#### Discretization of Numeric Variables\n",
    "\n",
    "As seen in the boxplots above, we are facing some issues with the distribution of `DEROG` and `DELINQ`. Given their distribution, it seems appropriate to discretize these variables.  Discretization is the process of converting a numeric variable into a discrete variable, i.e., a category. Since both, `DEROG` and `DELINQ` display a large number of zeros, we could, for example, consider one category level *isZero* and another *IsGreaterThenZero*. This would give a binary variable. We can also introduce more category levels to obtain a fine-grained categorical representation of the original numbers. Normally, the function `qcut()` is a good choice to discretize a variable based on quantiles. We cannot demonstrate this with `DEROG` and `DELINQ` since their distributions do not warrant using quantiles as split points. Let's first inspect the set of distinct values in these two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d9221",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DELINQ.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53100f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DEROG.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077a5736",
   "metadata": {},
   "source": [
    "How you proceed from here is based on judgment. We will exemplify two different techniques that could be considered in the focal case. For one feature, we will create a dummy, indicating whether or not the value of the variable is 0. For the other feature, we will group into three categories. As \"DELINQ\" shows fewer observations for the value 0, we will use this variable to divide into three groups: 0, 1 & >1. If you wish, this is a manual, or expert-based version of discretization in which we pick the boundaries of the buckets manually instead of picking them by looking at quantiles.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e2a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DELINQcat'] = '1+' #set default value to +1 for new variable \n",
    "df.loc[(df['DELINQ'] == 1), 'DELINQcat'] = '1' # change this value to 1, if value of DELINQ is 1\n",
    "df.loc[(df['DELINQ'] == 0), 'DELINQcat'] = '0'\n",
    "df['DELINQcat'] = df['DELINQcat'].astype('category')  # convert to categorical\n",
    "df.DELINQcat.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c475fe",
   "metadata": {},
   "source": [
    "We will proceed similarly with the variable \"DEROG\". We will create a dummy variable where every observation has the value 1 (true) if their value for the \"DEROG\" variable was 0. Every other value will be assigned the value 0 (false)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820d7a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DEROGzero'] = 0 #set default to 0\n",
    "df.loc[(df['DEROG'] == 0), 'DEROGzero'] = 1 #change to 1 if value of \"DEROG\" is 0 \n",
    "df['DEROGzero'] = df['DEROGzero'].astype('bool')\n",
    "df.DEROGzero.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed81d1da",
   "metadata": {},
   "source": [
    "We are not done with our EDA workflow. However, we will not make any more changes to the data. Therefore, it is a good time to save a copy of the prepared data so that we can re-use it in subsequent coding sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the data in the present, pre-processed format\n",
    "df.to_csv('./hmeq_prepared.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b302e",
   "metadata": {},
   "source": [
    "### Multivariate analysis of numeric variables\n",
    "\n",
    "After gaining more knowledge about the variables individually, it is important to examine their relationships more closely. In data science, this is a good way of identifying redundant information as well as variable interactions. \n",
    "Next, we will plot a heatmap. It shows the correlation for all numeric variables. Highly correlated variables are redundant as they convey the same pieces of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab4e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr= df.select_dtypes(include=np.number).corr()\n",
    "f,ax = plt.subplots(figsize=(18, 15))\n",
    "sns.heatmap(corr ,annot=True,linewidth=.5,fmt='1f');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a549511c",
   "metadata": {},
   "source": [
    "Due to the amount of variables, it can be hard to make sense of the plot and spot the important variable relationships. We can solve this by filtering from a specific threshold. For example, we consider a threshold of $\\rho=0.3$ below, and focus the plot to those variables whose pairwise correlation exceeds this threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f180eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(18, 15))\n",
    "sns.heatmap(corr[(corr >= 0.30) | (corr <= -0.30)],\n",
    "            annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2205ec",
   "metadata": {},
   "source": [
    "If you have trust in the threshold, the above chart makes variable selection easy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00601fe",
   "metadata": {},
   "source": [
    "### Multivariate analysis of categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae3a693",
   "metadata": {},
   "source": [
    "Categories and the binary target in particular are also useful to examine sub-groups. For example, we could calculate the mean of a/all numeric variables for good and bad borrowers. Enter `.groupby()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd94aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"BAD\").mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe7f227",
   "metadata": {},
   "source": [
    "Another standard operation when exploring categorical variables is to check cross-tabulations. Considering, for example, the variables `Reason`and `Job`, we can create a cross-tab as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01adbb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.REASON, df.JOB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff265e68",
   "metadata": {},
   "source": [
    "A cross-tab can be particularly informative when it includes the target variable. This way, we can spot whether certain category levels of the other (independent) variable are especially prominent with good or bad borrowers. In this use case, we would also want to switch from showing counts (as above) to showing relative frequencies. We achieve this by augmenting our call to `crosstab()` with the argument `normalize='index'`. Have a look into the [documentation of the function](https://pandas.pydata.org/docs/reference/api/pandas.crosstab.html) to understand why we select the option `'index'` for the function argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02df50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = pd.crosstab(df.JOB, df.BAD, normalize='index')\n",
    "job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4619040",
   "metadata": {},
   "source": [
    "Once again, we can also report the same information in a graphical way. A common way to display categorical variables is the stacked count plot. Let us analyze the variables `REASON` and `JOB` and how they are linked to the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8441c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason = df.groupby(['BAD', 'REASON'], observed=True).size().reset_index().pivot(columns='BAD', index='REASON', values=0)\n",
    "                                                                       \n",
    "reason.plot(kind='bar', stacked=True); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479afe6c",
   "metadata": {},
   "source": [
    "Let's do it one more time, this time showing the relative frequencies instead of the absolute counts. This might provide more insight into the variable distribution and how it differs across the two target groups (i.e., goods and bads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11b866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.div(job.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\n",
    "reason.div(reason.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bafbc2c",
   "metadata": {},
   "source": [
    "### Interactions between numeric and categorical variables \n",
    "Next, we can have a look at the distribution of our categories across the numerical variables. Violin plots are a great way to do so. The *seaborn* library makes creating these plots very easy. Below, we illustrate two options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7596dc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the category 'REASON' and create one plot for each numeric variable\n",
    "for col  in df.select_dtypes(include=np.number).columns:\n",
    "    plt.figure()\n",
    "    sns.violinplot(x='REASON', y=col, hue='BAD',\n",
    "                   split=True, inner=\"quart\",\n",
    "                   data= df)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d4d86",
   "metadata": {},
   "source": [
    "Let's repeat this for the variable 'JOB' but using a different approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d49e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for category JOB\n",
    "fig, axs = plt.subplots(3,3, figsize=(15, 10))\n",
    "plt.tight_layout(pad=0.5, w_pad=4, h_pad=1.0)  \n",
    "x = df.JOB\n",
    "\n",
    "sns.violinplot(x=x, y=\"LOAN\",  data=df,ax=axs[0,0])\n",
    "sns.violinplot(x=x, y=\"MORTDUE\", data=df,ax=axs[0,1])\n",
    "sns.violinplot(x=x, y=\"VALUE\", data=df,ax=axs[0,2])\n",
    "sns.violinplot(x=x, y=\"YOJ\", data=df,ax=axs[1,0])\n",
    "sns.violinplot(x=x, y=\"DEROG\", data=df,ax=axs[1,1])\n",
    "sns.violinplot(x=x, y=\"CLAGE\", data=df,ax=axs[1,2])\n",
    "sns.violinplot(x=x, y=\"NINQ\", data=df,ax=axs[2,0])\n",
    "sns.violinplot(x=x, y=\"CLNO\", data=df,ax=axs[2,1])\n",
    "sns.violinplot(x=x, y=\"DEBTINC\", data=df,ax=axs[2,2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd477e7a-c3c4-4952-b50b-4cd3d866906c",
   "metadata": {},
   "source": [
    "# Binary classification for PD modeling\n",
    "Having prepared and explored our data, we can proceed with predictive modeling. The lecture introduced the general classification setup and the logistic regression model. Let's revisit these elements in detail. Prior to that, this is a good time to separate our data into an array $y$ containing the target variable and a data frame $X$ containing the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac790af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['BAD', 'DELINQ', 'DEROG'])  # drop target and the two skewed features we discretized\n",
    "y = df['BAD']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2ea9f-4363-42a7-a5a3-ec2d037cff01",
   "metadata": {},
   "source": [
    "## Excercise 1: Plotting data for classification\n",
    "You will remember the many plots we came across when discussing regression. We also saw some analog plots for classification problems in the lecture. One of them was a 2d scatter plot displaying the bi-variate relationship between selected features and the binary target variable. \n",
    "\n",
    "![Classification problem in 2D](https://raw.githubusercontent.com/stefanlessmann/ESMT_IML/main/resources/2d_classification_problem.png)\n",
    "\n",
    "Your first task is to create a similar plot for the credit data. In principle, you can select any combination of features that you like.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1056e886-fb90-4846-a961-fe6543a6ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "x1 = 'xxx'  # select first feature of your choice\n",
    "x2 = 'xxx'  # select second feature of your choice\n",
    "\n",
    "# Write code to create the scatter plot of x1 vs. x2. Make sure your plot shows the data points from different classes (good and bad payers) in different colors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742e463-9ab9-49ca-a84e-3c1bfb004a90",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "Time to estimate our first model. We will use logistic regression. Think of it as an extension of linear regression for cases in which we work with a binary target variable. The lecture will soon provide more details.\n",
    "\n",
    "Just as in linear regression, logistic regression involves model training on labelled data. The below code uses the `sklearn` library to train a logistic regression-based classification model. \n",
    "\n",
    "### Dummy coding\n",
    "Before we proceed, we need to dummy code our categorical variables. This is a standard step in classification modeling. The `get_dummies()` function from the `pandas` library is a convenient way to achieve this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6f4db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X, drop_first=True)  \n",
    "X.info()  # Note how the function created new dummy variables in our feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45baf169",
   "metadata": {},
   "source": [
    "\n",
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9bfc1-ab50-465e-934f-1034f587ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression().fit(X, y)  # we define a random_state to ensure that we get the same results when re-running this cell multiple times\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb9920-d1b3-4a19-9dfa-0a14bb7210bc",
   "metadata": {},
   "source": [
    "Note that the `sklearn` implementation does not provide an informative summary, as did the library `statsmodels`, which we used to [illustrate regression analysis](https://github.com/Humboldt-WI/bads/blob/master/tutorial_notebooks/4_predictive_analytics_tasks.ipynb). In brief, this is because `sklearn` is designed to support prediction. Let's demonstrate how to do this, that is compute predictions using the trained model. For simplicity, we compute prediction for the training data. You already learnt that this is inappropriate. We do it here to keep things simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2856dfd-fe11-4ed1-803e-63f66c58b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X)  # simple way to compute predictions using logistic regression and any other machine learning model in sklearn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4565bc58-709f-4b00-919c-82ea24b30082",
   "metadata": {},
   "source": [
    "Likely, you are also interested to assess the model. There is an easy way to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7039a29-fedc-4932-a098-8052bf57657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = model.score(X, y)  # Call a general purpose evaluation function and obtain a (quality ) score of the model\n",
    "print('Logit model achieves a score of {:.3f} %'.format(perf*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b7202a-455b-44a3-9d16-78c40a5ea1a3",
   "metadata": {},
   "source": [
    "### Exercise 2: Diagnosing predictions\n",
    "A score of above 90 percent sounds very good. Actually, it is not, and your task is to find out why. Let's break it down into pieces.\n",
    "\n",
    "#### A) What score?\n",
    "Check the [sklearn documentation](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html) to understand what kind of score the function `score()` has provided. What is it that we see?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03225801-d197-471b-a2be-c88aff8d126e",
   "metadata": {},
   "source": [
    "**Your answer:** By default, the score reported is the classification accuracy, which depends on a default classification cut-off of 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786a24e3-05d9-4439-8958-f7459cd0af4f",
   "metadata": {},
   "source": [
    "#### B) Is it good or is it bad?\n",
    "Interpreting our score will be easier if we compare it to a baseline. But what baseline? We face a classification problem. There are two classes, good payers and bad payers, and we aim to tell these apart. Come up with a very basic strategy to solve the classification problem without using any model. Write a piece of code to calculate the performance of your super-basic strategy. \n",
    "> Hint: if you feel a bit lost, consider web searching for *dummy classifier* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca09e4-5475-48f5-b938-10d8d6cd79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to calculate the score of a dummy classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c128e4-0a74-47fc-b9d7-c54f894caec5",
   "metadata": {},
   "source": [
    "If you succeeded with the previous task, you will have found that a super-basic - stupid - classifier does as well as logistic regression. This is a devastating result. Our logistic regression classifier is just as good as a naive classifier, which always predicts the majority class. Put differently, the logistic regression *appears* completely useless.\n",
    "\n",
    "Note that our approach to compute the score of the naive classifier assumes that the positive class with $Y=1$ is the minority class. While this is typically the case, we should acknowledge that our approach is simplistic. It would be better to first establish which of the two classes is the majority class and to then use the fraction of that class as the accuracy score of a dummy classifier. While not too difficult, we leave this extension for the interested to perform and move on with probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b594ac70-c675-4eae-951a-2210ee923dc8",
   "metadata": {},
   "source": [
    "#### C) What about probabilities?\n",
    "Exactly, what about probabilities? The lecture introduced classification as a machine learning setup aimed at predicting class membership probabilities. So logistic regression should answer questions such as \"what is the estimated probability of the first credit applicant in our data set to repay?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f394a7cb-f1d2-4981-b0e0-7c73eb457a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to print the prediction of logistic regression for the first data point in our matrix X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8bc848-44a1-4a67-915f-93f335badc9d",
   "metadata": {},
   "source": [
    "Solving the above task, you will find that the prediction does not look like a probability. Examine this point in more detail. To that end, write code that tells you what *distinct* values logistic regression predicts. Put differently, your code should print out all unique values that logistic regression has predicted across all data points. \n",
    "\n",
    "Briefly explain your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a644d-a304-41c2-b07c-3c5177e1ca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to find out the distinct values of the predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b615d4-5931-42d3-9d64-57db12b4d03e",
   "metadata": {},
   "source": [
    "**What is the meaning of the predictions? Briefly explain.**\n",
    "\n",
    "your explanations goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a016d75f-8cda-47e8-b071-8efca464c6a0",
   "metadata": {},
   "source": [
    "Finally, we come back to the innocent question asked before, \"what is the estimated probability of the first credit applicant in our data set to repay?\". Given our previous analysis has not answered this question it is about time to. Write code to find out the estimated probability of the first applicant to be a bad credit risk?\n",
    "\n",
    "Just in case, mathematically, we could represent the sought probability as $\\hat{p}(BAD==1|X_1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a4de6-29db-4e86-9356-cac4fe339120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to obtain probability predictions from the logit model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8173f614-b287-4e1b-b4a3-2937c88e1383",
   "metadata": {},
   "source": [
    "### Visualizing the logistic regression\n",
    "\n",
    "#### Exercise 3: One more logistic regression\n",
    "Please estimate a second logistic regression model. This time, use only two features. Exercise 1 has asked you to examine combinations of features. Just continue with the two features you selected there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28c593-d90a-43c4-92f6-a2f20364ddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Our first feature is:\\t', x1)\n",
    "print('Our second feature is:\\t', x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127568a4-102e-4b5a-9b2b-9a7d522421fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to estimate a logistic regression classifier using only the two above features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c727e2-8442-4b32-b857-a747cc36ba62",
   "metadata": {},
   "source": [
    "#### The visual logistic regression\n",
    "As you will have guessed, the point of the above exercise 4 was only to obtain a logistic regression model that we can plot; hence the need to select two features. \n",
    "The visualization is somewhat complex. Thus, all code is readily available for you. Below we provide a function `plot_logit_decision_surface()`. \n",
    "**Do not be put off by the length of the code.** You are not supposed to look through the function at this point. Of course you can, but do not allow it to confuse you. It is a function to create a plot. That is all you need to know for now. Please execute the cell to make sure you can use the function in the next exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9a3c5d-614f-4ffb-9fd2-f67d66b0ebc2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_logit_decision_surface(model, data, x1, x2, save_fig=False):\n",
    "    '''\n",
    "        Visualization of logistic regression in 2D\n",
    "        \n",
    "        Creates a plot depicting the distribution of the input\n",
    "        data along two dimensions and the probability predictions\n",
    "        of a logistic regression model. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model :   An instance of the sklearn class LogisticRegression,  which        \n",
    "                  has been trained on the input data.\n",
    "\n",
    "        data  :   Pandas data frame providing the feature values.\n",
    "\n",
    "        x1, x2:   The function plots the results of logistic regression in\n",
    "                  two dimensions. The parameters x1 and x2 give the names\n",
    "                  of the features used for plotting. These features will be\n",
    "                  extracted from the data frame.\n",
    "\n",
    "        save_fig: Binary variable allowing you to save the figure as a PNG image. \n",
    "                  Default: False\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        The function does not return a result. It's purpose is to visualize \n",
    "        logistic regression model. The corresponding plot is the only output.\n",
    "    '''\n",
    "\n",
    "    #if len(model.coef_.ravel())!=2:\n",
    "    #    raise Exception('Please estimate a logit model using only two features!')\n",
    "    # Define some variables to govern the plot\n",
    "    bounds = data.describe().loc[[\"min\", \"max\"]][[x1, x2]].to_numpy()  # value ranges of the two features\n",
    "    eps = 5  # tolerance parameter \n",
    "\n",
    "    # Create hypothetical data points spanning the entire range of feature values.\n",
    "    # We need these to get from our logistic regression model a probability prediction\n",
    "    # for every possible data point\n",
    "    xx, yy = np.mgrid[(bounds[0,0]-eps):(bounds[1,0]+eps), (bounds[0,1]-eps):(bounds[1,1]+eps)]\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    # Perhaps the logistic regression model was fitted using the full data frame. \n",
    "    # To also work in that case, we extract the estimated regression coefficients \n",
    "    # corresponding to the two features we consider for plotting\n",
    "    feature_to_index = {name: idx for idx, name in enumerate(model.feature_names_in_)}  # create a dic as intermediate step\n",
    "    indices = [feature_to_index[f] for f in [x1, x2]]  # Find the indices of our two features of interest using the dic\n",
    "    w = model.coef_.ravel()[indices]  # estimated regression coefficients\n",
    "    b = model.intercept_  # estimated intercept of the logistic regression model\n",
    "\n",
    "    # Compute probability predictions over the entire space of possible feature values\n",
    "    # In the interest of robustness, we manually compute the logistic regression predictions\n",
    "    # using the regression coefficients extracted above\n",
    "    probs = 1/(1+np.exp(-(np.dot(grid, w.reshape(2,-1))+b))).reshape(xx.shape)\n",
    "\n",
    "    # We are finally ready to create our visualization\n",
    "    f, ax = plt.subplots(figsize=(8, 6))  # new figure\n",
    "    # Contour plot of the probability predictions across the entire feature range\n",
    "    contour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\", vmin=0, vmax=1)  \n",
    "    ax_c = f.colorbar(contour)\n",
    "    ax_c.set_label(\"$\\hat{p}(y=1|X)$\")\n",
    "    ax_c.set_ticks([0, .25, .5, .75, 1])\n",
    "\n",
    "    # Scatter plot of the actual data\n",
    "    ax.scatter(data[x1], data[x2], c=y, s=50, cmap=\"RdBu\", vmin=0, vmax=1,\n",
    "               edgecolor=\"white\", linewidth=1);\n",
    "    plt.xlabel(x1)\n",
    "    plt.ylabel(x2)\n",
    "    if save_fig==True:\n",
    "        plt.savefig('logit_contour.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0962265-96b0-45d9-8885-34b5ea6e5a95",
   "metadata": {},
   "source": [
    "#### Exercise 4: Surface plot\n",
    "We are almost ready. Also run the next cell, which will give you some instructions on how to use the plotting function. Note that this code also works for other functions. Just put a '?' in front of a function call and run the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797af5bd-df50-473f-8ece-bb6136ce3dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(plot_logit_decision_surface)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca16bb2-521e-4a19-b4ea-51aa70d0e180",
   "metadata": {},
   "source": [
    "I guess your next task is obvious. Write code to call the function providing the necessary parameters so that it can do its job. If used correctly, the function will create a plot like this one:\n",
    "![Contour plot of logistic regression model](https://raw.githubusercontent.com/stefanlessmann/ESMT_IML/main/resources/logit_contour.png)\n",
    "\n",
    "Let's if it works for you..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de265c-c77f-47c4-a90d-a5896197f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to call the function plot_logit_decision_surface()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc535c0-2689-48f6-9670-d73c08fb67bd",
   "metadata": {},
   "source": [
    "# Well done! This was another comprehensive set of exercises."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "B0rxPs4QEGtz",
    "27sCENzmoGcX"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "bads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
