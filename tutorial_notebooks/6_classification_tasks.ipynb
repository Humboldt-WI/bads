{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97f60c7",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorial_notebooks/6_classification_tasks.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd632d8e",
   "metadata": {},
   "source": [
    "# Tutorial 6 - Classification for Credit Risk Prediction\n",
    "In this demo notebook, we will revisit our lecture on classification models. To that end, we consider the logistic regression model and study how it allows us to approach a probability-to-default prediction task. The notebook comprises ready-to-use demo codes and small programming tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f867e0",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "Before moving on, let's import some of our standard library so that we have them ready when we need them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91904e69",
   "metadata": {},
   "source": [
    "## The HMEQ data set\n",
    "We continue using the \"Home Equity\" data set (HMEQ), which was introduced in your [last session](https://github.com/Humboldt-WI/bads/blob/master/tutorial_notebooks/5_data_prep_tasks.ipynb).\n",
    "To keep this notebook self-contained, we re-introduce the set of features below: \n",
    "\n",
    "- BAD: the target variable, 1=default; 0=non-default \n",
    "- LOAN: amount of the loan request\n",
    "- MORTDUE: amount due on an existing mortgage\n",
    "- VALUE: value of current property\n",
    "- REASON: DebtCon=debt consolidation; HomeImp=home improvement\n",
    "- JOB: occupational categories\n",
    "- YOJ: years at present job\n",
    "- DEROG: number of major derogatory reports\n",
    "- DELINQ: number of delinquent credit lines\n",
    "- CLAGE: age of oldest credit line in months\n",
    "- NINQ: number of recent credit inquiries\n",
    "- CLNO: number of credit lines\n",
    "- DEBTINC: debt-to-income ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ece5652",
   "metadata": {},
   "source": [
    "## Pandas reloaded\n",
    "You have already seen the Pandas library in action. We assume you have familiarized yourself with the demos and exercises of [Tutorial 5](https://github.com/Humboldt-WI/bads/blob/master/tutorial_notebooks/5_data_prep_tasks.ipynb). Let us briefly recap a few Pandas function to eyeball the data: \n",
    "\n",
    "- Preview the first rows of a DataFrame: `df.head()`\n",
    "- Get a structural summary of a DataFrame: `df.info()`\n",
    "- Get summary statistics of numerical columns: `df.describe()`\n",
    "\n",
    "Below, we load the HMEQ data set from our GitHub repository and store a copy in the variable `df` (for DataFrame). Apply the above function to the variable to  re-familiarize yourself with the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1242423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data directly from GitHub\n",
    "data_url = 'https://raw.githubusercontent.com/Humboldt-WI/bads/master/data/hmeq.csv'\n",
    "df = pd.read_csv(data_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space to eyeball and explore the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93cec23",
   "metadata": {},
   "source": [
    "## Preparing the data for analysis\n",
    " [Tutorial 5](https://github.com/Humboldt-WI/bads/blob/master/tutorial_notebooks/5_data_prep_tasks.ipynb) was all about exploratory data analysis and data preparation. The HMEQ datasets exhibits some quality problems  and requires some preparation. More specifically, [Tutorial 5](https://github.com/Humboldt-WI/bads/blob/master/tutorial_notebooks/5_data_prep_tasks.ipynb) covered:\n",
    " - Handling missing values\n",
    " - Truncating outliers\n",
    " - Encoding categorical variables\n",
    " - Feature scaling\n",
    "\n",
    "We will not repeat everything here. Simply go back to the previous tutorial if unsure about any of the above preparation steps. Given that we will keep using the HMEQ dataset in future tutorials, we put all Python codes to convert from the raw version of the data to a preprocessed \"ready-for-analysis\" in the below function `prepare_hmeq_data`. The code may look somewhat involved, but you should be able to follow most of it based on what you have learned so far. Also, it is fine to skip the details for now and simply use the function as a black box. Just make sure to execute the code cell so that the function is defined in your notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db984387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def prepare_hmeq_data(df: pd.DataFrame, missing_dummy=True, outlier_factor: float = 0, scale_features: bool = True, verbose=0) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\" Prepare the HMEQ data for analysis.\n",
    "        Missing values will be imputed using the median (numerical features) or the mode (categorical features).\n",
    "        Optionally, a dummy variable can be added for each feature indicating for which cases, in that feature, imputation was carried out. \n",
    "        Other data preparation steps, including outlier truncation (using the IQR method), feature scaling (z-transformation), and dicretization\n",
    "        can be configured via arguments.\n",
    "\n",
    "    Parameters\n",
    "    ----------  \n",
    "    df : pd.DataFrame\n",
    "        Raw HMEQ data as a Pandas DataFrame.\n",
    "    missing_dummy : bool, optional\n",
    "        Whether to add a dummy variable indicating missingness for DEBTINC. Default is True.\n",
    "    outlier_factor : float, optional\n",
    "        Factor to determine the extent of outlier truncation using the IQR method. Default is 0 (no truncation). \n",
    "        If set to a positive number, outliers are truncated at Q1 - outlier_factor*IQR and Q3 + outlier_factor*IQR.\n",
    "    scale_features : bool, optional\n",
    "        Whether to scale numerical features using z-transformation. Default is True.\n",
    "    verbose : int, optional\n",
    "        Verbosity level for logging. Default is 0 (no logging).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : pd.DataFrame\n",
    "        Prepared feature matrix.\n",
    "    y : pd.Series\n",
    "        Target variable (BAD).\n",
    "    \"\"\"\n",
    "    \n",
    "    X = df.copy()  # Create a copy of the original data frame\n",
    "    y = X.pop('BAD')  # Separate the target variable\n",
    "    ####################################################################\n",
    "    # Type conversion\n",
    "    #-------------------------------------------------------------------\n",
    "    df['BAD'] = df['BAD'].astype('bool')  # The target variable has only two states so that we can store it as a boolean\n",
    "    df['REASON'] = df['REASON'].astype('category')  # Code categories properly \n",
    "    df['JOB'] = df['JOB'].astype('category')\n",
    "    df['LOAN'] = df['LOAN'].astype('float')  # Ensure all numerical features use data type float (for indexing) \n",
    "\n",
    "    cat_features = df.select_dtypes(include=['category']).columns  # Get an index of the categorical columns\n",
    "    num_features = df.select_dtypes(include=['float']).columns  # Get an index of the numerical columns\n",
    "\n",
    "    ####################################################################\n",
    "    # Missing value imputation\n",
    "    #-------------------------------------------------------------------\n",
    "    for col in X.columns:\n",
    "        # Check if there are missing values in this column\n",
    "        if X[col].isna().any():\n",
    "            # If requested, add dummy BEFORE imputation to capture original missingness\n",
    "            if missing_dummy==True:\n",
    "                dummy_col_name = f\"{col}_missing\"\n",
    "                X[dummy_col_name] = X[col].isna()\n",
    "                if verbose>0: print(f\"Added missingness dummy for column {col} as {dummy_col_name}\")\n",
    "\n",
    "            # Handle floats: fill with median\n",
    "            if col in num_features:\n",
    "                median_value = X[col].median()\n",
    "                X[col] = X[col].fillna(median_value)\n",
    "                if verbose>0: print(f\"Filled missing values in numerical column {col} with median value {median_value}\")\n",
    "\n",
    "            # Handle categoricals/objects: fill with mode\n",
    "            elif col in cat_features:\n",
    "                # mode() returns a Series; take the first mode if it exists\n",
    "                modes = X[col].mode(dropna=True)\n",
    "                if len(modes) > 0:\n",
    "                    mode_value = modes.iloc[0]\n",
    "                    X[col] = X[col].fillna(mode_value)\n",
    "                    if verbose>0: print(f\"Filled missing values in categorical column {col} with mode value {mode_value}\")\n",
    "                else:\n",
    "                    raise Exception(f\"No mode found for column {col} during imputation. Leaving this column untouched.\")\n",
    "                continue\n",
    "            else:\n",
    "                raise Exception(f\"Column\")\n",
    "\n",
    "    # Verify that there are no missing values left\n",
    "    check = X.isna().sum().sum()\n",
    "    if check > 0:\n",
    "        raise Exception(f\"We still observe {check} missing values in the data.\")\n",
    "    ####################################################################\n",
    "    # Feature engineering: \n",
    "    # Domain knowledge suggests that the features DELINQcat and DEROG are important predictors of loan default.\n",
    "    # However, their distribution is highly skewed, with many zeros and few high values. \n",
    "    # To aid modeling, we add a dummy feature distinguishing cases where these features are zero versus non-zero \n",
    "    #-------------------------------------------------------------------\n",
    "    X['DELINQ_nz'] = pd.cut(X['DELINQ'], bins=[-1, 0, float('inf')], labels=[0, 1])\n",
    "    X['DEROG_nz'] = pd.cut(X['DEROG'], bins=[-1, 0, float('inf')], labels=[0, 1])\n",
    "\n",
    "    ####################################################################\n",
    "    # Truncate outliers among numerical features\n",
    "    #-------------------------------------------------------------------\n",
    "    if outlier_factor > 0:\n",
    "        for col in num_features:\n",
    "            q1, q3 = X[col].quantile(0.25), X[col].quantile(0.75)\n",
    "            min_x, max_x = X[col].min(), X[col].max()\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = np.max([q1 - outlier_factor * iqr, min_x])\n",
    "            upper_bound = np.min([q3 + outlier_factor * iqr, max_x])\n",
    "            X[col] = X[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "            if lower_bound > min_x or upper_bound < max_x:\n",
    "                if verbose>0: print(f\"Truncated outliers in column {col} to range [{lower_bound}, {upper_bound}]\")\n",
    "\n",
    "    #-------------------------------------------------------------------\n",
    "    # Scale numerical features using the z-transformation (if requested)\n",
    "    #-------------------------------------------------------------------\n",
    "    if scale_features == True:\n",
    "        scaler = StandardScaler()\n",
    "        X[num_features] = scaler.fit_transform(X[num_features])\n",
    "    #-------------------------------------------------------------------\n",
    "    # Dummy encode categorical features\n",
    "    #-------------------------------------------------------------------\n",
    "    dummy_codes = pd.get_dummies(X[cat_features], drop_first=True)  # obtain dummy codes\n",
    "    X.drop(cat_features, axis=1, inplace=True)    # remove original categorical features\n",
    "    X = pd.concat([X, dummy_codes], axis=1)  # add dummy codes back to feature matrix     \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4439d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "X, y = prepare_hmeq_data(df)\n",
    "# Preview the processed feature matrix\n",
    "X.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd477e7a-c3c4-4952-b50b-4cd3d866906c",
   "metadata": {},
   "source": [
    "# Binary classification for PD modeling\n",
    "Having prepared our data, we can proceed with predictive modeling. The lecture introduced the general classification setup as an extension of regression. You will remember the many plots we came across when discussing regression. We also saw some analog plots for classification problems in the lecture. One of them was a 2d scatter plot displaying the bi-variate relationship between selected features and the binary target variable. \n",
    "<p align=\"left\">\n",
    "  <img src=\"https://raw.githubusercontent.com/stefanlessmann/ESMT_IML/main/resources/2d_classification_problem.png\" alt=\"2D classification problem\" width=\"640\" />\n",
    "</p>\n",
    "Let us recreate such a plot for the HMEQ data. We arbitrarily select `LOAN` and `VALUE` as our two features of interest. Interact with the code to create the scatterplot for different pairs of variables. Also try to interpret the plot. What does it tell you about the classification problem? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1056e886-fb90-4846-a961-fe6543a6ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot of our classification problem\n",
    "x1 = 'LOAN'  # select first feature of your choice\n",
    "x2 = 'VALUE'  # select second feature of your choice\n",
    "\n",
    "# Create a scatter plot for LOAN vs VALUE, colored by BAD\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[x1][y==0], X[x2][y==0], color='blue', label='Non-default (BAD=0)', alpha=0.6)\n",
    "plt.scatter(X[x1][y==1], X[x2][y==1], color='red', label='Default (BAD=1)', alpha=0.6)\n",
    "plt.xlabel(x1)\n",
    "plt.ylabel(x2)\n",
    "plt.title(f'Scatter Plot of {x1} vs {x2} colored by repayment status')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3837ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the seaborn library could be used instead, which is often simpler to create standard visualizations.\n",
    "# Here is an example needed only one line of code:\n",
    "sns.scatterplot(data=X, x=x1, y=x2, hue=y.map({0: \"Repayment\", 1: \"Default\"}), alpha=0.8);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742e463-9ab9-49ca-a84e-3c1bfb004a90",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "Time to estimate our first model. We will use logistic regression. Think of it as an extension of linear regression for cases in which we work with a binary target variable. Recall the formula of the logistic function:\n",
    "\n",
    "$$ \\frac{1}{1+exp(-\\eta)} $$\n",
    "where\n",
    "$$ \\eta = b + w_1 x_1 + w_2 x_2 + ... + w_p x_p $$\n",
    "\n",
    "Below, we use the sklearn library to estimate a logistic regression model on the prepared HMEQ data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9bfc1-ab50-465e-934f-1034f587ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression-based model for classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression().fit(X, y)  # we define a random_state to ensure that we get the same results when re-running this cell multiple times\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb9920-d1b3-4a19-9dfa-0a14bb7210bc",
   "metadata": {},
   "source": [
    "Note that the `sklearn` implementation does not provide an informative summary, as we might expect based on our experiences with linear regression. The sklearn library is designed for prediction. That is why much of the functionality on regression diagnostics, statistical tests, and so on, is missing. You can check out the library `statsmodels` for an implementation of logistic regression, and other [statistical models](https://www.statsmodels.org/stable/index.html) that better supports explanatory modeling. We focus on prediction.\n",
    "\n",
    "\n",
    "Let's first demonstrate how to use our logistic regression for prediction. All sklearn models provide a function `model.predict()`, which requires, as argument, a matrix of feature values. Equipped with this information let's try to create our standard input to forecast model evaluation: a table with actual values of the target variable $Y$ and model-based forecasts $ \\hat{Y} $.\n",
    "\n",
    "<p align=\"left\">\n",
    "  <img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/y_yhat.png\" alt=\"Comparing forecasts to actuals\" width=\"170\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2856dfd-fe11-4ed1-803e-63f66c58b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed in class \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79099b86",
   "metadata": {},
   "source": [
    "# Data organization\n",
    "It is good practice to split labelled data into training and test partitions, using the training set for model estimation and the test set of evaluation. \n",
    "\n",
    "\n",
    "<p align=\"left\">\n",
    "  <img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/holdout_method.png\" alt=\"Holdout method\" width=\"640\" />\n",
    "</p>\n",
    "\n",
    "The key sklearn function to support this step is `train_test_split()`. Of course, sklearn also provides support for cross-validation, which we will explore in a later tutorial. For now, a simple train-test split is all we need to illustrate proper evaluation. The next code cell illustrates the use of the function and it is commonly used to create four variables storing, respectively, the feature matrix of the training and test partitions and the corresponding target labels.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "random_seed = 312  # for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33220884",
   "metadata": {},
   "source": [
    "## Mini-Task:\n",
    "Estimate a new logit model using only the training data. Then, compute test set predictions. Overwrite our current variables `model` and `yhat` to store the new model and the test set predictions, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892b91f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for the mini task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c128e4-0a74-47fc-b9d7-c54f894caec5",
   "metadata": {},
   "source": [
    "### Exercises 1 Evaluating predictions\n",
    "Now that we set up our model evaluation dataset, your first exercise is to compute some of the standard classification performance metrics introduced in the lecture. In particular, compute the following metrics based on the true and predicted labels stored in the variables `y` and `yhat`, respectively:\n",
    "- Accuracy $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "- Precision $\\frac{TP}{TP + FP}$\n",
    "- Recall $\\frac{TP}{TP + FN}$\n",
    "- F1-score  $\\frac{2 \\cdot Precision \\cdot Recall}{Precision + Recall}$\n",
    "\n",
    "Of course, we could calculate these metrics manually using basic Python commands. However, sklearn provides ready-to-use functions, which makes evaluation much easier. The next coding cell already imports the relevant functions.  \n",
    "\n",
    "Sklearn also facilitates the computation of a confusion matrix and the ROC curve, which are useful tools to visualize classification performance. Explore the classes `ConfusionMatrixDisplay` and `RocCurveDisplay`, which we import below. Write code to create a plot of the confusion matrix and the ROC curve respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58854350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes for Exercise 1: Standard evaluation metrics for classification models\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf90f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes for Exercise 1 continued: Visualization of classification model performance\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aacf02",
   "metadata": {},
   "source": [
    "## Discussion point: what is wrong with the ROC curve?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becc7abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discussion codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8173f614-b287-4e1b-b4a3-2937c88e1383",
   "metadata": {},
   "source": [
    "# Visualizing the logistic regression\n",
    "We conclude the notebook with with a visualization of the logistic regression model. To abstract from the complexities of real-word data like our HMEQ data set, we will use sklearn functions to create synthetic data for classification with a clear signal. We will also restrict ourselves to two features (i.e., 2D) to facilitate visualization of the data and the final logistic model. \n",
    "\n",
    "The function for visualizing the data and logistic regression model called `plot_logit_decision_surface()` is defined below. The code cell may look somewhat involved, but you should be able to follow most of it based on what you have learned so far. Also, it is fine to skip the details of the implementation. However, make sure to execute the next code cell so that the function is defined in your notebook environment. The following code cell will then put everything together, first creating synthetic data and then calling the visualization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec45064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logit_decision_surface(Xtr, Xts, ytr, yts):\n",
    "    \"\"\"\n",
    "    Visualization of logistic regression in 2D.\n",
    "    Plots probability surface, training data (faded/small), and test data (dominant).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Xtr : np.ndarray\n",
    "        Training feature matrix.\n",
    "    Xts : np.ndarray\n",
    "        Test feature matrix.\n",
    "    ytr : np.ndarray            \n",
    "        Training labels.\n",
    "    yts : np.ndarray\n",
    "        Test labels.\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        The function displays a plot and does not return any value.        \n",
    "    \"\"\"\n",
    "\n",
    "    # Fit model\n",
    "    lr = LogisticRegression().fit(Xtr, ytr)\n",
    "\n",
    "    # Use both train + test to define bounds\n",
    "    Xall = np.vstack([Xtr, Xts])\n",
    "    min_x, min_y = Xall.min(axis=0)\n",
    "    max_x, max_y = Xall.max(axis=0)\n",
    "\n",
    "    eps = 0.5     # padding around data\n",
    "    res = 300     # grid resolution per axis\n",
    "\n",
    "    # Grid for probability surface\n",
    "    xs = np.linspace(min_x - eps, max_x + eps, res)\n",
    "    ys = np.linspace(min_y - eps, max_y + eps, res)\n",
    "    xx, yy = np.meshgrid(xs, ys)\n",
    "\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    probs = lr.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Probability contour\n",
    "    levels = np.linspace(0, 1, 26)\n",
    "    contour = ax.contourf(xx, yy, probs, levels=levels,\n",
    "                          cmap=\"RdBu\", vmin=0, vmax=1)\n",
    "    cbar = fig.colorbar(contour)\n",
    "    cbar.set_label(r\"$\\hat{p}(y=1|X)$\")\n",
    "    cbar.set_ticks([0, .25, .5, .75, 1])\n",
    "\n",
    "    # ------------------------\n",
    "    # TRAINING DATA (small, transparent)\n",
    "    # ------------------------\n",
    "    ax.scatter(\n",
    "        Xtr[:, 0], Xtr[:, 1], \n",
    "        c=ytr, \n",
    "        cmap=\"RdBu\",\n",
    "        vmin=0, vmax=1,\n",
    "        s=30,                    # smaller\n",
    "        alpha=0.5,               # transparent so test dominates\n",
    "        marker=\"o\",              # filled circle\n",
    "        edgecolor=\"white\"\n",
    "    )\n",
    "\n",
    "    # ------------------------\n",
    "    # TEST DATA (larger, highlighted)\n",
    "    # ------------------------\n",
    "    ax.scatter(\n",
    "        Xts[:, 0], Xts[:, 1],\n",
    "        c=yts,\n",
    "        cmap=\"RdBu\",\n",
    "        vmin=0, vmax=1,\n",
    "        s=50,                    # larger\n",
    "        marker=\"o\",              # triangle to distinguish\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(r\"$X_1$\")\n",
    "    ax.set_ylabel(r\"$X_2$\")\n",
    "    ax.set_xlim(min_x - eps, max_x + eps)\n",
    "    ax.set_ylim(min_y - eps, max_y + eps)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7fdf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# generate linearly separable data with 2 classes\n",
    "Xsyn, Ysyn = make_blobs(n_samples=600,\n",
    "                  n_features=2,\n",
    "                  centers=[[-1, -1], [1, 1]],\n",
    "                  cluster_std=1, \n",
    "                  random_state=random_seed)\n",
    "\n",
    "\n",
    "# Data partitioning\n",
    "Xtr, Xts, ytr, yts = train_test_split(Xsyn, Ysyn, test_size=0.3, random_state=random_seed)\n",
    "# Calling our plotting function\n",
    "plot_logit_decision_surface(Xtr, Xts, ytr, yts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc535c0-2689-48f6-9670-d73c08fb67bd",
   "metadata": {},
   "source": [
    "# Well done! This was another comprehensive set of exercises."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "B0rxPs4QEGtz",
    "27sCENzmoGcX"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "bads310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
