{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97f60c7",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorial_notebooks/7_deep_learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd632d8e",
   "metadata": {},
   "source": [
    "# Tutorial 7 - Neural Networks for Predictive Modeling\n",
    "This notebook revisits our lecture on deep learning. We begin with examining the calculations in simple neural networks (NNs), which consists of chains of linear projections and nonlinear transformations. We reproduce this behavior using the `Numpy` library. Thereafter, we introduce the functionality of `sklearn` for developing NNs for regression and classification. The notebook concludes with three exercises concerning the architecture of NNs, how to define it in `sklearn`, and how its implications for NN training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f867e0",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "Execute the following code cells to setup the environment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc4b78a",
   "metadata": {},
   "source": [
    "# Foundations of feedforward neural networks\n",
    "This section revisits the deep learning lecture, illustrating discussed concepts using the *Numpy* library for scientific computing.\n",
    "\n",
    "## Linear regression as directed graph\n",
    "Let's begin by revisiting the calculation of a neural network (NN) in the simplest case possible, namely a linear regression model, which you can think of as the smallest possible \"neural network\".\n",
    "\n",
    "First, we need some data. \n",
    "<p align=\"left\">\n",
    "  <img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/dl/Folie6.PNG\" alt=\"NN as a directed graph\" width=\"320\" />\n",
    "</p>\n",
    "\n",
    "As this is just a demonstration, we create some synthetic data for a linear regression problem using only two features using the *sklearn* function `make_regression()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403e700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data with two features for regression\n",
    "from sklearn.datasets import make_regression\n",
    "n_rows = 100\n",
    "n_columns = 2\n",
    "noise_level = 15\n",
    "r_state = 123\n",
    "\n",
    "X, y = make_regression(n_samples=n_rows, n_features=n_columns, n_informative=n_columns, bias=True, noise=noise_level, random_state=r_state)\n",
    "\n",
    "# Plot the data in two scatter plots\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\n",
    "\n",
    "for i in np.arange(X.shape[1]):\n",
    "    sns.scatterplot(x=X[:,i], y=y, ax=ax[i])\n",
    "    ax[i].set_title(f'Synthetic Data ($X_{i}$ vs  y)')\n",
    "    ax[i].set_xlabel(f'Feature $X_{i}$')\n",
    "    ax[i].set_ylabel('Target y')    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848f22a2",
   "metadata": {},
   "source": [
    "The lecture suggested that we feed our data row-by-row through the neural network. \n",
    "<hr>\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/dl/Folie7.PNG\" alt=\"NN as a directed graph\" width=\"320\" />\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/dl/Folie10.PNG\" alt=\"NN as a directed graph\" width=\"320\" />\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "Let's implement this idea in code. Our data has two features. Thus, the input layer of our *mini-NN* has exactly this many nodes. The next *hidden* layer consists of only one node, as we reproduce a linear regression. Thus, our NN comprises three edges, each associated with a weight. These are the parameters of the NN (aka linear regression). To illustrate the calculation, we will sample weights randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d82cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample weights for our mini neural network\n",
    "n_columns = X.shape[1]  # number of input features\n",
    "w = np.random.randn(n_columns)  # for each feature, we need one weight w \n",
    "b = np.random.randn(1)  # we also need one bias term \n",
    "\n",
    "def mini_nn_predict(row, weights, bias):\n",
    "    \"\"\"Simple mini neural network prediction function effectively performing linear regression.\n",
    "    \n",
    "    Inputs:\n",
    "        row (np.array): Input data row with two features.\n",
    "        weights (np.array): Weights for the edges from input to hidden layer.\n",
    "        bias (float): Bias term for the hidden layer node.\n",
    "        \n",
    "    Returns:\n",
    "        float: Predicted target value.\n",
    "    \"\"\"\n",
    "    # Compute the weighted sum of inputs plus bias\n",
    "    z = np.dot(row, weights) + bias\n",
    "    return z\n",
    "\n",
    "# Loop through the dataset row by row and make predictions for each row\n",
    "yhat = []  # list to store predictions\n",
    "for i in np.arange(X.shape[0]):\n",
    "    row = X[i, :]\n",
    "    pred = mini_nn_predict(row, w, b)\n",
    "    yhat.append(pred)\n",
    "\n",
    "yhat = np.array(yhat)\n",
    "# collect predictions into a DataFrame for comparison\n",
    "results_df = pd.DataFrame({'Y': y, 'Yhat': yhat.flatten()})\n",
    "results_df.head(10)  # preview first 10 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5752eec7",
   "metadata": {},
   "source": [
    "Eyeballing the data is enough to see that the predictions are horrible. We could easily prove that by computing, for example, the root- mean squared error (RMSE) between the true and predicted target values:\n",
    "```python\n",
    "# RMSE of mini-NN\n",
    "mse = np.mean( (y - yhat)**2)\n",
    "print(f'Mean Squared Error: {np.sqrt(mse):.2f}')\n",
    "``` \n",
    "However, as we have not trained the model in any way, but sampled the weights randomly, we should not expect useful predictions in the first place. The point of the demo is to illustrate the calculations of a NN from input to output; that is, how the data flows through the network structure. To better show this, let's next show a more efficient way to compute the predictions for all data points at once using matrix multiplications.\n",
    "\n",
    "$$\\hat{y} = X \\cdot w + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a2d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More efficient prediction using matrix multiplication\n",
    "# instead of looping through each row\n",
    "yhat = np.dot(X, w) + b\n",
    "# Add these predictions to the results DataFrame to prove they are the same\n",
    "results_df[\"Yhat_matmul\"] = yhat.flatten()\n",
    "results_df.head(10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d81eac",
   "metadata": {},
   "source": [
    "## A first neural network\n",
    "\n",
    "We are getting closer to *real* networks. So far, we have only illustrated the data flow through a linear regression. In the lecture, we introduced NNs as a generalization of linear models where, in the hidden layer, we somewhat *run multiple regressions in parallel*. Another generalization we discussed concerns the use of *non-linear activation functions* in the hidden layers. \n",
    "<p align=\"left\">\n",
    "  <img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/dl/Folie22.PNG\" alt=\"3 layer NN\" width=\"640\" />\n",
    "</p>\n",
    "Let's next illustrate these two concepts.\n",
    "\n",
    "As to the nonlinear activation, $g( \\cdot)$ in the slide, we select the logistic (sigmoid) function. The below helper function `g_fun_logit()` provides an implementation of the logistic function:\n",
    "\n",
    "$$ g(z) = \\frac{1}{1 + e^{-z}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c4048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# Ingredients of our first real NN\n",
    "#-------------------------------------------------------------------------------\n",
    "n_hidden_nodes = 2  # number of nodes in hidden layer\n",
    "n_out = 1  # number of output nodes (for regression, typically 1)\n",
    "\n",
    "# Helper function to compute logistic activation\n",
    "def g_fun_logit(z):\n",
    "    \"\"\"Logistic (sigmoid) activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Matrix of weights from input layer to hidden layer (initialized randomly)\n",
    "W_k = np.random.randn(n_columns, n_hidden_nodes) \n",
    "b_k = np.random.randn(n_hidden_nodes)  # bias vector for hidden layer (initialized randomly)\n",
    "\n",
    "# Matrix of weights from hidden layer to output layer (initialized randomly)\n",
    "V_k = np.random.randn(n_hidden_nodes, n_out) \n",
    "b = np.random.randn(n_out)  # bias for output layer (initialized randomly)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Forward pass through the NN:\n",
    "# (with many print statements to illustrate the shapes of the matrices/vectors)\n",
    "#-------------------------------------------------------------------------------\n",
    "print('Shape of the data is: ', X.shape)\n",
    "print('Shape of the input-to-hidden layer weight matrix: ', W_k.shape)\n",
    "# 1. Compute the input to the hidden layer (hL)\n",
    "in_hL = np.dot(X, W_k) + b_k  # Linear transformation to hidden layer\n",
    "print('Shape of the input to the hidden layer: ', in_hL.shape)\n",
    "# 2. Apply activation function (logistic) to get hidden layer output (hL_out)\n",
    "hL_out = g_fun_logit(in_hL) \n",
    "print('Shape of the output of the hidden layer: ', in_hL.shape)\n",
    "# 3. Compute the input to the output layer (out_in)\n",
    "print('Shape of the hidden-to-output layer weight matrix: ', V_k.shape)\n",
    "out_in = np.dot(hL_out, V_k) + b  # Linear transformation to output layer\n",
    "print('Shape of the input to the output layer: ', out_in.shape)\n",
    "# 4. For regression, the output is just the linear output (no activation)\n",
    "yhat = out_in\n",
    "print('Shape of the final output predictions: ', yhat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7249f822",
   "metadata": {},
   "source": [
    "We recommend that you rerun the above code cell multiple times using different values for the number of hidden nodes, e.g., `n_hidden_nodes = 2`, `n_hidden_nodes = 5`, or `n_hidden_nodes = 10`. The print-statements allow you to observe how different settings for the number of hidden nodes translate into weigfht matrices of different sizes. Note also that the dimension of the input data and the NN outout do not change.\n",
    "\n",
    "Having exemplified the data flow through a three-layer NN, we can use the demo to abstract a bit and think about deep neural networks with many layers and nodes. The lecture sketched the calculations of such networks using matrix multiplications and activation functions as follows.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/dl/Folie24.PNG\" alt=\"NN forward path\" width=\"320\" />\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/dl/Folie25.PNG\" alt=\"NN forward path\" width=\"320\" />\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Above we have seen, for each hidden layer, we need two weight matrices, one connecting the hidden layer to the previous layer, and one connecting it to the proceeding layer. The dimensionality of these matrices depends on the sizes of the involved layers. For each note in one layer, we need one weight (i.e., edge) to connect it to one node in the other layer. For example, above we defined our first matrix of weights as: \n",
    "\n",
    "```python\n",
    "W1 = np.random.randn(n_columns, n_hidden_nodes) \n",
    "```\n",
    "\n",
    "Once we know - or have decided on - the architecture of the NN (i.e., how many layers and the sizes of the layers), we can create all weight matrices accordingly. With these weight matrices, we can calculate the NN output, as exemplified above. However, the code can easily get messy unless we put effort into organizing the calculations and writing efficient code. For example, proper deep learning libraries like [Pytorch](https://pytorch.org/) or [Tensorflow](https://www.tensorflow.org/) use object-oriented programming concepts and abstract layers as classes. You can find many demos of how to implement a neural network *from scratch* online if interested. We will switch to ready-to-use implementations from here on and explore the functionality of *sklearn* for deep learning in the following. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb60ff",
   "metadata": {},
   "source": [
    "# Neural networks in SKLEARN\n",
    "The *sklearn* library provides two classes for NNs, `MLPRegressor` and `MLPClassifier`, which support, respectively, regression and classification problems. \n",
    "\n",
    "## An NN for the synthetic data\n",
    "Let's first revisit our synthetic regression data from above. Although a NN is much of an overkill for this simple toy problem, the dataset sufficies to get familiar with the key functions for training and prediction. Unsurprisingly, these are the same that every *sklearn* algorithm provides, `.fit()` and `.predict()`. The next code cell provides a minimalistic demo of how to train and apply a NN to our toy data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a5c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor  # importing the MLPRegressor class\n",
    "\n",
    "nn = MLPRegressor(random_state=r_state, activation='logistic', \n",
    "                  hidden_layer_sizes=(10, 8, 6, 2), max_iter=2000)\n",
    "nn.fit(X, y)  # training the NN on the synthetic data\n",
    "yhat_nn = nn.predict(X)  # making predictions with the trained NN\n",
    "\n",
    "results_df['Yhat_nn'] = yhat_nn  # Showcase NN predictions in our dataframe\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression().fit(X, y)\n",
    "results_df[\"LinReg\"] = linreg.predict(X)\n",
    "results_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154b4d9a",
   "metadata": {},
   "source": [
    "You will have noticed a **warning message** from *sklearn* when executing the code cell above. The message informs us that the training of the NN did not converge. Eyeballing the predictions of the NN, you can also see that these are much better than the random predictions from before. However, they are still notably different from the true values of the target. This is surprising because a powerful NN should be able to perfectly fit such a simple linear regression problem. The reasons for suboptimal performance are twofold.\n",
    "\n",
    "First, we did not configure our NN in any way. We did not say how many layers we want. We did not say how large these layers should be. We did not specify the activation function. We did not say how many iterations we allow for training. We did not say anything, but relied exclusively on default settings as implemented in *sklearn*. That is always a bad idea.\n",
    "\n",
    "Second, when working with NNs, we must prepare the data properly. NNs are sensitive to the scaling of the input features. In our synthetic data, the two features have different scales. The scale of the target is also different. These differences in scales make it hard for the NN to learn the relationship between features and the target. We can fix the problem by standardizing the features using, for example, `StandardScaler` from *sklearn*:\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "Furthermore, we should also scale the target variable for regression problems. From a theoretical perspective, scaling the target should not be necessary. Practically, however, it is often  difficult to train a good NN without scaling both, the features and the target. Adding scaling, the NN predictions become much better, as illustrated in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90234ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Scale feature values in our dataset and also the target\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1, 1)).flatten()  # some reshaping is necessary to get the code to work. You do not have to worry about this now.\n",
    "\n",
    "# Repeat neural network training using the scaled data\n",
    "nn = MLPRegressor(random_state=r_state)  # using one hidden layer with 5 nodes\n",
    "nn.fit(X_scaled, y_scaled)  # training the NN on the synthetic data\n",
    "yhat_nn = nn.predict(X_scaled)  # making predictions with the trained NN\n",
    "yhat_nn = scaler.inverse_transform(yhat_nn.reshape(-1, 1)).flatten()  # inverse transform to original scale\n",
    "\n",
    "results_df['Yhat_nn'] = yhat_nn  # Showcase NN predictions in our dataframe\n",
    "results_df.head(10)\n",
    "\n",
    "print(f'R2 score of the NN is {r2_score(y_true=y, y_pred=yhat_nn):.2f}')\n",
    "results_df[[\"Y\", \"Yhat_nn\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f589b9",
   "metadata": {},
   "source": [
    "Observing $R^2$ being close to 1 indicates that the NN has learned the underlying linear relationship in the data almost perfectly. However, the synthetic regression problem was very simple and a linear regression model would have performed just as good. Try it out yourself if interested:\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "yhat_lr = lr.predict(X)\n",
    "print(f'R2 score of the LR is {r2_score(y_true=y, y_pred=yhat_lr):.2f}')\n",
    "```\n",
    "It is about time to move on to some more interesting and challenging data. Our standard HMEQ dataset will serve this purpose in the next section. Also, we still have to explore options to configure and tune NNs in *sklearn*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91904e69",
   "metadata": {},
   "source": [
    "## A simple NN for classification\n",
    "We explore the use of NNs for classification problems using our standard HMEQ dataset. Running the next code cell, you can download the **preprocessed data** right from our [GitHub repository](https://github.com/Humboldt-WI/bads), whereby the actual preprocessing is based on the helper function of [Tutorial 5](https://github.com/Humboldt-WI/bads/blob/master/tutorial_notebooks/5_data_prep_tasks.ipynb); just in case you are interested. To stress that this data is ready-for-use, the following code trains a logistic regression model, which will later serve as a baseline to our NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1242423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data from GitHub\n",
    "data_url = 'https://raw.githubusercontent.com/Humboldt-WI/bads/master/data/hmeq_prepared.csv'\n",
    "df = pd.read_csv(data_url)\n",
    "# Separate features and target\n",
    "X = df.copy()\n",
    "y = X.pop(\"BAD\")\n",
    "# Train a logistic regression model as a baseline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model = LogisticRegression(random_state=r_state)\n",
    "lr_model.fit(X, y)\n",
    "# Obtain probabilistic predictions\n",
    "yhat_lr = lr_model.predict_proba(X)[:, 1]  # get predicted probabilities for the positive class\n",
    "# Compute AUC score for information\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(f'ROC AUC score of the LR is {roc_auc_score(y_true=y, y_score=yhat_lr):.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aacf02",
   "metadata": {},
   "source": [
    "### NN Exercises\n",
    "To practice the use of NNs in *sklearn*, solve the following exercise tasks. For all tasks, you can use the entire dataset $X$ and labels $y$, as available in the notebook in the variables `X` and `y`, respectively. **Do not worry about train-test-splitting** for now.  \n",
    "\n",
    "#### 1. Classification network\n",
    "- Train a NN for the HMEQ classification problem. Use the `MLPClassifier` class from *sklearn.neural_network*. Fit the NN on the training data and make predictions on the dataset. All steps have been illustrated above for the regression case. You only need to use class `MLPClassifier` instead of `MLPRegressor`.\n",
    "- Compute the AUC of your NN, as illustrated above for the logistic regression baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9293a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for your solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354e65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As seen above, the MLPClassifier often triggers convergence warnings\n",
    "# Running this code cell will suppress these warnings for the rest of the notebook \n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c610f0",
   "metadata": {},
   "source": [
    "#### 2. NN architecture\n",
    "Exercise 1 provided a first estimate of how well the NN can fit the training data. In this exercise, you examine how its ability to fit the training set varies when we vary the NN architecture. \n",
    "- Read the documentation of the `MLPClassifier` class in *sklearn* to understand how to configure the NN architecture. In particular, focus on the parameter `hidden_layer_sizes`.\n",
    "- Write down the NN architecture of the network that you created in exercise 1. How many layers and nodes had your NN?\n",
    "- Train an NN with the following architecture and assess its AUC on the training data:\n",
    "  - No. of layers: 4\n",
    "  - Nodes per layer: 10, 5, 5, 3\n",
    "  - Activation function: TanH \n",
    "- (Optional) Experiment with different architectures. Create a `List` with different candidate values for the number of layers, and a second `List` with candidate values for the number of nodes per (hidden layer). Write a nested `for` loop to iterate through **all combinations** of these candidates. For each combination, train a NN and compute its AUC on the training data. Store the results in a third  `List` for later inspection.\n",
    "```python\n",
    "# Example code snippet for the optional task\n",
    "layer_options = [2, 3, 4]\n",
    "node_options = [5, 10, 20]\n",
    "results = []\n",
    "# Two nested for loops to iterate through all combinations\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b65331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for your solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb551e7",
   "metadata": {},
   "source": [
    "#### 3. Random initialization\n",
    "NNs are typically initialized randomly. This means that each time you train a NN, you may get different results. In this exercise, you examine the strengths of this variation, that is the sensitivity of the NN with respect to the random initialization.\n",
    "- Call the below helper function `random_seed_experiment()` to create a list with random seed values for your experiment. \n",
    "- Loop over the random seed value. For each value, train an `MLPClassifier` on the training data, compute this classifier's mean squared error (MSE) on the training data, and store the result. For the NN, you can use any of the architectures explored in exercise 1. \n",
    "- Show the variation of the NN's MSE in a box-plot\n",
    "- (Optional) Consider repeating the experiment multiple times with different settings for the number of iterations (argument `iterations` in the function `random_seed_experiment()`) to better understand how much NN training results vary across different random weight initializations. \n",
    "- (Optional) Consider repeating the experiment for different NN architectures to see how sensitive different architectures are to random initialization. \n",
    "\n",
    "> Note that training multiple NNs in a loop can take some time. Be patient!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29dbded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for exercise 3\n",
    "def random_seed_experiment(iterations :int = 10 ):\n",
    "    \"\"\"\n",
    "    Helper functions that randomly generates integer values as random seeds.\n",
    "\n",
    "    Inputs:\n",
    "        iterations (int) = 10: \n",
    "            Number of random seeds to generate.\n",
    "        \n",
    "    Returns:\n",
    "        seeds (list): \n",
    "            List of randomly generated integer seeds.\n",
    "\n",
    "    Example:\n",
    "        seeds = random_seed_experiment(iterations=5)\n",
    "        print(seeds)\n",
    "    \"\"\"                          \n",
    "    seeds = []\n",
    "    for _ in range(iterations):\n",
    "        seed = np.random.randint(0, 10000)  # Generate a random integer seed\n",
    "        seeds.append(seed)\n",
    "    return seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bce0e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for your solution\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "B0rxPs4QEGtz",
    "27sCENzmoGcX"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "bads310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
