{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iGz0qnmS-UK"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorial_notebooks/5_data_prep_solutions.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHScrtl_S-UK"
   },
   "source": [
    "# Tutorial 5 - Pandas library for data preparation\n",
    "\n",
    "<span style=\"font-weight: bold; color: red;\">This version includes solutions to the exercises. </span>\n",
    "\n",
    "The notebook revisits our lecture on EDA and data preparation. In this scope, you will further deepen your understanding of **Pandas**, the goto library for working with tabular data in Python. We will exemplify two core Pandas classes *data series* and *data frame*. To that end, the demo notebook introduces a real-world data associated with credit scoring. \n",
    "\n",
    "Here is the outline for today:\n",
    "- The HMEQ data set\n",
    "- Pandas reloaded ...\n",
    "- Data preparation\n",
    "- Explanatory data analysis\n",
    "\n",
    "Before moving on, let's import some of our standard library so that we have them ready when we need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33ZdGrYCS-UK"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BKS1jVVS-UK"
   },
   "source": [
    "## The HMEQ data set\n",
    "Our data set, called the  \"Home Equity\" or, in brief, HMEQ data set, is provided by www.creditriskanalytics.net. It comprises  information about a set of borrowers, which are categorized along demographic variables and variables concerning their business relationship with the lender. A binary target variable called 'BAD' is  provided and indicates whether a borrower has repaid her/his debt. You can think of the data as a standard use case of binary classification.\n",
    "\n",
    "You obtain the data, together with other interesting finance data sets, directly from www.creditriskanalytics.net. The website also provides a brief description of the data set. Specifically, the data set consists of 5,960 observations and 13 features including the target variable. The variables are defined as follows:\n",
    "\n",
    "- BAD: the target variable, 1=default; 0=non-default \n",
    "- LOAN: amount of the loan request\n",
    "- MORTDUE: amount due on an existing mortgage\n",
    "- VALUE: value of current property\n",
    "- REASON: DebtCon=debt consolidation; HomeImp=home improvement\n",
    "- JOB: occupational categories\n",
    "- YOJ: years at present job\n",
    "- DEROG: number of major derogatory reports\n",
    "- DELINQ: number of delinquent credit lines\n",
    "- CLAGE: age of oldest credit line in months\n",
    "- NINQ: number of recent credit inquiries\n",
    "- CLNO: number of credit lines\n",
    "- DEBTINC: debt-to-income ratio\n",
    "\n",
    "As you can see, the features aim at describing the financial situation of a borrower. We will keep using the data set for many modeling tasks in this demo notebook and future demo notebook. So it makes sense to familiarize yourself with the above features. Make sure you understand what type of information they provide and what this information might reveal about the risk of defaulting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFgoPwxcS-UL"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dhX-K3MS-UL"
   },
   "source": [
    "# Foundations of the Pandas Library\n",
    "\n",
    "## Loading data from the WWW\n",
    "The `Pandas` library supports various ways to load data from, e.g., your hard disk, a server somewhere in your network, etc. Here, we consider the easiest setting, which is loading data from the web. All we need for this is an URL. The following code loads the data directly from our [BADS repository](https://github.com/Humboldt-WI/bads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CFQPyjiS-UN",
    "outputId": "17a8c028-8539-4a22-9c1c-359146d2761a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd  # import library\n",
    "\n",
    "# Load the data directly from GitHub\n",
    "data_url = 'https://raw.githubusercontent.com/Humboldt-WI/bads/master/data/hmeq.csv'\n",
    "df = pd.read_csv(data_url)\n",
    "\n",
    "# To show that everything worked out, we can print the first few rows of the data frame\n",
    "df.head(10)  # print ten rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mj0wk9gzS-UN"
   },
   "source": [
    "## Eyeballing data \n",
    "The Pandas data frame provides a ton of useful functions for data handling. We begin with showcasing some standard functions that one needs every time when working with data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PH9g48kDS-UN",
    "outputId": "4d778539-f9ee-4e39-9bbe-22063bc53025"
   },
   "outputs": [],
   "source": [
    "# Query some properties of the data\n",
    "print('Dimensionality of the data is {}'.format(df.shape))  # .shape returns a tuple\n",
    "print('The data set has {} cases.'.format(df.shape[0]))     # we can also index the elements of that tuple\n",
    "print('The total number of elements is {}.'.format(df.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DdYdSXezS-UN",
    "outputId": "964fceae-a2a3-4a05-f5fb-5a4cf04755f1"
   },
   "outputs": [],
   "source": [
    "# Obtain a more technical overview of the data \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnNVr6oXS-UO"
   },
   "source": [
    "The above output displays some useful information how exactly data is stored. We learn about the data type of each feature (i.e, each *data series* object), missing values, and the total amount of memory that the data consumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3_MZA2rS-UO",
    "outputId": "1a0b20f0-3173-427c-a9bb-4cf55500223e"
   },
   "outputs": [],
   "source": [
    "# Produce summary statistics (to R-programmers: this is equivalent to the famous R function summary())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5Y4xUmnS-UP"
   },
   "source": [
    "The previous demos gave as an overview of the data. However, if you compare the output to the `describe()` function list of features given on the www.creditriskanalytics.net website (see above), you will notice that we are missing some features. For example, we lack a summary of the feature *REASON*; same with *JOB*. If you think about it, that actually makes sense. The result from the function `info()` showed how these features are stored as data type *object*. They are not stored as numeric variables. Consequently, statistical / mathematical operations like computing a mean or quantile are undefined and cannot be computed for these variables. That said, you can still force the `describe()` function to consider all features in its output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating data\n",
    "We discussed indexing and slicing in the contexts of Python `lists` and other containers like dictionaries. In `Pandas`, `Numpy`, and other libraries, indexing/slicing are equally important and work in similar ways. Here, we provide a few more demos on common ways to use indexing in `Pandas`. A web search for \"pandas data frame indexing\" will provide many additional insights if you are interested. Likewise, feel free to skip this part if you already feel comfortable with data frame indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic indexing of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing a single column by name\n",
    "df['BAD']\n",
    "# Alternatively, you can access a single column using dot-notation\n",
    "df.BAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the *R* programmers: we can index our data in a way similar to *R*. Note the use of `loc[]`. This is a special type of syntax you need to memorize. Also note that we specify the columns we want to index using a `list`. Hence the inner box bracket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-style indexing of selected rows and columns\n",
    "df.loc[0:4, [\"BAD\", \"LOAN\"]]  # select row 0, 1, 2, 3 and for those rows only the columns BAD and LOAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access columns by a numerical index using .iloc\n",
    "df.iloc[0:4, 0]\n",
    "df.iloc[0:4, [0, 3, 5]]\n",
    "df.iloc[0:4, np.arange(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few cautionary notes on numerical indexing in Python. The function `iloc()` considers the index of the data frame. In the above output, this is the left-most column without header. We have not defined a custom row index and Python uses consecutive integer numbers by default. However, a data frame could also have a custom index. In such a case, calls to `iloc()` need to refer to the custom index. It is good practice to eyeball a data frame and verify the way in which rows are indexed prior to using `iloc()`.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other common forms of indexing and subset selection\n",
    "It is also common practice to select rows based on comparisons of feature values using. You can achieve this using `.loc`. Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.BAD == 1, :]  # Get all observations with target variable BAD = 1. The : means you want to retrieve all columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"NINQ\"]>12, [\"LOAN\", \"VALUE\", \"NINQ\"]]  # Another example where we select only a subset of the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with high-dimensional data sets, you will often perform certain actions only with columns or a specific data type. To that end, you should know the function `select_dtypes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(float)  # select all numerical columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9KB26FrS-UR"
   },
   "source": [
    "## Manipulating data\n",
    "We often have to manipulate data. For example, imputing missing values as part of data preparation (see later) will require us to change the data stored in a data frame. `Pandas` supports many ways to manipulate data. Let's introduce a few common options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5EaoWGmS-UR"
   },
   "source": [
    "### Using in-build Pandas functions\n",
    "Many functions that `Pandas`provide result in data changes. One example is the `sort_values` function, which we demonstrate below. By default, functions like `sort_values` do not alter the data in a `DataFrame` directly. Instead, they return a new `DataFrame` in which the data was changed. Here is an example. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df.sort_values(by=\"LOAN\", ascending=False)  # We can specify the column by which to sort and the order; next to other arguments\n",
    "df_sorted.head(10)  # Print a preview of the data; like above when introducing the method .head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlQ9W9kxS-US"
   },
   "source": [
    "Note the row index (left-most column). The index tells us that the order of the rows is different. That was to be expected because we sorted the the feature *LOAN*. In the original data, which we store in the variable `df`, we still have the original row order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of the demo was to show that, by default, `Pandas` function will not alter the `DataFrame` directly. Therefore, you see many codes of this form: \n",
    "```\n",
    "new_data_frame = old_data_frame.someFunction()\n",
    "``` \n",
    "Occasionally, you can overwrite this default behavior. Some `Pandas` functions provide an argument `inplace`. Setting this argument to `True` would then alter a `Data Frame` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sort_values(by=\"LOAN\", ascending=False, inplace=True)  # Running this line would change your data frame\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NW4D58fUS-UT"
   },
   "source": [
    "### The apply function\n",
    "If you have used R, you will know the `apply()` function. It kinda does what the name suggests. It let's you define a function and apply that function to every element in a data frame. Combine that with indexing and you obtain a powerful way to selectively alter your data. \n",
    "<br>\n",
    "We provide some demos in the following, where, for simplicity, we consider only the numerical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlC_-KNbS-UT"
   },
   "outputs": [],
   "source": [
    "df_numerical = df.select_dtypes(float) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-itB_qwS-UT"
   },
   "source": [
    "Silly example: say you want to square the values of all your features. You can achieve this by calling `.apply()` on a `DataFrame` providing a suitable function as argument. In this - silly - example, we can use the in-built `Numpy` function `square`. However, we could also use a custom function, or define the function directly within the call to `.apply`. The latter is a more advanced Python concept known as *lambda function*. Websearch for it if interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AiQxmyzlS-UT",
    "outputId": "d93ce20b-14d3-4b37-fad4-512fb65aae02"
   },
   "outputs": [],
   "source": [
    "# All three examples below are equivalent\n",
    "\n",
    "# Using apply together with an existing function\n",
    "df_squared = df_numerical.apply(np.square)  # note that the reference the function. Thus it is np.square and not np.square(). When adding brackets, we call the function. \n",
    "\n",
    "# Using apply together with a custom function\n",
    "def my_square(x):\n",
    "    return x*x\n",
    "\n",
    "df_squared = df_numerical.apply(my_square)\n",
    "\n",
    "# Using apply together with a lamda function\n",
    "df_squared = df_numerical.apply(lambda x: x * x) # you can define a function directly like here, we have a square function\n",
    "\n",
    "df_squared.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFnWNqP1S-UU"
   },
   "source": [
    "So this was apply in action. Writing your own custom function and then feeding every column of a data frame or a selection thereof - by indexing - let you perform some powerful operations. We will see more meaningful use cases as we go along (spoiler alert: we use `apply()` for outlier handling below) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7uvFSUsS-UU"
   },
   "source": [
    "# Data preparation\n",
    "Data preparation is a mega-topic. It will accompany us throughout the whole course. I this part, we focus on some typical issues in our data and demonstrate how to perform standard data prep tasks using `Pandas`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZL4c5HyS-UU"
   },
   "source": [
    "### Altering data types\n",
    "We start with a rather technical bit, data types. Remember the way our data is stored at the moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6zC5rYPS-UU",
    "outputId": "623dc9d4-fbc1-49ed-df5a-a8af6c4fc3ce"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QP1iSYXDS-UU"
   },
   "source": [
    "The features *JOB* and *REASON* are stored as data type `object`. This is the most general data type in Python. A variable of this type can store pretty much any piece of data, numbers, text, dates, times, ... This generality has a price. First, storing data as data type `object` consumes a lot of memory. Second, we cannot access specific functionality that is available for a specific data type only. Functions to manipulate text are an example. These are available for data of type `string` but not for data of type `object`. \n",
    "<br>\n",
    "In our case, the two features that Pandas stores as objects are actually categorical variables. We can easily verify this using, e.g., `value_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbBgoi3uS-UU",
    "outputId": "3fc66b34-74e2-472b-a185-0c929b7edc33"
   },
   "outputs": [],
   "source": [
    "print(df.REASON.value_counts())  # so REASON is a binary variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3FeAT1dS-UU",
    "outputId": "382ae29c-898f-4505-f9b2-7ddd148bb057"
   },
   "outputs": [],
   "source": [
    "print(df.JOB.value_counts())  # JOB is a categorical variable with many levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4JsilV3S-UU"
   },
   "source": [
    "Knowing our two \"object features\" are categories, we should alter their data type accordingly. To that end, we make use of the function `astype`, which facilitates converting one data type into another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MO6IxmTS-UU",
    "outputId": "8f52d60c-034d-4ecb-9fad-873995bbc710"
   },
   "outputs": [],
   "source": [
    "# Code categories properly \n",
    "df['REASON'] = df['REASON'].astype('category')\n",
    "df['JOB'] = df['JOB'].astype('category')\n",
    "df.info()  # verify the conversion was successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXA9BTp_S-UV"
   },
   "source": [
    "Although it does not really matter for this tiny data set, note that the conversion from object to category has reduced the amount of memory that the data frame consumes. On my machine, we need 524.2 KB after the translation, whereas we needed more than 600 KB for the original data frame. If you work with millions of observations the above conversion will result in a significant reduction of memory consumption. If memory consumption is an issue, we could a significant further reduction by reducing the precision of the numerical variables. Downcasting from float64 to float32 bit might is likely ok for predictive modeling. Also, the target variable is stored as an integer but we know that it has only two states. Thus, we can convert the target to a boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-K5ln75FS-UV"
   },
   "outputs": [],
   "source": [
    "# The target variable has only two states so that we can store it as a boolean\n",
    "df['BAD'] = df['BAD'].astype('bool')\n",
    "\n",
    "# For simplicity, we also convert LOAN to a float so that all numeric variables are of type float\n",
    "df['LOAN'] = df['LOAN'].astype(np.float64)\n",
    "\n",
    "# Last, let's change all numeric variables from float64 to float32 to reduce memory consumption\n",
    "num_vars = df.select_dtypes(include=np.float64).columns\n",
    "df[num_vars] = df[num_vars].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhqJ1Ty0S-UV"
   },
   "source": [
    "Invest some time to understand the above codes. Our examples start to combine multiple pieces of functionality. For example, the above demo uses indexing, functions, and function arguments to perform tasks. Keep practicing and you will become familiar with the syntax.\n",
    "<br>\n",
    "Finally, let's verify our changes once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cuBwfDSS-UV",
    "outputId": "e07680f7-dac9-423e-aa82-ee14b05f690f"
   },
   "outputs": [],
   "source": [
    "# Check memory consumption after the conversions\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8t60vNRS-UV"
   },
   "source": [
    "In total, our type conversions reduced memory consumption by more than a half. You might want to bear this potential in mind when using your computer to process larger data sets. Should you be interested in some more information on memory efficiency, have a look at this post at [TowardDataScience.com](https://towardsdatascience.com/pandas-save-memory-with-these-simple-tricks-943841f8c32). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ypE7GuOS-UW"
   },
   "source": [
    "## Missing values\n",
    "You might have already noticed that our data contains many missing values. This is common when working with real data. Likewise, handling missing values is a standard task in data preparation. `Pandas` provides the function `.isna()` as entry point to the corresponding functionality and helps with identifying the relevant cases.\n",
    "\n",
    "*Note*: `Pandas` also supports an equivalent function called `.isnull()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeahGMfES-UW",
    "outputId": "9141fa4f-01b3-4f3f-fa63-5e744bcb1c5f"
   },
   "outputs": [],
   "source": [
    "# Boolean mask of same size as the data frame to access missing values via indexing\n",
    "missing_mask = df.isna()\n",
    "\n",
    "print(f'Dimension of the mask: {missing_mask.shape}')\n",
    "print(f'Dimension of the data frame: {df.shape}')\n",
    "\n",
    "missing_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aBde7TBS-UW"
   },
   "source": [
    "We can now count the number of missing values per row or per column or in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_DgQ7QHS-UW",
    "outputId": "b931d0ae-fece-4d25-d8d9-0a352b4da582"
   },
   "outputs": [],
   "source": [
    "# missing values per row\n",
    "miss_per_row = missing_mask.sum(axis=1)\n",
    "print('Missing values per row:\\n', miss_per_row)\n",
    "\n",
    "# missing values per column\n",
    "miss_per_col = missing_mask.sum(axis=0)\n",
    "print('Missing values per column:\\n', miss_per_col )\n",
    "\n",
    "# count the total number of missing values\n",
    "n_total_missing = missing_mask.sum().sum()\n",
    "print(f'Total number of missing values: {n_total_missing}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_9jyQY_S-UW"
   },
   "source": [
    "It can be useful to visualize the *missingness* in a data set by means of a heatmap. Note how the below example gives you a good intuition of how and where the data set is affected by missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggtNOCJKS-UW",
    "outputId": "c66c163d-4ae4-46ec-978a-49cca891d75d"
   },
   "outputs": [],
   "source": [
    "sns.heatmap(df.isna())  # quick visualization of the missing values in our data set\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vnw8ySRgS-UW"
   },
   "source": [
    "### Categorical features\n",
    "Let's start with the two categorical features. The heatmap suggests that `REASON` exhibits more missing values than `JOB`. We will treat them differently for the sake of illustration. Now that we start altering our data frame more seriously, it is a good idea to make a copy of the data so that we can easily go back to a previous state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuSqLNgzS-UW"
   },
   "outputs": [],
   "source": [
    "# copy data\n",
    "df_orig = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7nqgRZuS-UX"
   },
   "source": [
    "#### Adding a new category level\n",
    "One way to treat missing values in a categorical feature is to introduce a new category level \"IsMissing\". We will demonstrate this approach for the feature *REASON*. \n",
    "<br>One feature of the category data type in Pandas is that category levels are managed. We cannot add levels directly. Thus, before assigning the missing values our new category level *IsMissing*, we first need to introduce this level. We basically tell our data frame that *IsMissing* is another suitable entry for *REASON* next to the levels that already exist in the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdBWbWtfS-UX",
    "outputId": "b0338de2-ff64-4ff7-d922-1142f7f92879"
   },
   "outputs": [],
   "source": [
    "# Variable REASON: we treat missing values as a new category level.\n",
    "# First we need to add a new level\n",
    "df.REASON = df.REASON.cat.add_categories(['IsMissing'])\n",
    "\n",
    "# Now we can do the replacement\n",
    "df.REASON[df.REASON.isnull() ] = \"IsMissing\"\n",
    "df.REASON.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXAjFQFtS-UX",
    "outputId": "b5d3bf61-c04e-4d4f-a5f0-d345f8d3b184"
   },
   "outputs": [],
   "source": [
    "df.REASON.isna().sum()  # verify that no more missing values exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tf_LOVmPS-UX"
   },
   "source": [
    "#### Mode replacement\n",
    "For the feature *JOB*, which is multinomial, we replace missing values with the mode. Please note that this is a crude way to handle missing values. I'm not endorsing it! But you should have at least seen a demo. Here it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTNUN63yS-UX",
    "outputId": "a5ecc720-55f7-48b2-967a-27b387f50ad1"
   },
   "outputs": [],
   "source": [
    "# Determine the mode\n",
    "mode_of_job = df.JOB.mode()\n",
    "print(mode_of_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4M32XJxQS-UY",
    "outputId": "a6d86590-c2d8-4716-adcf-9fa8e258b3ac"
   },
   "outputs": [],
   "source": [
    "# replace missing values with the mode\n",
    "df.JOB[df.JOB.isnull() ] = df.JOB.mode()[0]  # the index [0] is necessary as the result of calling mode() is a Pandas Series\n",
    "# verify that no more missing values exist\n",
    "df.JOB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4SvhbFnS-UY",
    "outputId": "ecb4b0ef-ca74-4890-cc24-0b46e0738747"
   },
   "outputs": [],
   "source": [
    "# Verify more seriously that missing value replacement was successful\n",
    "if df.REASON.isnull().any() == False and df.JOB.isnull().any() == False:\n",
    "    print('well done!')\n",
    "else:\n",
    "    print('ups')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXZaNFpGS-UZ"
   },
   "source": [
    "### Numerical features\n",
    "We have a lot of numerical features. To keep things simple, we simply replace all missing values with the median. Again, this is  a crude approach that should be applied with care; if at all. However, it nicely shows how we can process several columns at once using a loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjlpGozNS-UZ"
   },
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='float32').columns:  # loop over all numeric columns\n",
    "    if df[col].isna().sum() > 0:                         # check if there are any missing values in the current feature\n",
    "        m = df[col].median(skipna=True)                  # compute the median of that feature\n",
    "        df[col].fillna(m, inplace=True)                  # replace missing values with the median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should you wonder whether it is necessary to write a loop to perform this rather standard operation, the answer is no. You could achieve the same result more elegantly when combining the `fillna()` method with a call to the method `transform()`. Here is how this would look like:\n",
    "```python\n",
    "# Alternative approach to impute missing values with the feature median\n",
    "cols = df.select_dtypes(include='float32').columns \n",
    "\n",
    "df[cols] = df[cols].transform(lambda x: x.fillna(x.median()))\n",
    "``` \n",
    "The function `transform()` applies a function to each column of the DataFrame. The lambda function takes each column, fills the missing values with the median of that column, and returns the transformed column. This way, you avoid looping over each column manually. This version can be considered more elegant, but our first shot, writing a loop, may legitimately be considered more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dukR_VzGS-UZ",
    "outputId": "13f49cee-b983-4e17-ddb3-ebef2477dff7"
   },
   "outputs": [],
   "source": [
    "# Verify there are no more missing values in the data\n",
    "n_total_missing = df.isna().sum().sum()\n",
    "if  n_total_missing == 0:\n",
    "    print('Well done, no more missing values!')\n",
    "else:\n",
    "    print(f'Ups! There are still {n_total_missing} missing values.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdaOAlZTTEZT"
   },
   "source": [
    "# Summary of useful Pandas functions\n",
    "\n",
    "Many useful tricks with `Pandas` (here `df` is a pandas DataFrame and `col` is one of its columns):\n",
    "\n",
    "| Goal | Possible Code |\n",
    "| --- | --- |\n",
    "| Get df column (column name must have no spaces) | `df.col` |\n",
    "| Get df column | `df[\"col\"]` |\n",
    "| Example condition: only select rows where `col1 > 1` | `df[\"col\"] > 1` |\n",
    "| Use index names to select rows and columns | `df.loc[row_list, col_list]` |\n",
    "| Use index numbers to select rows and columns | `df.iloc[row_list, col_list]` |\n",
    "| Get df column based on a condition | `df.loc[condition, ['col2','col3',...]]`|\n",
    "| Group df by values of `col` | `df.groupby(\"col\")` |\n",
    "| Perform function on `col2` for each group of `col1` | `df.groupby(\"col1\")[\"col2\"].fun()` |\n",
    "| Find value counts of each value in `col` | `df.groupby(['col']).size()`| \n",
    "| Get column mean and ignore null values | `df[\"col\"].mean(skipna=True)` |\n",
    "| Get column mode | `df[\"col\"].mode()` |\n",
    "| Get column median | `df[\"col\"].median()` |\n",
    "| Get rows of the 95th quantile of `col` | `df[\"col\"].quantile(q=0.95)` |\n",
    "| Filter `df` with a boolean condition | `df.query(condition)` |\n",
    "| Create tally of `col2` by values of `col1` | `pd.crosstab(df['col1'], df['col2']`) |\n",
    "| Pivot rows and columns | `df.pivot(index='col1', columns='col2', values='col3')` | \n",
    "| Sort values by `col` and save `df` in this order | `df.sort_values(by='col', inplace=True)` |\n",
    "| Apply function to each column of `df` | `df.apply(fun)` |\n",
    "| Save `df` as CSV in working directory | `df.to_csv('./file_name.csv', index=False)` |\n",
    "| Count the number of times each value occurs | `df['col'].value_counts()` |\n",
    "| Change column's data type | `df['col'] = df['col'].astype('type')` |\n",
    "| Create boolean matrix of `df` where `True` indicates null value | `df.isnull()` | \n",
    "| Create boolean matrix of `df` where `True` indicates null value | `df.isna()` | \n",
    "| Create copy of df | `df_copy = df.copy()` |\n",
    "| Add new category to categorical variable | `df.col.cat.add_categories(['New C'], inplace=True)` |\n",
    "| Replace null values with `\"IsMissing\"` | `df.col[df.col.isnull()] = \"IsMissing\"` |\n",
    "| Fill missing values with median and save `df` | `df['col'].fillna(median_value, inplace=True)` |\n",
    "| Calculate time at execution (must import `time` library) | `time.time()` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dependency of loan amount and credit risk\n",
    "Examine the dependency between the loan amount (i.e., feature `LOAN`) and the default risk. You find  information on the latter in the column `df[\"BAD\"]`. A value of 1 indicates that a borrower is a defaulter (i.e., bad risk). Specifically:\n",
    "1. Calculate the average of the feature `LOAN`\n",
    "2. Calculate the average `LOAN` amount separately for bad and good risk using logical indexing. \n",
    "3. Interpret the results of your analysis. Is there a dependency between `LOAN` and default risk?\n",
    "4. Re-calculate the average `LOAN` amount for good and bad risks. This time, make use of the function `group_by`, which exists for data frames.  \n",
    "5. Extend the previous task by computing the group-wise median for all numerical features in the data frame\n",
    "\n",
    "\n",
    "**Extension:** a nice extension of subtasks 1 to 3 would be to secure your interpretation with a statistical hypothesis test. Perhaps you know a suitable test. If not, run a web search for, e.g., *“statistical test for difference in means python”*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solutions to the exercises\n",
    "#--------------------------------------------------\n",
    "# 1. Average income in the data\n",
    "df[\"LOAN\"].mean()\n",
    "\n",
    "# 2. Average income among goods and bads separately\n",
    "ix_allbad = df[\"BAD\"] == True \n",
    "avg_bad = df.loc[ix_allbad, \"LOAN\"].mean()\n",
    "print(\"Average loan amount among BADs: \", avg_bad)\n",
    "print(\"Average loan amount among GOODs: \", df.loc[~ix_allbad, \"LOAN\"].mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Average feature values using groupby\n",
    "\n",
    "# Note that the result of the grouping is a data frame. Thus, you can first group\n",
    "# the data and then apply any other function that works for data frames including\n",
    "# calculating aggregates and indexing. \n",
    "# To solve the task, we first group, then index, and finally calculate the mean \n",
    "# and do all of that in only one line \n",
    "df.groupby(by=\"BAD\")[\"LOAN\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Group-wise median of all numerical features\n",
    "df.groupby(by=\"BAD\").median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Outliers\n",
    "The lecture introduced a rule of thumb saying that, for a given feature, a feature value $x$ can be considered an outlier if \n",
    "$$x >q_3(X) + 1.5 \\cdot IQR(X)$$\n",
    "\n",
    "where $q_3(X)$ denotes the third quantile of the distribution of feature $X$ and $IQR(X)$ the corresponding inter-quartile range.\n",
    "\n",
    "1. Use the `Pandas` method `quantile` to compute the third and first quartile of feature `LOAN`.\n",
    "2. Compute the threshold value that a feature value $x$ must not exceed according to the above equation. Store the result in a variable. \n",
    "3. Use logical indexing to identify all upper outliers in the feature `LOAN`.\n",
    "4. Create a new data frame that has no outliers in the feature `LOAN`. To that end: \n",
    "- Reuse your solution to task 3 to identify outliers using indexing\n",
    "- Change the `LOAN` values for all outlier cases to the threshold you computed in step 2.\n",
    "5. Write a custom function that implements the functionality you created in task 4. Make the feature to work on an argument of your function.\n",
    "6. Call your custom function for all numerical features in the data frame. The goal is to create a data frame that does not have any upper outlier in any of its numerical features. To demonstrate the capabilities of your function, set the threshold to $3 \\cdot IQR(X)$. This way, only extreme outliers will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First and third quantile of the LOAN feature\n",
    "quantiles = df[\"LOAN\"].quantile(q=[0.25, 0.75])\n",
    "\n",
    "# To extract the actual numbers into easy-to-use variables,\n",
    "# we can first create a tuple and then use unpacking\n",
    "q1, q3 = (quantiles.values)\n",
    "print(f\"The first and third quartile are, respectively {q1} and {q3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Threshold value for upper outlier\n",
    "tau = q3 + 1.5*(q3-q1)\n",
    "tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Find upper outliers in the LOAN feature\n",
    "ix_upper_outlier = df[\"LOAN\"]>tau \n",
    "df.loc[ix_upper_outlier, \"LOAN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Remove upper outliers in feature LOAN\n",
    "df.loc[ix_upper_outlier, \"LOAN\"] = tau  # outlier truncation\n",
    "df.loc[ix_upper_outlier, \"LOAN\"]  # print results to see the effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Customer function for outlier detection and removal \n",
    "def outlier_truncation(x, factor=1.5):\n",
    "    \"\"\"\n",
    "    Identifies outlier values based on the inter-quartile range IQR. \n",
    "    Corresponding outliers are truncated and set to a contant value equal to the IQR\n",
    "    times a factor, which, following Tuckey's rule, we set to 1.5 by default\n",
    "    \n",
    "        Parameters:\n",
    "            x (Pandas Series): A data frame column to scan for outliers\n",
    "            factor (float): An outlier is a value this many times the IQR above q3/below q1\n",
    "            \n",
    "        Returns:\n",
    "            Adjusted variable in which outliers are truncated\n",
    "    \"\"\"\n",
    "    x_new = x.copy()\n",
    "    \n",
    "    # Calculate IQR\n",
    "    IQR = x.quantile(0.75) - x.quantile(0.25) \n",
    "    \n",
    "    # Define upper/lower bound\n",
    "    upper = x.quantile(0.75) + factor*IQR\n",
    "    lower = x.quantile(0.25) - factor*IQR\n",
    "    \n",
    "    # Truncation\n",
    "    x_new[x < lower] = lower.astype(np.float32)  # downcasting to float32 is needed to ensure\n",
    "    x_new[x > upper] = upper.astype(np.float32)  # compatibility with how we store the data in our data frame \n",
    "    \n",
    "    return x_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Application of the function to all numerical features in the data \n",
    "\n",
    "# Select numeric variables for outlier treatment. \n",
    "ix_numerical = df.select_dtypes(include=\"float32\").columns\n",
    "\n",
    "# Process every selected column using apply\n",
    "# Updated 10.06.20 to show passing arguments to the 'applied' functions. Just send a tuple with arguments in the order as specified\n",
    "# by the called function leaving out the first argument (see, https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.apply.html)\n",
    "df[ix_numerical] = df[ix_numerical].apply(outlier_truncation, axis=0, factor=3)  \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scaling numerical features\n",
    "Another common data preparation task is scaling numerical features. The goal is to ensure that all features have the same scale. This is important for many machine learning algorithms. The lecture introduced two common scaling methods: min-max scaling and z-score scaling.\n",
    "The `sklearn` library provides implementations of both approaches in the classes `MinMaxScaler` and `StandardScaler`, which are part of the module `preprocessing`. Expericence their functionality to solving the following exercises.\n",
    "\n",
    "1. Import the class `MinMaxScaler` and `StandardScaler` from the module `preprocessing` in the library `sklearn`.\n",
    "2. Familiarize yourself with the functioning of the `StandardScaler` using its documentation and other sources (e.g., web search). \n",
    "3. Test the `StandardScaler` by applying it to the numerical features `LOAN`. Afterwards, the scaled feature should have a mean of 0 and a standard deviation of 1. Write a few lines of code to verify this.\n",
    "4. The use of the `MinMaxScaler` is similar to the `StandardScaler`. Apply the `MinMaxScaler` to all other numerical features in the data set. More specifically, \n",
    "- Create a new data frame that contains only the numerical features.\n",
    "- Remove the feature `LOAN` from that data frame; as we already scaled it in task 3.\n",
    "- Apply the `MinMaxScaler` to the new data frame.\n",
    "- Write a few lines of code to verify that the scaling was successful. To that end, recall what the 'MinMaxScaler' does.\n",
    "- Combine the scaled features with the feature `LOAN` and the categorical features in a new `DataFrame`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import the class MinMaxScaler and StandardScaler from the module preprocessing in the library sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# 2. Familiarize yourself with the functioning of the StandardScaler using its documentation and other sources (e.g., web search).\n",
    "# For example, you could start here: https://scikit-learn.org/1.5/api/sklearn.preprocessing.html\n",
    "\n",
    "# 3. Test the StandardScaler by applying it to the numerical feature LOAN. Afterwards, the scaled feature should have a mean of 0 and a standard deviation of 1. Write a few lines of code to verify this.\n",
    "scaler = StandardScaler()\n",
    "df[\"LOAN_scaled\"] = scaler.fit_transform(df[[\"LOAN\"]])\n",
    "\n",
    "# Verify the scaling\n",
    "print(f\"Mean of LOAN_scaled: {df['LOAN_scaled'].mean()}\")\n",
    "print(f\"Standard deviation of LOAN_scaled: {df['LOAN_scaled'].std()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Apply the MinMaxScaler to all other numerical features in the data set.\n",
    "# Create a new data frame that contains only the numerical features.\n",
    "df_numerical = df.select_dtypes(include=\"float32\").copy()\n",
    "\n",
    "# Remove the feature LOAN from that data frame; as we already scaled it in task 3.\n",
    "df_numerical.drop(columns=[\"LOAN\"], inplace=True)\n",
    "\n",
    "# Apply the MinMaxScaler to the new data frame.\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df_numerical_scaled = pd.DataFrame(min_max_scaler.fit_transform(df_numerical), columns=df_numerical.columns)\n",
    "\n",
    "# Verify the scaling\n",
    "print(f\"Min values of scaled features:\\n{df_numerical_scaled.min()}\")\n",
    "print(f\"Max values of scaled features:\\n{df_numerical_scaled.max()}\")\n",
    "\n",
    "# Combine the scaled features with the feature LOAN and the categorical features in a new DataFrame.\n",
    "df_scaled = pd.concat([df[[\"LOAN_scaled\"]], df_numerical_scaled, df.select_dtypes(include=[\"category\", \"bool\"])], axis=1)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discretizing numerical features\n",
    "Discretizing numerical features is another common data preparation task. The goal is to convert continuous numerical features into discrete bins or categories. This can be useful for certain types of analysis and modeling. The `pandas` library provides the `cut` and `qcut` functions for this purpose.\n",
    "\n",
    "1. Familiarize yourself with the `cut` and `qcut` functions in the `pandas` library using their documentation and other sources (e.g., web search).\n",
    "2. Use the `cut` function to discretize the `LOAN` feature into 5 equal-width bins. Assign meaningful labels to each bin (e.g., 'Very Low', 'Low', 'Medium', 'High', 'Very High').\n",
    "3. Verify the binning by displaying the first few rows of the data frame and checking the `LOAN` feature.\n",
    "4. Use the `qcut` function to discretize the `MORTDUE` feature into 4 quantile-based bins. Assign meaningful labels to each bin (e.g., 'Q1', 'Q2', 'Q3', 'Q4').\n",
    "5. Verify the binning by displaying the first few rows of the data frame and checking the `MORTDUE` feature.\n",
    "6. Create a new data frame that includes the discretized `LOAN` and `MORTDUE` features along with the other original features.\n",
    "7. Write a custom function that takes a data frame and a list of numerical features as input and returns a new data frame with all specified features discretized into a given number of bins using the `cut` function. Test your function on the numerical features in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Use the cut function to discretize the LOAN feature into 5 equal-width bins. Assign meaningful labels to each bin (e.g., 'Very Low', 'Low', 'Medium', 'High', 'Very High').\n",
    "loan_bins = pd.cut(df[\"LOAN\"], bins=5, labels=[\"Very Low\", \"Low\", \"Medium\", \"High\", \"Very High\"])\n",
    "df[\"LOAN_bins\"] = loan_bins  # We add a new column as opposed to overwriting the existing column\n",
    "\n",
    "# 3. Verify the binning by displaying the first few rows of the data frame and checking the LOAN feature.\n",
    "print(df[[\"LOAN\", \"LOAN_bins\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Use the qcut function to discretize the MORTDUE feature into 4 quantile-based bins. Assign meaningful labels to each bin (e.g., \"Q1\", \"Q2\", \"Q3\", \"Q4\").\n",
    "mortdue_bins = pd.qcut(df[\"MORTDUE\"], q=4, labels=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n",
    "\n",
    "# 5. Verify the binning by displaying the first few rows of the data frame and checking the MORTDUE feature.\n",
    "df[\"MORTDUE_bins\"] = mortdue_bins\n",
    "print(df[[\"MORTDUE\", \"MORTDUE_bins\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create a new data frame that includes the discretized LOAN and MORTDUE features along with the other original features.\n",
    "df_discretized = df.drop(columns=[\"LOAN\", \"MORTDUE\"])\n",
    "df_discretized = pd.concat([df_discretized, loan_bins, mortdue_bins], axis=1)\n",
    "df_discretized  # preview the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. Write a custom function that takes a data frame and a list of numerical features as input and returns a new data frame with all specified features discretized into a given number of bins using the cut function. Test your function on the numerical features in the data frame.\n",
    "def discretize_features(df, features, bins=5, labels=None):\n",
    "    \"\"\"\n",
    "    Discretizes the specified columns of a DataFrame into equal-width bins.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame containing the data to be discretized.\n",
    "    columns (list of str): The list of column names to be discretized.\n",
    "    bins (int): The number of equal-width bins to use for discretization.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A new DataFrame with the specified columns discretized into bins.\n",
    "\n",
    "    \"\"\"\n",
    "    df_discretized = df.copy()\n",
    "    for feature in features:\n",
    "        df_discretized[feature + \"_bins\"] = pd.cut(df_discretized[feature], bins=bins, labels=labels)\n",
    "    return df_discretized\n",
    "\n",
    "# Test the function on the numerical features in the data frame\n",
    "ix_numerical = df.select_dtypes(include=\"float32\").columns\n",
    "df_discretized_all = discretize_features(df, ix_numerical, bins=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "df_discretized_all.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "4_nb_data_preparation (2).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "bads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
