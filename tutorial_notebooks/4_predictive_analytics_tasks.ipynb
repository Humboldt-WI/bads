{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ka20bzp6PcNj"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorial_notebooks/4_predictive_analytics_tasks.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swSaDjWvNDnY"
   },
   "source": [
    "# Tutorial 4 - Foundations of predictive analytics\n",
    "The lecture has introduced the standard data structure for predictive analytics. We need data with input variables and a **target variable**. The goal of predictive analytics is to derive a functional relationship between the input variables and the target. We assume that we can observe, measure, or control the input variables. Hence, our predictive model (the functional relationship between inputs and the target that we infer from past data), facilitates forecasting the expected value of the target variable based on the input variables. Whenever we observe a new case, we gather the values of the input variables for that case and feed them into our prediction model. Receiving that input, the model will produce a forecast of the target variable value for that case. So, predictive analytics is all about finding *good* **input-to-output mappings**.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/sl_xy_map.png\" width=\"300\" height=\"200\" alt=\"Supervised Learning Principle\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " You can think of linear regression. Formally speaking, a linear regression function maps inputs $\\boldsymbol x = {x_1, x_2, ..., x_d}$ from the domain $X \\in  \\mathbb{R}^d$ to the outputs $y \\in  \\mathbb{R}$. \n",
    "\n",
    "\n",
    "\n",
    "Recall from the lecture that many alternative terms are in use to refer to the input variables. Covariates, (independent) variables, attributes are only a few examples. In the interest of having consistent terminology, we will use the term **features** instead of input variables in the following. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKHjoRMQy-9K"
   },
   "source": [
    "## Our first predictive model: linear regression\n",
    "Linear regression assumes a linear additive relationship between features and the target. Specifically, we assume a model:\n",
    "$$ y = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + ... + \\beta_m x_m + \\epsilon $$\n",
    "where $y$ is the target variable, $\\beta$ denotes the regression coefficients (as usual), $x_j, j=1, ..., d$ are our features, and $\\epsilon$ denotes the error term. Adopting the above perspective, when using linear regression, we assume we *know* the true functional form of the input-to-output mapping. Specifically, we assume this mapping to be linear and additive. Under this assumption, our task is to find the unknown parameters that characterize our mapping function, and these are the regression coefficients $\\beta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKHjoRMQy-9K"
   },
   "source": [
    "### Data generation\n",
    "\n",
    "To warm-up, we create synthetic data for regression modeling. To keep things simple, we consider a univariate setting with only one feature. The classic example in business is that of a price response function, so we can assume that our single feature corresponds to the sales price of some product and our target to the sales volume of that product at a given price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "f9aYJsZRO65y"
   },
   "outputs": [],
   "source": [
    "# load relevant libraries\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "random.seed(888)  # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1 (peer programming): Synthetic data for regression\n",
    "\n",
    "##### a) Generate feature values\n",
    "Create a table (i.e. matrix) of normally distributed random numbers. This table will serve as our synthetic feature matrix $X$.\n",
    "- Declare variables to control the number of data points and the number of features. \n",
    "- Use the `Numpy` function `random.normal()` to create a normally distributed random numbers with suitable dimensionality.\n",
    "- Store the resulting random number matrix in a variable X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Generate dependent variable (aka target)\n",
    "Create a dependent variable $y$. To achieve this, recall the regression equation shown above. Given you already created $X$, you need regression coefficients $\\beta$ and residuals $\\epsilon$. Since we work with synthetic data, we can simply set $\\beta$ to some arbitrary values or sample *true* coefficient values randomly. As to the residuals, we must generate these as random numbers. Lastly, we must ensure that you *true* coefficients and random numbers are of the right dimensionality. \n",
    "- Create a variable `beta` as an array of random numbers of *suitable size*\n",
    "- Create a variable `epsilon` as an array of random numbers of *suitable size*\n",
    "- Create a variable `y` and compute its value by evaluating the regression equation $y= \\beta \\times X + \\epsilon$. Note that $\\times$ refers to scalar multiplication of feature matrix $X$ and coefficient vector $\\beta$. You can use the function `dot()` from `Numpy` to compute the dot product. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSsNJpQ8fE26"
   },
   "source": [
    "##### c) Putting everything together\n",
    "Create a matrix of scatter plots using the `Matplotlib` function `subplots`. \n",
    "- Study the documentation to understand how the function works and what inputs it requires\n",
    "- For each feature, label the x-axis of the scatter plot so as to also display the *true* coefficient of the corresponding feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "tv86W5nHQy3G",
    "outputId": "031ff86e-d9b6-4c32-ff00-c7dac59f1a26"
   },
   "outputs": [],
   "source": [
    "# Exercise 1c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IpEyVXay-9Q"
   },
   "source": [
    "### Linear regression\n",
    "The lecture elaborated on linear regression including its internal functioning. Recall our visual summary of the method: <br>\n",
    "<img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/linreg/summary.PNG\" width=\"640\" height=\"360\" alt=\"Linear Regression as Supervised Learning Algorithm\">\n",
    "<br>\n",
    "In the following, we revisit the two key steps of estimating the regression model and using it to compute forecasts\n",
    "\n",
    "#### OLS estimate of regression coefficients\n",
    "The lecture briefly mentioned that for linear regression, it is straightforward to *estimate* a model because we can compute the minimum of the least-square loss function analytically. The equation was $$ \\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top} y $$\n",
    "\n",
    "##### Exercise 2: Compute the OLS estimate by hand\n",
    "To implement the above equation, you can make use of the following `Numpy` functions:\n",
    "- `.transpose()` to compute the transpose of a matrix\n",
    "- `.dot()` to compute the dot product \n",
    "- `.linalg.inv()` to compute the inverse of a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eY0FIDkJy-9S",
    "outputId": "14c08c92-b275-4135-c503-cfd9fb95d465"
   },
   "outputs": [],
   "source": [
    "# Exercise 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While calculating the normal equation *by hand* is useful for education, we would never do this in practice. Instead, we would use a suitable library to fit (aka) train a linear regression model. Of course, this is our next exercise. \n",
    "\n",
    "##### Exercise 3: Linear regression using sklearn\n",
    "Python features at least two commonly used libraries to estimate linear regression models, `statsmodels` and `sklearn`. The former is suitable for *explanatory modeling*. For prediction, `sklearn` is the better choice. Here, we focus on `sklearn`. To use it for estimating a linear regression model using our synthetic data, we need to implement the following steps:\n",
    "- Import the class `LinearRegression` from the namespace `sklearn.linear_model`.\n",
    "- Apply the method `fit` to our data, which we store in the variables `X` and `y`\n",
    "\n",
    "To then check that the values of the estimated coefficients are the same as those we computed above, you can access the estimated coefficients `lr_model.coef_`, where we assume that the fitted model is stored in a variable with name `lr_model`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forecasting\n",
    "To complete our first linear regression demo and the part on working with synthetic data, let us also illustrate the second core step in supervised machine learning, the **calculation of forecasts**. We follow the previous approach of first doing the calculations *by hand* and then using a library to do it, which is representative of how we would proceed in practice.  \n",
    "\n",
    "##### Exercise 4: Calculation of forecasts\n",
    "Here is the set of tasks to illustrate the calculation of forecasts by hand and using `Numpy`.\n",
    "- Reusing codes from Exercise 1, create some additional synthetic data\n",
    "    - Call the variables storing your new data `X_test` and `y_test`\n",
    "    - For `X_test`, you need to create new random feature values\n",
    "    - For `y_test`, you need to create new residuals, whereas you re-use the *true* coefficients\n",
    "- To calculate forecasts *by hand*, use your vector of OLS coefficients `beta_hat` and apply it to your new synthetic data `X_test`. Recall that the calculation of regression model outputs involves the dot product $\\hat{\\beta} \\times X$. We saw ways to do this calculation using `Numpy`.\n",
    "- To calculate forecasts using `sklearn`, which is a lot easier, you only need to call the method `predict` to your trained regression model `lr_model`, which you created in exercise 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_ngsAb7fu__"
   },
   "source": [
    "##### Exercise 5: Calculation of forecast accuracy\n",
    "The lecture sketched a few common performance metrics to assess linear regression models including the mean squared error (MSE). Recall that MSE is defined as:  <br><br>\n",
    "$ MSE = \\frac{1}{n}\\sum_{i=1}^{n} \\left( Y_i - \\hat{Y}_i \\right)^2 $,\n",
    "<br><br>\n",
    "with:\n",
    "- $n$ = number of data points\n",
    "- $Y$ = true values of the target variable\n",
    "- $\\hat{Y}$ = forecasts of the regression model\n",
    "\n",
    "Provided you solved exercise 4, you have already calculated predictions. Calculate, for a last time *by hand*, the MSE of your regression model. Afterwards, run a web-search to find a function that does the calculation for you, and re-implement the code to calculate predictions using that function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qTxqBKaJjC-x",
    "outputId": "14eb895c-0d93-4552-dde4-567cb84b692b"
   },
   "outputs": [],
   "source": [
    "# Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "That's the end of today, another demo notebook completed. *Well Done!*\n",
    "\n",
    "We not actually spend so much time on prediction but concentrated on basic methods like linear regression, which can be used for prediction. And, importantly, we have spent a lot of time on the data that we need for prediction. Data with features and a target variable. Having experienced how such data really looks and how you can create it yourself will help you a lot on your data science journey. \n",
    "\n",
    "Next up, we continue with elaborating on data handling and readying data for modeling."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "3_nb_predictive_analytics_lessmann.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "bads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
