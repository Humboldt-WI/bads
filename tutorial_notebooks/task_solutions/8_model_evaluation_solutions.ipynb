{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97f60c7",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorial_notebooks/8_model_evaluation_solutions.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd632d8e",
   "metadata": {},
   "source": [
    "# Tutorial 8 - Machine Learning Model Evaluation\n",
    "\n",
    "<span style=\"font-weight: bold; color: red;\">This version includes solutions to the exercises. </span>\n",
    "\n",
    "In this tutorial, we revisit the evaluation of machine learning, and more specifically classification models. We will cover the following topics:\n",
    "- Data organization\n",
    "    - Train-test split\n",
    "    - Cross-validation\n",
    "- Measures of classification performance\n",
    "    - Confusion matrix and associated indicators\n",
    "    - ROC Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f867e0",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "### Standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85d210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91904e69",
   "metadata": {},
   "source": [
    "### The HMEQ data set\n",
    "We continue using the \"Home Equity\" data set (HMEQ), which we can nicely load and prepare using the helper function `get_HMEQ_credit_data`, which is available in our courses module `bads_helper_functions.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c383882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bads_helper_functions as bads  # import module with bads helper functions\n",
    "X, y = bads.get_HMEQ_credit_data()  # load the data \n",
    "X  # preview the data   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb489532",
   "metadata": {},
   "source": [
    "# Data organization I: Holdout method\n",
    "The idea of the *holdout method* is to evaluate a model on data it hasn't seen during training, providing a better estimate of the model's real-world performance. To that end, we randomly split the data set into two parts: a training set and a test set. The training set is used to train the model, while the test set is used for model assessment.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/holdout_method.png\" width=\"854\" height=\"480\" alt=\"Holdout method\">\n",
    "\n",
    "## Exercise 1: Train-test split\n",
    "Implement the *holdout method* using the `train_test_split` function from the module `sklearn.model_selection`. More specifically, your tasks are as follows:\n",
    "\n",
    "- Import the function `train_test_split` from `sklearn.model_selection`.\n",
    "- Split the data set into a training set and a test set. Use 75% of the data for training and 25% for testing.\n",
    "- Store the resulting training and test sets in the variables `X_train`, `X_test`, `y_train`, and `y_test`.\n",
    "- Print the dimensions of the training and test sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d3cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# Solution to exercise I \n",
    "#------------------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=888)  # partition the data into 75% training and 25% test\n",
    "\n",
    "print(f\"Training set dimension (observations x features): {X_train.shape[0]} x {X_train.shape[1]}\")   \n",
    "print(f\"Test set dimension (observations x features): {X_test.shape[0]} x {X_test.shape[1]}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b504809",
   "metadata": {},
   "source": [
    "# Model training\n",
    "Up to this point, the lecture introduced you to two algorithms for classification: logistic regression and decision trees. We will use both algorithms to train models on the training set. To that end, we re-use codes from previous tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f78f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "logreg = LogisticRegression(random_state=888)  # instantiate the logistic regression model\n",
    "logreg.fit(X_train, y_train)  # fit the regression model to the training data\n",
    "\n",
    "# DECISION TREE\n",
    "dtree = DecisionTreeClassifier(random_state=888, max_depth=5)  # instantiate the decision tree model\n",
    "dtree.fit(X_train, y_train)  # grow the tree using the training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe48a94",
   "metadata": {},
   "source": [
    "# Measures of classifier performance\n",
    "The `sklearn` library provides several functions to evaluate the performance of a classifier. In general, functionality for model evaluation is available in the module `sklearn.metrics`. The lecture introduced two common approaches for classifier assessment, the confusion matrix and the ROC curve. We will consider both methods to evaluate the performance of the models trained in the previous step.\n",
    "\n",
    "## ROC Curve\n",
    "The main difference between ROC analysis and the confusion matrix, as well as accuracy indicators derived from the confusion matrix is that ROC analysis considers all possible thresholds for classifying observations. Specifically, the ROC curve is a graphical representation of the trade-off between the true positive rate (TPR) and the false positive rate (FPR) for different classification thresholds. The area under the ROC curve (AUC) is a single number summary of a classifier's ROC curve and a widely used indicator when comparing alternative classifiers.  \n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/ROC-curve.png\" width=\"854\" height=\"480\" alt=\"Confusion Matrix\">\n",
    "\n",
    "\n",
    "### Exercise 2\n",
    "Your task is to compare the performance of the logistic regression (i.e., `logreg`) versus the classification tree (i.e., `dtree`) using ROC analysis. Specifically:\n",
    "- Import the function `roc_curve` and `roc_auc_score` from `sklearn.metrics`.\n",
    "- For each model, \n",
    "    - produce probabilistic predictions on the test set.\n",
    "    - compute the AUC using the function `roc_auc_score` and print the result.\n",
    "    - create an ROC curve using the method `from_predictions` of the class `RocCurveDisplay`.\n",
    "\n",
    "> Following the above steps will lead to creating two ROC curves, one for each classifier. As an **extra challenge**, try to plot both ROC curves in the same plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c35783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# Solution to exercise 2\n",
    "#------------------------------------------------------------\n",
    "from sklearn.metrics import roc_auc_score, RocCurveDisplay\n",
    "\n",
    "my_models = [logreg, dtree]  # list of models to evaluate\n",
    "\n",
    "fig, ax = plt.subplots()  # To plot the ROC curves of multiple models in the same plot, we need to create upfront\n",
    "\n",
    "# Loop over the models\n",
    "for model in my_models: \n",
    "  # compute probability predictions on the test set\n",
    "  yhat = model.predict_proba(X_test)[:,1]  # we are only interested in the probability of class 1\n",
    "\n",
    "  # compute the AUC\n",
    "  auc = roc_auc_score(y_test, yhat)\n",
    "  print(f\"The AUC of {type(model).__name__} is: {auc:.4f}\")\n",
    "  \n",
    "  # plot the ROC curve\n",
    "  RocCurveDisplay.from_predictions(y_test, yhat, ax=ax)\n",
    "  \n",
    "\n",
    "# Add diagonal line corresponding to a random classifier\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='red')\n",
    "plt.show()  # display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0030d25",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "The confusion matrix of a binary classifier is a 2x2 matrix that contains four values: true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). The following image illustrates the confusion matrix and some common performance measures derived from it. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/confusion_matrix.PNG\" width=\"854\" height=\"480\" alt=\"Confusion Matrix\">\n",
    "\n",
    "\n",
    "Producing a confusion matrix follows the same steps as ROC analysis. Given that the previous analysis suggests the decision tree is superior to logistic regression, we will focus on the decision tree model and first demonstrate how to create a confusion matrix using the default classification threshold $\\tau=0.5$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66352847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Create confusion matrix: note how the method from_estimator \n",
    "# bypasses the calculation of predictions. Instead of calling predict(), \n",
    "# we can directly give the trained model as argument\n",
    "ConfusionMatrixDisplay.from_estimator(dtree, X_test, y_test)  \n",
    "plt.title(f\"Confusion matrix of {type(model).__name__}\")\n",
    "plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782564f3",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "The lecture made a strong case for *not using the default threshold of $\\tau=0.5$*. We will illustrate a better approach in the tutorial. For now, your task is to consider a custom threshold of $\\tau=0.8$. Recreate the confusion matrix using this threshold. To achieve this:\n",
    "- Compute probabilistic predictions for the test set.\n",
    "- Convert the probabilistic predictions into binary predictions using the threshold $\\tau=0.8$.\n",
    "- Create a confusion matrix using `ConfusionMatrixDisplay.from_predictions()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c8f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# Solution to exercise 3\n",
    "#------------------------------------------------------------\n",
    "\n",
    "# Step 1: Compute probability predictions for the test set\n",
    "yhat_proba = dtree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Step 2: Apply custom threshold to convert probabilities to binary predictions\n",
    "custom_threshold = 0.8\n",
    "yhat_custom = (yhat_proba >= custom_threshold).astype(int)\n",
    "\n",
    "# Step 3: Display the confusion matrix using the custom threshold predictions\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, yhat_custom)\n",
    "plt.title(f\"Confusion matrix with custom threshold {custom_threshold}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa17bacc",
   "metadata": {},
   "source": [
    "### Optimal threshold\n",
    "The question of what is an optimal threshold is debatable. For the sake of illustration, let's say we seek a threshold such that the difference between the TPR and the FPR is maximal. This point is also known as [*Youdon's J*](https://en.wikipedia.org/wiki/Youden%27s_J_statistic). While it is often referred to as the *optimal threshold*, we reiterate that this view is debatable. In a business context, we would rather think of an optimal threshold as a threshold that minimizes the costs arising from wrong classifications and, by extension, decisions. \n",
    "\n",
    "The following codes illustrate how to first compute an optimal threshold and then create the confusion matrix for that threshold. In this course, we introduce the function `roc_curve`, which provides yet another way to create an ROC curve. Importantly, this function does not perform any plotting but returns the values underlying a ROC curve, namely, the FPR, TPR, and the thresholds used to compute the ROC curve. We can use the latter to compute the optimal threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e65224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Step 1: Compute probability predictions for the test set\n",
    "yhat = dtree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Step 2: Compute the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, yhat)  # the function \n",
    "\n",
    "# Step 3: Find the optimal threshold\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(f\"Optimal threshold: {optimal_threshold:.4f}\")\n",
    "\n",
    "# Step 4: Apply the optimal threshold to convert probabilities to binary predictions\n",
    "yhat_optimal = (yhat >= optimal_threshold).astype(int)\n",
    "\n",
    "# Step 5: Display the confusion matrix using the optimal threshold predictions\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, yhat_optimal)\n",
    "plt.title(f\"Confusion matrix with optimal threshold {optimal_threshold:.2f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68cbe20",
   "metadata": {},
   "source": [
    "### Classification report\n",
    "While inspecting a confusion matrix is useful, it is often more informative to consider the specific performance indicartors derived from it. The `skelarn` library provides a classification report, which includes the classifiers precision, recall, and F1-score, defined as the harmonic mean of precision and recall: \n",
    "\n",
    "$$F_1 = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall}$$ \n",
    "\n",
    "#### Exercise 4\n",
    "Locate the function the function `classification_report` from the module `sklearn.metrics` and call it to create a classification report for the logistic regression model and the decision tree, both with default threshold. Needless to say, you create the classification report based on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84a1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# Solution to exercise 4\n",
    "#------------------------------------------------------------\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Classification report for logistic regression (default threshold)\n",
    "report = classification_report(y_test, logreg.predict(X_test))   \n",
    "print(\"LOGISTIC REGRESSION\\n\")\n",
    "print(report) \n",
    "\n",
    "# Classification report for decision tree (default threshold)\n",
    "print(\"\\nDECISION TREE\\n\")\n",
    "report = classification_report(y_test, dtree.predict(X_test))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aadd4c1",
   "metadata": {},
   "source": [
    "The output of the `sklearn` classification report includes the following metrics for each class:\n",
    "\n",
    "- **Precision**: The ratio of true positive predictions to the total predicted positives.\n",
    "- **Recall**: The ratio of true positive predictions to the total actual positives.\n",
    "- **F1-score**: The harmonic mean of precision and recall.\n",
    "- **Support**: The number of actual occurrences of the class in the dataset.\n",
    "\n",
    "Additionally, it provides overall metrics such as accuracy, macro average, and weighted average for precision, recall, and F1-score, whereby the weights are calculated based on the support of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba19a82",
   "metadata": {},
   "source": [
    "# Data organization II: Cross-validation\n",
    "The holdout method is a simple and effective way to evaluate a model. However, it has some limitations. For example, the performance of a model can vary significantly depending on the data used for training and testing. To mitigate this issue, we can use cross-validation. The idea is to split the data set into $k$ folds and train the model $k$ times, each time using a different fold for testing and the remaining folds for training. The performance of the model is then averaged over the $k$ iterations.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/cross_validation.png\" width=\"854\" height=\"480\" alt=\"Cross validation process\">\n",
    "\n",
    "Unsurprisingly, the `sklearn` library provides several functions to perform cross-validation. The arguably easiest way involves using the function `cross_val_score` from the module `sklearn.model_selection`. It allows performing cross-validation and computing one performance metric for each fold. The function returns an array of scores, one for each fold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe05388",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Your task is to implement a 10-fold cross-validation of the decision tree classifier using the function `cross_val_score()`. More specifically:\n",
    "- Examine the function's documentation and make sure you understand its key arguments.\n",
    "- Write code to call the function such that it cross-validates a decision tree using the HMEQ data set \n",
    "    - Make sure to perform 10 fold cross-validation\n",
    "    - Inspect the argument `scoring`, which allows specifying a performance indicator. Set this argument such that it uses the AUC. \n",
    "- Print the average AUC of the tree and its standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c578a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# Solution to exercise 5\n",
    "#------------------------------------------------------------\n",
    "from sklearn.model_selection import cross_val_score \n",
    "\n",
    "# Cross-validation with 10 folds \n",
    "cv_scores = cross_val_score(dtree, X, y, cv=10, scoring='roc_auc')  # compute the AUC scores \n",
    "print(f\"Decision tree AUC scores: {cv_scores}\")  # display the AUC scores\n",
    "print(f\"Decision tree mean AUC (std. AUC): {cv_scores.mean():.4f}({cv_scores.std():.4f})\")  # display the mean AUC score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b42fce",
   "metadata": {},
   "source": [
    "## Cross-validation using the Class `KFold`\n",
    "\n",
    "If `cross_val_score` is the easiest way to do cross-validation, then the other end of the spectrum is the provided by the class `KFold`. This class allows for much more flexibility and control over the cross-validation process. However, this comes at the cost of having to write more code. Here is an example that illustrate how to use `KFold` to perform cross-validation. To demonstrate the flexibility of `KFold`, we will use it to compute multiple performance measures (i.e., AUC, and F1 score) for each fold and within each fold for both, the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa049d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score    \n",
    "k = 10  # number of folds\n",
    "kf = KFold(n_splits=k)  # initialize cross-validation process\n",
    "\n",
    "# Initialize the classifier\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "i = 1  # counter for the fold number\n",
    "# Iterating over the k folds\n",
    "for train_index, test_index in kf.split(X):  # KFold gives as two arrays with the indices of the training and validation data of the current iteration\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]  # we can use these arrays to index our original data set: here we construct the feature matrices\n",
    "    y_train, y_test = y[train_index], y[test_index]  # and here we construct the arrays with the true targets\n",
    "\n",
    "    # Fitting the tree to the training set of THIS ITERATION\n",
    "    tree.fit(X_train, y_train)\n",
    "\n",
    "    # Compute predictions on the test set of THIS ITERATION\n",
    "    yhat_class = tree.predict(X_test)  # discrete predictions\n",
    "    yhat_proba = tree.predict_proba(X_test)[:,1]  # probabilities of class 1\n",
    "\n",
    "    # Compute model test set performance for every error measure we are interested in\n",
    "    auc = roc_auc_score(y_test, yhat_proba)\n",
    "    f1 = f1_score(y_test, yhat_class)\n",
    "    print(f\"Fold {i}\\tTest set AUC: {auc:.4f}\")\n",
    "    print(f\"Fold {i}\\tTest set F1: {f1:.4f}\")\n",
    "    \n",
    "    # Repeat the previous steps to also compute training set performance\n",
    "    yhat_class = tree.predict(X_train)\n",
    "    yhat_proba = tree.predict_proba(X_train)[:,1]\n",
    "    auc = roc_auc_score(y_train, yhat_proba)\n",
    "    f1 = f1_score(y_train, yhat_class)\n",
    "    print(f\"Fold {i}\\tTraining set AUC: {auc:.4f}\")\n",
    "    print(f\"Fold {i}\\tTraining set F1: {f1:.4f}\")\n",
    "    i += 1  # increment the fold counter\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163e4c67",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "While perhaps hinting at the flexibility of `KFold`, the previous example shows some very bad coding practice. The calculation of test and training set performance across multiple performance metrics is repetitive. More severely, the demo does not store any results. Plotting the development of performance across folds and computing an average performance in the end is, thus, impossible. Draw on your Python skills to improve the above demo. Specifically:   \n",
    "- Identify a suitable data structure to store the performance metrics per fold for the training and test set \n",
    "- Revise the code to store the performance metrics in the data structure you identified.\n",
    "- Compute the average performance across all folds and print the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a6a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# Solution to exercise 6\n",
    "#------------------------------------------------------------\n",
    "\n",
    "# Initialize the classifier\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "# Number of folds\n",
    "k = 10\n",
    "kf = KFold(n_splits=k)\n",
    "\n",
    "# Initialize a dictionary to store performance metrics\n",
    "performance = {\n",
    "    'train_auc': [],\n",
    "    'test_auc': [],\n",
    "    'train_f1': [],\n",
    "    'test_f1': []\n",
    "}\n",
    "\n",
    "# Iterating over the k folds\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fitting the tree to the training set of THIS ITERATION\n",
    "    tree.fit(X_train, y_train)\n",
    "\n",
    "    # Compute predictions on the test set of THIS ITERATION\n",
    "    yhat_class_test = tree.predict(X_test)\n",
    "    yhat_proba_test = tree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Compute predictions on the training set of THIS ITERATION\n",
    "    yhat_class_train = tree.predict(X_train)\n",
    "    yhat_proba_train = tree.predict_proba(X_train)[:, 1]\n",
    "\n",
    "    # Compute model test set performance for every error measure we are interested in\n",
    "    test_auc = roc_auc_score(y_test, yhat_proba_test)\n",
    "    test_f1 = f1_score(y_test, yhat_class_test)\n",
    "    \n",
    "    # Compute model training set performance for every error measure we are interested in\n",
    "    train_auc = roc_auc_score(y_train, yhat_proba_train)\n",
    "    train_f1 = f1_score(y_train, yhat_class_train)\n",
    "\n",
    "    # Store the performance metrics\n",
    "    performance['train_auc'].append(train_auc)\n",
    "    performance['test_auc'].append(test_auc)\n",
    "    performance['train_f1'].append(train_f1)\n",
    "    performance['test_f1'].append(test_f1)\n",
    "\n",
    "# Convert the performance dictionary to a DataFrame\n",
    "performance_df = pd.DataFrame(performance)\n",
    "\n",
    "# Compute the average performance across all folds\n",
    "average_performance = performance_df.mean()\n",
    "print(\"Average performance across all folds:\")\n",
    "print(average_performance)\n",
    "\n",
    "# Display the performance DataFrame\n",
    "print(\"\\nPerformance metrics for each fold:\")\n",
    "performance_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "B0rxPs4QEGtz",
    "27sCENzmoGcX"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "bads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
