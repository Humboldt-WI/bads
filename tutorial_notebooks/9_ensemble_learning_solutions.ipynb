{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97f60c7",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorial_notebooks/9_ensemble_learning_solutions.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd632d8e",
   "metadata": {},
   "source": [
    "# Tutorial 9 - Ensemble Learning \n",
    "\n",
    "<span style=\"font-weight: bold; color: red;\">This version includes solutions to the exercises. </span>\n",
    "\n",
    "In this tutorial, we revisit our lecture on ensemble learning, which has explained new algorithms, namely Random Forest (RF) and Gradient Boosting (GBM), and, more generally, introduced us to the space of *ensemble learners*, which generate a composite forecast by integrating the predictions of multiple *base models*. We will cover the following topics:\n",
    "\n",
    "#### Contents\n",
    "\n",
    "1. [Foundations of Ensemble Modeling and Forecast Combination](#foundations)  \n",
    "2. [Bagging and Random Forest](#bagging-rf)  \n",
    "3. [Gradient Boosting](#gradient-boosting)  \n",
    "4. [Hyperparameter Tuning for Ensemble Models](#modelsel)\n",
    "\n",
    "Within each section, you will find:\n",
    "- A **Concept Recap** highlighting the key ideas from the lecture.\n",
    "- A **Programming Demo** illustrating these ideas in Python.\n",
    "- **Exercises** to deepen your understanding and practice these methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f867e0",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "### Standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c85d210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91904e69",
   "metadata": {},
   "source": [
    "### The HMEQ data set\n",
    "We continue using the \"Home Equity\" data set (HMEQ), which we can nicely load and prepare using the helper function `get_HMEQ_credit_data`, which is available in our courses module `bads_helper_functions.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c383882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bads_helper_functions as bads  # import module with bads helper functions\n",
    "X, y = bads.get_HMEQ_credit_data()  # load the data \n",
    "\n",
    "print(\"Data loaded. Shape of X: \", X.shape, \"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2179a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X  # preview the data   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb489532",
   "metadata": {},
   "source": [
    "<a id=\"foundations\"></a>\n",
    "# 1. Foundations of Ensemble Modeling and Forecast Combination\n",
    "\n",
    "## 1.1 Concept Recap\n",
    "In ensemble modeling, multiple models (referred to as “base learners” or “weak learners”) are combined to produce a single predictive model. The motivation behind ensemble modeling is that by combining various learners, we often get better generalization performance than using a single learner. Ensemble models help to:\n",
    "\n",
    "- Reduce variance (stabilize predictions).\n",
    "- Potentially reduce bias (when certain conditions are met).\n",
    "- Provide more robust and reliable predictions across different problem domains.\n",
    "\n",
    "Techniques include:\n",
    "\n",
    "- Averaging Methods: e.g., Bagging (Bootstrap Aggregating)\n",
    "- Boosting Methods: e.g., Gradient Boosting\n",
    "- Heterogeneous approaches, which integrate different learning algorithms to farm the base models. Examples include the Stacking algorithm, which combines base models using a meta-model (see, e.g., [this tutorial for a demo](https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/) if interested). The lecture did not elaborate on heterogeneous ensemble learners; neither will this tutorial. We mainly mention them for the sake of completeness. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73cc8ad",
   "metadata": {},
   "source": [
    "## 1.2 Programming Demo: Simple Averaging Ensemble\n",
    "Below is a simple demonstration of how one might combine three different models (a Logistic Regression and two Decision Trees) by averaging their probability estimates. This is actually a heterogeneous ensemble, as we combine different types of models. The example is meant to convey the generality of ensemble modeling, which the lecture illustrated as follows: <br>\n",
    "<br>\n",
    "<img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/ensemble_learning.png\" width=\"854\" height=\"480\" alt=\"Ensemble Learning\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d3cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=888)  # partition the data into 75% training and 25% test\n",
    "\n",
    "# Define base models\n",
    "base_learners = [\n",
    "    LogisticRegression(max_iter=1000, random_state=888),\n",
    "    DecisionTreeClassifier(max_depth=3),\n",
    "    DecisionTreeClassifier(max_depth=10)\n",
    "]\n",
    "\n",
    "# Train the models\n",
    "for m in base_learners:\n",
    "    m.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "probs = []\n",
    "for m in base_learners:\n",
    "    yhat = m.predict_proba(X_test)[:, 1]\n",
    "    probs.append(yhat)\n",
    "\n",
    "# Simple average forecast \n",
    "ensemble_prob = np.mean(probs, axis=0)\n",
    "\n",
    "# Performance evaluation\n",
    "print('Performance evaluation in terms of AUC:')\n",
    "print('-' * 50)\n",
    "print(f'Ensemble \\t {roc_auc_score(y_true=y_test, y_score=ensemble_prob):.3f}')\n",
    "print('-' * 50)\n",
    "for i, p in enumerate(probs):\n",
    "    print(f'Base model {i} \\t {roc_auc_score(y_true=y_test, y_score=p):.3f}')\n",
    "print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b504809",
   "metadata": {},
   "source": [
    "<a id=\"bagging-rf\"></a>\n",
    "\n",
    "# 2. Bagging and Random Forest\n",
    "\n",
    "## 2.1 Concept Recap\n",
    "Bagging (Bootstrap Aggregating):\n",
    "\n",
    "In bagging, we train multiple instances of the same model class on different bootstrap samples (randomly sampled with replacement) of the original training set.\n",
    "The final prediction is usually made by majority voting (classification) or averaging (regression).\n",
    "\n",
    "Random Forest:\n",
    "\n",
    "Random Forest is an extension of bagging, which uses Decision Trees as base learners. Random Forest also uses feature sub-sampling at each split to decorrelate individual trees. Much empirical evidence suggests that Random Forest is a very effective and robust algorithm for a wide range of prediction tasks. It is also relatively simple to tune.\n",
    "\n",
    "## 2.2 Programming Demo: Random Forest\n",
    "Below, we train a simple Random Forest, measure its AUC and plot the ROC curve. To showcase the effectiveness of Random Forest, we add the results of the previous demo, our heterogeneous ensemble, to the ROC plot. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f78f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "# Instantiate and train RandomForest\n",
    "rf_model = RandomForestClassifier(n_estimators=100,  # the number of base model trees\n",
    "                                   max_depth=5,      # the maximum depth of each tree\n",
    "                                   random_state=888  # Random number seed. Recall that RF is stochastic due to bootstrap sampling and random subspace\n",
    "                                   )\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "rf_probs = rf_model.predict_proba(X_test)[:,1]\n",
    "print(f\"Random Forest AUC: {roc_auc_score(y_true=y_test, y_score=rf_probs):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86153c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RF performance and compare to benchmarks\n",
    "f, ax = plt.subplots(figsize=(8,6))\n",
    "RocCurveDisplay.from_estimator(estimator=rf_model, X=X_test, y=y_test, ax=ax)\n",
    "RocCurveDisplay.from_predictions(y_true=y_test, y_pred=ensemble_prob, ax=ax, name=\"Simple avg. ensemble\")\n",
    "for i, p in enumerate(probs):\n",
    "    RocCurveDisplay.from_predictions(y_true=y_test, y_pred=p, ax=ax, name=f\"Base model {i}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='red')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7d027b",
   "metadata": {},
   "source": [
    "## 2.1 Exercise 1\n",
    "Unlike RF, the Bagging algorithm is not specific to Decision Trees. It can be applied to any base learner.\n",
    "\n",
    "Here is your task:<br>\n",
    "- Train two Bagging classifiers, one using logistic regression and one using decision trees to train the base models. This allows for a comparison of which algorithm, logistic regression or decision tree, benefits more from bagging. An implementation of the Bagging algorithm is available in the `sklearn.ensemble` module. \n",
    "- For each Bagging classifier, use 50 base learners and set the random state to 888. \n",
    "  - For the logistic regression base learner, set `max_iter=1000`, and `random_state=888`\n",
    "  - For the decision tree base learner, set `max_depth=3`\n",
    "  - Note that these are the same settings as used in our first demo on the simple average ensemble. \n",
    "- Create an ROC chart to compare the models. Specifically:\n",
    "  - Plot the ROC curve for the bagged logistic regression\n",
    "  - Plot the ROC curve for the bagged decision tree\n",
    "  - Plot the ROC curve for the non-bagged logistic regression trained in the first demo. To do this, simply reuse the already stored predictions `probs[0]`\n",
    "  - Also add a ROC curve for the non-bagged decision tree trained. Again, simply reuse the available predictions `probs[1]`\n",
    "- Add a legend to the plot to distinguish the different models.\n",
    "    \n",
    "\n",
    "<details> <summary>Hint on bagging </summary> Use the following scaffolding to configure the Bagging classifier. :\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bagging_model = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=___), \n",
    "    n_estimators=___, \n",
    "    random_state=___\n",
    ")\n",
    "bagging_model.fit(X_train, y_train)\n",
    "# Evaluate your model\n",
    "```\t\n",
    "</details>\n",
    "\n",
    "<details> <summary>Hint on the ROC curve </summary> Note that the previous ROC curve example illustrates how to draw curves for the non-bagged models. Assuming you have executed previous code cells, you can reuse the stored predictions `probs[0]` and `probs[1]` to plot the ROC curves for the non-bagged models as follows:\n",
    "\n",
    "```python\n",
    "RocCurveDisplay.from_predictions(y_true=y_test, y_pred=probs[0], ax=ax, name=f\"Logistic regression\")\n",
    "```\t\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46649a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Define the Bagging classifiers\n",
    "bagging_logreg = BaggingClassifier(\n",
    "    base_estimator=LogisticRegression(max_iter=1000, random_state=888),\n",
    "    n_estimators=50,\n",
    "    random_state=888\n",
    ")\n",
    "\n",
    "bagging_dtree = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=3),\n",
    "    n_estimators=50,\n",
    "    random_state=888\n",
    ")\n",
    "\n",
    "# Train the Bagging classifiers\n",
    "bagging_logreg.fit(X_train, y_train)\n",
    "bagging_dtree.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "yhat_logreg = bagging_logreg.predict_proba(X_test)[:, 1]\n",
    "yhat_dtree = bagging_dtree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "# Plot ROC curves\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "RocCurveDisplay.from_predictions(y_true=y_test, y_pred=yhat_logreg, ax=ax, name=\"Bagged Logistic Regression\")\n",
    "RocCurveDisplay.from_predictions(y_true=y_test, y_pred=yhat_dtree,  ax=ax, name=\"Bagged Decision Tree\")\n",
    "RocCurveDisplay.from_predictions(y_true=y_test, y_pred=probs[0],    ax=ax, name=f\"Logistic regression\")\n",
    "RocCurveDisplay.from_predictions(y_true=y_test, y_pred=probs[1],    ax=ax, name=f\"Decision Tree\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c34c96",
   "metadata": {},
   "source": [
    "Examining the result of Exercise 1, what can you say about the performance of the bagged logistic regression and the bagged decision tree compared to the non-bagged models?\n",
    "\n",
    "More interestingly, what factors might explain the observed effectiveness of bagging (or lack thereof)?\n",
    "\n",
    "**Answer:**\n",
    "- The bagged version of a classifier and the non-bagged version perform almost identical. This implies that bagging was not effective in increasing predictive performance.\n",
    "- It is not surprising that bagging failed in this case.\n",
    "  - Logistic regression is a stable classifier. Fitting multiple instances on bootstrapped samples does yield must diversity. All base models will show very similar coefficients and, therefore, produce similar forecasts. Averaging over very similar forecasts does not boost performance. Thus, bagging is not expected to improve the performance of logistic regression or, more generally, a **stable classifier**.\n",
    "  - Decision trees, on the other hand, are unstable classifiers. They are sensitive to small changes in the training data. Therefore, we can legitimately expect Bagging to boost performance. The reasons it failed in this case is our configuration of the tree learning algorithm. Setting `max_depth=3`, we allow it to only produce shallow trees. This limits the degree to which bootstrap sampling can yield diverse trees substantially. Therefore, the base model trees in our ensemble are also very similar; like with logistic regression. \n",
    "\n",
    "  To verify you could conduct yet another experiment comparing the performance of a deep tree to the performance of a bagging ensemble with deep trees as base learners. Exercise 1 provides 95% of all codes needed to do this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779d17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA: Comparison of deep tree to bagged deep tree\n",
    "bagging_deep_tree = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=10),\n",
    "    n_estimators=50,\n",
    "    random_state=888\n",
    ")\n",
    "\n",
    "bagging_deep_tree.fit(X_train, y_train)\n",
    "yhat = bagging_deep_tree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Plot ROC curves. Note that we reuse the predictions from the deep tree trained\n",
    "# in the demo on the simple average ensemble\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "RocCurveDisplay.from_predictions(y_true=y_test, y_pred=yhat, ax=ax, name=\"Bagged Deep Tree\")\n",
    "RocCurveDisplay.from_predictions(y_true=y_test, y_pred=probs[2], ax=ax, name=f\"Deep Tree\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='red')\n",
    "plt.show()\n",
    "\n",
    "# Compute the relative performance boost of bagging\n",
    "auc_bag = roc_auc_score(y_true=y_test,   y_score=yhat)\n",
    "auc_nobag = roc_auc_score(y_true=y_test, y_score=probs[2])\n",
    "assert auc_bag >= auc_nobag\n",
    "perf_boost = (auc_bag-auc_nobag)/auc_nobag\n",
    "print(f'Bagging improved deep tree performance by {100*perf_boost:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b74301",
   "metadata": {},
   "source": [
    "<a id=\"gradient-boosting\"></a>\n",
    "\n",
    "# 3. Gradient Boosting\n",
    "## 3.1 Concept Recap\n",
    "Boosting builds an ensemble of weak learners in a sequential way, where each new learner attempts to correct the errors of the previous ensemble. Gradient Boosting is a general boosting framework:\n",
    "\n",
    "- Models are added sequentially.\n",
    "- Each subsequent model is trained to reduce the residual errors (or gradient of the loss) of the current ensemble.\n",
    "- Learning rate (shrinkage) controls how strongly each new model influences the ensemble.\n",
    "\n",
    "Common implementations:\n",
    "- GradientBoostingClassifier (in sklearn.ensemble)\n",
    "- XGBoost, LightGBM, CatBoost (popular specialized libraries)\n",
    "\n",
    "## 3.2 Programming Demo: Gradient Boosting\n",
    "The code snippet below demonstrates training and evaluation of a gradient boosting classifier using the `GradientBoostingClassifier` implementation from `sklearn.ensemble`. While this implementation suffices for our purposes, it is worth noting that the specialized libraries like XGBoost, LightGBM, and CatBoost are preferred in practice due to their superior performance and scalability. Thus, when working on real-world projects and with larger data sets, you should consider using one of these libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8c9ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100,\n",
    "                                      learning_rate=0.1,\n",
    "                                      max_depth=3,\n",
    "                                      random_state=888)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "RocCurveDisplay.from_estimator(estimator=gb_model, X=X_test, y=y_test, ax=ax)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='red')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33627f0",
   "metadata": {},
   "source": [
    "## 3.3 Exercise 2\n",
    "All previous demos and exercises have used default hyperparameters for the ensemble models. However, hyperparameter tuning is crucial for achieving optimal performance.\n",
    "\n",
    "Here is your task:\n",
    "Drawing on the above demo of GBM, experiment with different settings for the hyperparameters `learning_rate` and `n_estimators`. Record the AUC values of different models with different hyperparameters to study how the hyperparameters affect performance.\n",
    "\n",
    "<details> <summary>Hint</summary> \n",
    "One way to approach exercise 2 is to loop over different candidate settings for each hyperparameter and record the AUC in each round. For example, you could:<br><br>\n",
    "1. Loop over a small set of learning_rate values, e.g. `[0.01, 0.1, 0.2]`. <br>\n",
    "2. Loop over a small set of n_estimators values, e.g. `[50, 100, 200]`. <br>\n",
    "3. Measure the the AUC of the corresponding GBM model using `roc_auc_score()`. <br>\n",
    "4. Keep track of the highest AUC value obtained thus far and the corresponding hyperparameters. <br>\n",
    "  - To do this, you could use a variable `best_auc`, which you compare to the AUC obtained in the current iteration.<br>\n",
    "  - Any time you observe the current AUC to be higher than the best AUC, update the best AUC. <br>\n",
    "  - You can proceed in the same way with the hyperparameters.<br>\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a710af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 2\n",
    "# Experiment with different settings for learning_rate and n_estimators\n",
    "learning_rates = [0.01, 0.1, 0.2]\n",
    "n_estimators_list = [50, 100, 200]\n",
    "\n",
    "best_auc = 0\n",
    "best_params = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for n_estimators in n_estimators_list:\n",
    "        gb_model = GradientBoostingClassifier(n_estimators=n_estimators,\n",
    "                                              learning_rate=lr,\n",
    "                                              max_depth=3,\n",
    "                                              random_state=42)\n",
    "        gb_model.fit(X_train, y_train)\n",
    "        yhat = gb_model.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_true=y_test, y_score=yhat)\n",
    "        \n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_params = {'learning_rate': lr, 'n_estimators': n_estimators}\n",
    "        \n",
    "        print(f'Learning rate: {lr}, n_estimators: {n_estimators}, AUC: {auc:.3f}')\n",
    "\n",
    "print(f'\\nBest AUC: {best_auc:.3f} with parameters: {best_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0627c33a",
   "metadata": {},
   "source": [
    "<a id=\"modelsel\"></a>\n",
    "\n",
    "# 4. Model Selection and Hyperparameter Tuning\n",
    "Exercise 2 already touched on the task of model selection and hyperparameter tuning. This is a crucial step in the machine learning pipeline. Therefore, `sklearn` offers powerful functionality to support this step, which we will now explore. \n",
    "\n",
    "## 4.1 Concept Recap\n",
    "Machine learning models rely on various parameters to make predictions. These parameters fall into two main categories:\n",
    "\n",
    "### Model Parameters: \n",
    "These are learned directly from the data during training. Examples include the coefficients in linear regression or the split thresholds in a decision tree.\n",
    "\n",
    "### Hyperparameters:\n",
    "These are configurable settings chosen before training by the data scientist. They influence the training process and model behavior. Examples include the learning rate in gradient boosting, the number of trees in a Random Forest, or maximum tree depth in decision trees.\n",
    "\n",
    "Hyperparameter tuning (aka model selection) is the process of finding the optimal hyperparameters for a given model and data set. This is typically done using a search algorithm, such as grid search or random search. Recall our brief discussion of these methods in the lecture.\n",
    "<br>\n",
    "<img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/gridsearch.png\" width=\"854\" height=\"480\" alt=\"Grid Search\">\n",
    "\n",
    "\n",
    "## 4.2 Programming Demo: Hyperparameter Tuning\n",
    "A versatile implementation of grid search is available in the the `GridSearchCV` class in `sklearn.model_selection`. It allows us to perform an exhaustive search over a specified parameter grid. The code snippet below demonstrates a solution to Exercise 2 using `GridSearchCV` to tune the hyperparameters of a gradient boosting classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce1a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune GBM hyperparameters with GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Perform grid search: this may take a while since we assess 3*3=9 models using 5-fold cross-validation for each\n",
    "gb_model = GradientBoostingClassifier(max_depth=3, random_state=888)\n",
    "grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, cv=5, scoring='roc_auc', verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print performance statistic and configuration of the best model found during grid search\n",
    "print(f'Best AUC: {grid_search.best_score_:.3f}')\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cca850",
   "metadata": {},
   "source": [
    "**Remarks:**\n",
    "- When defining the parameter grid, you can specify multiple values for each hyperparameter. The grid search will then evaluate all possible combinations of these values. Note that the data type of the grid is a `dictionary`. It is crucial that you use the correct hyperparameter names as keys in the dictionary. These names must match the hyperparameter names of the model you are tuning.\n",
    "\n",
    "- Beyond the learning algorithm and the corresponding parameter grid, `GridSearchCV` provides several other arguments to configure the search. Above, we used the `cv` argument to configure the number of cross-validation folds to use. A look into the comprehensive documentation illustrates all available options.\n",
    "\n",
    "- Grid search is essentially an algorithm for optimization. We search for the *best* hyperparameters. This requires deciding on an objective. By default, `GridSearchCV` will use classification accuracy and $R^2$ for classification and regression models, respectively. To overwrite this default and search for the hyperparameters yielding the highest AUC, we set the `scoring` argument to `roc_auc`. You can find a list of available metrics in the [sklearn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter).\n",
    "<br><br>\n",
    "\n",
    "In case you seek more insights into the performance of different hyperparameters, you can extract a `dictionary` with detailed results as shown below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef0e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract dictionary with results from grid search\n",
    "results = grid_search.cv_results_\n",
    "\n",
    "# Visualize performance grid as a heatmap\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(results['mean_test_score'].reshape(3, 3), annot=True, fmt='.3f', xticklabels=learning_rates, yticklabels=n_estimators_list, ax=ax)\n",
    "plt.xlabel('Learning rate')\n",
    "plt.ylabel('N estimators')\n",
    "plt.title('Grid search performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34747efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly, we can extract the best model from the grid search and evaluate it on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "yhat = best_model.predict_proba(X_test)[:, 1]\n",
    "print(f'Test AUC: {roc_auc_score(y_true=y_test, y_score=yhat):.3f}')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "RocCurveDisplay.from_estimator(estimator=grid_search.best_estimator_, X=X_test, y=y_test, ax=ax)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8645bb",
   "metadata": {},
   "source": [
    "Note how grid search has facilitated a small further increase of the AUC compared to the untuned GBM classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a6cbda",
   "metadata": {},
   "source": [
    "## 4.3 Exercise 3\n",
    "In Exercise 2, you experimented with different hyperparameters for the GBM model. Now, you will do the same for the Random Forest model. More specifically,\n",
    "\n",
    "- Identify three hyperparameters of the Random Forest model that you want to tune.\n",
    "- Define a parameter grid with multiple values for each hyperparameter.\n",
    "- Use `GridSearchCV` to tune the hyperparameters of a Random Forest classifier that yield the maximum **F1 score**.\n",
    "- Report the performance of each hyperparameter combination and identify the best-performing combination. A simple print statement will suffice.\n",
    "- Compare the performance of the tuned Random Forest model to the Random Forest model that we created in the demo 2.2. Compare the two classifiers using ROC analysis and report the relative improvement in F1 due to hyperparameter tuning. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d675af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# Define the parameter grid for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [10, 100, 250],\n",
    "    'max_depth': [3, 10, 20],\n",
    "    'max_features': [1, 2, 4, 8]\n",
    "}\n",
    "\n",
    "# Perform grid search with RF classifier and F1 score as the evaluation metric (may take a while)\n",
    "grid_search_rf = GridSearchCV(estimator=RandomForestClassifier(random_state=888), \n",
    "                              param_grid=param_grid_rf, \n",
    "                              cv=5, scoring='f1', verbose=2)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Print the best parameters and the corresponding F1 score\n",
    "print(f'Best F1 Score: {grid_search_rf.best_score_:.3f}')\n",
    "print(f'Best parameters: {grid_search_rf.best_params_}')\n",
    "rf_tuned = grid_search_rf.best_estimator_  # Extract the best model from the grid search\n",
    "\n",
    "# Recreate Random Forest from 2.2 for comparison\n",
    "rf_untuned = RandomForestClassifier( n_estimators=100,  # the number of base model trees\n",
    "                                     max_depth=5,      # the maximum depth of each tree\n",
    "                                     random_state=888  # Random number seed. Recall that RF is stochastic due to bootstrap sampling and random subspace\n",
    "                                   )\n",
    "rf_untuned.fit(X_train, y_train)\n",
    "\n",
    "# Compare the performance of the tuned Random Forest model to the untuned Random Forest model using ROC analysis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "RocCurveDisplay.from_estimator(estimator=rf_tuned,   X=X_test, y=y_test, ax=ax, name=\"Tuned Random Forest\")\n",
    "RocCurveDisplay.from_estimator(estimator=rf_untuned, X=X_test, y=y_test, ax=ax, name=\"Untuned Random Forest\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='red')\n",
    "plt.show()\n",
    "\n",
    "# Compute the relative improvement in F1 score (reusing the instance of the untuned RF from Section 2.2)\n",
    "f1_tuned = f1_score(  y_true=y_test, y_pred=rf_tuned.predict(X_test))\n",
    "f1_untuned = f1_score(y_true=y_test, y_pred=rf_untuned.predict(X_test))\n",
    "print(f'Test F1 Score with (without) tuning: {f1_tuned:.3f} ({f1_untuned:.3f})')\n",
    "\n",
    "rel_improve = (f1_tuned - f1_untuned) / f1_untuned * 100\n",
    "print(f'Relative improvement in F1 Score: {rel_improve:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc9a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the performance of each hyperparameter combination\n",
    "results_rf = grid_search_rf.cv_results_\n",
    "for i in range(len(results_rf['params'])):\n",
    "    print(f'Parameters: {results_rf[\"params\"][i]}, F1 Score: {results_rf[\"mean_test_score\"][i]:.3f}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "B0rxPs4QEGtz",
    "27sCENzmoGcX"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "bads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
