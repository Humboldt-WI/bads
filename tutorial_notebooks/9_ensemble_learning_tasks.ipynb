{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97f60c7",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorial_notebooks/9_ensemble_learning_tasks.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd632d8e",
   "metadata": {},
   "source": [
    "# Tutorial 9 - Ensemble Learning \n",
    "\n",
    "In this tutorial, we revisit our lecture on ensemble learning, which has explained new algorithms, namely Random Forest (RF) and Gradient Boosting (GBM), and, more generally, introduced us to the space of *ensemble learners*, which generate a composite forecast by integrating the predictions of multiple *base models*. We will cover the following topics:\n",
    "\n",
    "#### Contents\n",
    "\n",
    "1. [Foundations of Ensemble Modeling and Forecast Combination](#foundations)  \n",
    "2. [Bagging and Random Forest](#bagging-rf)  \n",
    "3. [Gradient Boosting](#gradient-boosting) \n",
    "\n",
    "Within each section, you will find:\n",
    "- A **Concept Recap** highlighting the key ideas from the lecture.\n",
    "- A **Programming Demo** illustrating these ideas in Python.\n",
    "- **Exercises** to deepen your understanding and practice these methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f867e0",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "### Standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85d210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91904e69",
   "metadata": {},
   "source": [
    "### The HMEQ data set\n",
    "We continue using the \"Home Equity\" data set (HMEQ), which we can nicely load and prepare using the helper function `get_HMEQ_credit_data`, which is available in our courses module `bads_helper_functions.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c383882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bads_helper_functions as bads  # import module with bads helper functions\n",
    "X, y = bads.get_HMEQ_credit_data()  # load the data \n",
    "\n",
    "print(\"Data loaded. Shape of X: \", X.shape, \"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2179a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X  # preview the data   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb489532",
   "metadata": {},
   "source": [
    "<a id=\"foundations\"></a>\n",
    "# 1. Foundations of Ensemble Modeling and Forecast Combination\n",
    "\n",
    "## 1.1 Concept Recap\n",
    "In ensemble modeling, multiple models (referred to as “base learners” or “weak learners”) are combined to produce a single predictive model. The motivation behind ensemble modeling is that by combining various learners, we often get better generalization performance than using a single learner. Ensemble models help to:\n",
    "\n",
    "- Reduce variance (stabilize predictions).\n",
    "- Potentially reduce bias (when certain conditions are met).\n",
    "- Provide more robust and reliable predictions across different problem domains.\n",
    "\n",
    "Techniques include:\n",
    "\n",
    "- Averaging Methods: e.g., Bagging (Bootstrap Aggregating)\n",
    "- Boosting Methods: e.g., Gradient Boosting\n",
    "- Heterogeneous approaches, which integrate different learning algorithms to farm the base models. Examples include the Stacking algorithm, which combines base models using a meta-model (see, e.g., [this tutorial for a demo](https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/) if interested). The lecture did not elaborate on heterogeneous ensemble learners; neither will this tutorial. We mainly mention them for the sake of completeness. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73cc8ad",
   "metadata": {},
   "source": [
    "\n",
    "# 1.2 Programming Demo: Simple Averaging Ensemble\n",
    "Below is a simple demonstration of how one might combine three different models (a Logistic Regression and two Decision Trees) by averaging their probability estimates. This is actually a heterogeneous ensemble, as we combine different types of models. The example is meant to convey the generality of ensemble modeling, which the lecture illustrated as follows: <br>\n",
    "<br>\n",
    "<img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/ensemble_learning.png\" width=\"854\" height=\"480\" alt=\"Ensemble Learning\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d3cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=888)  # partition the data into 75% training and 25% test\n",
    "\n",
    "# Define base models\n",
    "base_learners = [\n",
    "    LogisticRegression(max_iter=1000, random_state=888),\n",
    "    DecisionTreeClassifier(max_depth=3),\n",
    "    DecisionTreeClassifier(max_depth=10)\n",
    "]\n",
    "\n",
    "# Train the models\n",
    "for m in base_learners:\n",
    "    m.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "probs = []\n",
    "for m in base_learners:\n",
    "    yhat = m.predict_proba(X_test)[:, 1]\n",
    "    probs.append(yhat)\n",
    "\n",
    "# Simple average forecast \n",
    "ensemble_prob = np.mean(probs, axis=0)\n",
    "\n",
    "# Performance evaluation\n",
    "print('Performance evaluation in terms of AUC:')\n",
    "print('-' * 50)\n",
    "print(f'Ensemble \\t {roc_auc_score(y_true=y_test, y_score=ensemble_prob):.3f}')\n",
    "print('-' * 50)\n",
    "for i, p in enumerate(probs):\n",
    "    print(f'Base model {i} \\t {roc_auc_score(y_true=y_test, y_score=p):.3f}')\n",
    "print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b504809",
   "metadata": {},
   "source": [
    "<a id=\"bagging-rf\"></a>\n",
    "\n",
    "# 2. Bagging and Random Forest\n",
    "\n",
    "## 2.1 Concept Recap\n",
    "Bagging (Bootstrap Aggregating):\n",
    "\n",
    "In bagging, we train multiple instances of the same model class on different bootstrap samples (randomly sampled with replacement) of the original training set.\n",
    "The final prediction is usually made by majority voting (classification) or averaging (regression).\n",
    "\n",
    "Random Forest:\n",
    "\n",
    "Random Forest is an extension of bagging, which uses Decision Trees as base learners. Random Forest also uses feature sub-sampling at each split to decorrelate individual trees. Much empirical evidence suggests that Random Forest is a very effective and robust algorithm for a wide range of prediction tasks. It is also relatively simple to tune.\n",
    "\n",
    "## 2.2 Programming Demo: Random Forest\n",
    "Below, we train a simple Random Forest, measure its AUC and plot the ROC curve. To showcase the effectiveness of Random Forest, we add the results of the previous demo, our heterogeneous ensemble, to the ROC plot. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f78f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "# Instantiate and train RandomForest\n",
    "rf_model = RandomForestClassifier(n_estimators=100,  # the number of base model trees\n",
    "                                   max_depth=5,      # the maximum depth of each tree\n",
    "                                   random_state=888  # Random number seed. Recall that RF is stochastic due to bootstrap sampling and random subspace\n",
    "                                   )\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "rf_probs = rf_model.predict_proba(X_test)[:,1]\n",
    "print(f\"Random Forest AUC: {roc_auc_score(y_true=y_test, y_score=rf_probs):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86153c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RF performance and compare to benchmarks\n",
    "f, ax = plt.subplots(figsize=(8,6))\n",
    "RocCurveDisplay.from_estimator(estimator=rf_model, X=X_test, y=y_test, ax=ax)\n",
    "RocCurveDisplay.from_predictions(y_true=y_test, y_pred=ensemble_prob, ax=ax, name=\"Simple avg. ensemble\")\n",
    "for i, p in enumerate(probs):\n",
    "    RocCurveDisplay.from_predictions(y_true=y_test, y_pred=p, ax=ax, name=f\"Base model {i}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='red')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7d027b",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Unlike RF, the Bagging algorithm is not specific to Decision Trees. It can be applied to any base learner.\n",
    "\n",
    "Here is your task:<br>\n",
    "- Train two Bagging classifiers, one using logistic regression and one using decision trees to train the base models. This allows for a comparison of which algorithm, logistic regression or decision tree, benefits more from bagging. An implementation of the Bagging algorithm is available in the `sklearn.ensemble` module. \n",
    "- For each Bagging classifier, use 50 base learners and set the random state to 888. \n",
    "  - For the logistic regression base learner, set `max_iter=1000`, and `random_state=888`\n",
    "  - For the decision tree base learner, set `max_depth=3`\n",
    "  - Note that these are the same settings as used in our first demo on the simple average ensemble. \n",
    "- Create an ROC chart to compare the models. Specifically:\n",
    "  - Plot the ROC curve for the bagged logistic regression\n",
    "  - Plot the ROC curve for the bagged decision tree\n",
    "  - Plot the ROC curve for the non-bagged logistic regression trained in the first demo. To do this, simply reuse the already stored predictions `probs[0]`\n",
    "  - Also add a ROC curve for the non-bagged decision tree trained. Again, simply reuse the available predictions `probs[1]`\n",
    "- Add a legend to the plot to distinguish the different models.\n",
    "    \n",
    "\n",
    "<details> <summary>Hint on bagging </summary> Use the following scaffolding to configure the Bagging classifier. :\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bagging_model = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=___), \n",
    "    n_estimators=___, \n",
    "    random_state=___\n",
    ")\n",
    "bagging_model.fit(X_train, y_train)\n",
    "# Evaluate your model\n",
    "```\t\n",
    "</details>\n",
    "\n",
    "<details> <summary>Hint on the ROC curve </summary> Note that the previous ROC curve example illustrates how to draw curves for the non-bagged models. Assuming you have executed previous code cells, you can reuse the stored predictions `probs[0]` and `probs[1]` to plot the ROC curves for the non-bagged models as follows:\n",
    "\n",
    "```python\n",
    "RocCurveDisplay.from_predictions(y_true=y_test, y_pred=probs[0], ax=ax, name=f\"Logistic regression\")\n",
    "```\t\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46649a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c34c96",
   "metadata": {},
   "source": [
    "Examining the result of Exercise 1, what can you say about the performance of the bagged logistic regression and the bagged decision tree compared to the non-bagged models?\n",
    "\n",
    "More interestingly, what factors might explain the observed effectiveness of bagging (or lack thereof)?\n",
    "\n",
    "**Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce6643",
   "metadata": {},
   "source": [
    "<a id=\"gradient-boosting\"></a>\n",
    "\n",
    "# 3. Gradient Boosting\n",
    "## 3.1 Concept Recap\n",
    "Boosting builds an ensemble of weak learners in a sequential way, where each new learner attempts to correct the errors of the previous ensemble. Gradient Boosting is a general boosting framework:\n",
    "\n",
    "- Models are added sequentially.\n",
    "- Each subsequent model is trained to reduce the residual errors (or gradient of the loss) of the current ensemble.\n",
    "- Learning rate (shrinkage) controls how strongly each new model influences the ensemble.\n",
    "\n",
    "Common implementations:\n",
    "- GradientBoostingClassifier (in sklearn.ensemble)\n",
    "- XGBoost, LightGBM, CatBoost (popular specialized libraries)\n",
    "\n",
    "## 3.2 Programming Demo: Gradient Boosting\n",
    "The code snippet below demonstrates training and evaluation of a gradient boosting classifier using the `GradientBoostingClassifier` implementation from `sklearn.ensemble`. While this implementation suffices for our purposes, it is worth noting that the specialized libraries like XGBoost, LightGBM, and CatBoost are preferred in practice due to their superior performance and scalability. Thus, when working on real-world projects and with larger data sets, you should consider using one of these libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd505fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100,\n",
    "                                      learning_rate=0.1,\n",
    "                                      max_depth=3,\n",
    "                                      random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "RocCurveDisplay.from_estimator(estimator=gb_model, X=X_test, y=y_test, ax=ax)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='red')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f23dcb",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "All previous demos and exercises have used default hyperparameters for the ensemble models. However, hyperparameter tuning is crucial for achieving optimal performance.\n",
    "\n",
    "Here is your task:\n",
    "Drawing on the above demo of GBM, experiment with different settings for the hyperparameters `learning_rate` and `n_estimators`. Record the AUC values of different models with different hyperparameters to study how the hyperparameters affect performance.\n",
    "\n",
    "<details> <summary>Hint</summary> \n",
    "One way to approach exercise 2 is to loop over different candidate settings for each hyperparameter and record the AUC in each round. For example, you could:<br><br>\n",
    "1. Loop over a small set of learning_rate values, e.g. `[0.01, 0.1, 0.2]`. <br>\n",
    "2. Loop over a small set of n_estimators values, e.g. `[50, 100, 200]`. <br>\n",
    "3. Measure the the AUC of the corresponding GBM model using `roc_auc_score()`. <br>\n",
    "4. Keep track of the highest AUC value obtained thus far and the corresponding hyperparameters. <br>\n",
    "  - To do this, you could use a variable `best_auc`, which you compare to the AUC obtained in the current iteration.<br>\n",
    "  - Any time you observe the current AUC to be higher than the best AUC, update the best AUC. <br>\n",
    "  - You can proceed in the same way with the hyperparameters.<br>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "221d6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 2"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "B0rxPs4QEGtz",
    "27sCENzmoGcX"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "bads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
