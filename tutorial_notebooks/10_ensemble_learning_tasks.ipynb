{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97f60c7",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorial_notebooks/10_ensemble_learning_tasks.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd632d8e",
   "metadata": {},
   "source": [
    "# Tutorial 10 - Ensemble Learning \n",
    "\n",
    "In this tutorial, we revisit the ensemble learning lecture, which introduced *ensemble learners*, approaches that generate a composite forecast by integrating the predictions of multiple *base models*. In terms of concrete algorithms, we learned about Random Forest (RF) and Gradient Boosting (GBM).\n",
    "\n",
    "This tutorial covers the following topics:\n",
    "\n",
    "#### Table of Contents\n",
    "\n",
    "1. [Foundations of Ensemble Modeling and Forecast Combination](#foundations)  \n",
    "2. [Bagging and Random Forest](#bagging-rf)  \n",
    "3. [Gradient Boosting](#gradient-boosting) \n",
    "\n",
    "Within each section, you find:\n",
    "- A **Concept Recap** highlighting the key ideas from the lecture.\n",
    "- A **Programming Demo** illustrating these ideas in Python.\n",
    "- **Exercises** to deepen your understanding and practice these methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f867e0",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "### Standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91904e69",
   "metadata": {},
   "source": [
    "### The HMEQ data set\n",
    "We continue using the \"Home Equity\" data set (HMEQ), which was introduced in [Tutorial 6](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorial_notebooks/6_classification_tasks.ipynb). That tutorial also provided a helper function,`prepare_hmeq_data`,  to load and prepare the data. To avoid copy/pasting that (comprehensive) helper function into this and future tutorials, we moved it into a Python module `bads_helper_functions.py`. The module is available in the [BADS GitHub repository](https://github.com/Humboldt-WI/bads). Below, we first import our custom module and then call the helper function from that module. For thw code to function on your machine, you have to make sure that the **module is found by your Python interpreter**. While there are different ways to ensure this, the simplest way would be to save the module to the **same folder** to which you also saved **this notebook**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c383882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bads_helper_functions as bads  # import module with bads helper functions\n",
    "# Load the data directly from GitHub\n",
    "data_url = 'https://raw.githubusercontent.com/Humboldt-WI/bads/master/data/hmeq.csv'\n",
    "hmeq = pd.read_csv(data_url)\n",
    "\n",
    "X, y = bads.prepare_hmeq_data(hmeq)  \n",
    "print(\"Data loaded. Shape of X: \", X.shape, \"Shape of y:\", y.shape)\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb489532",
   "metadata": {},
   "source": [
    "<a id=\"foundations\"></a>\n",
    "# 1. Foundations of Ensemble Modeling and Forecast Combination\n",
    "\n",
    "## 1.1 Concept Recap\n",
    "In ensemble modeling, multiple models (referred to as “base learners” or “weak learners”) are combined to produce a single predictive model. The motivation behind ensemble modeling is that by combining various learners, we often get better generalization performance than using a single learner. Ensemble models help to:\n",
    "\n",
    "- Reduce variance (stabilize predictions).\n",
    "- Potentially reduce bias (when certain conditions are met).\n",
    "- Provide more robust and reliable predictions across different problem domains.\n",
    "\n",
    "Techniques include:\n",
    "\n",
    "- Averaging Methods: e.g., Bagging (Bootstrap Aggregating)\n",
    "- Boosting Methods: e.g., Gradient Boosting\n",
    "- Heterogeneous approaches, which integrate different learning algorithms to farm the base models. Examples include the Stacking algorithm, which combines base models using a meta-model (see, e.g., [this tutorial for a demo](https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/) if interested). The lecture did not elaborate on heterogeneous ensemble learners; neither will this tutorial. We mainly mention them for the sake of completeness. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73cc8ad",
   "metadata": {},
   "source": [
    "\n",
    "## 1.2 Programming Demo: Simple Averaging Ensemble\n",
    "Below is a simple demonstration of how one might combine three different models (a Logistic Regression and two Decision Trees) by averaging their probability estimates. This is actually a heterogeneous ensemble, as we combine different types of models. The example is meant to convey the generality of ensemble modeling, which the lecture illustrated as follows: <br>\n",
    "<br>\n",
    "<p align=\"left\" class=\"alert\">\n",
    "  <img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/ensemble_learning.png\" width=\"640\" alt=\"Ensemble Learning\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d3cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "fix_seed = 888  # seed for random number generator\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=fix_seed)  # partition the data into 75% training and 25% test\n",
    "\n",
    "# Define base models\n",
    "base_learners = [\n",
    "    LogisticRegression(max_iter=1000,    random_state=fix_seed),\n",
    "    DecisionTreeClassifier(max_depth=3,  random_state=fix_seed),\n",
    "    DecisionTreeClassifier(max_depth=10, random_state=fix_seed)    \n",
    "]\n",
    "\n",
    "# Train the models\n",
    "for m in base_learners:\n",
    "    m.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "probs = []\n",
    "for m in base_learners:\n",
    "    yhat = m.predict_proba(X_test)[:, 1]\n",
    "    probs.append(yhat)\n",
    "\n",
    "# Simple average forecast \n",
    "ensemble_prob = np.mean(probs, axis=0)\n",
    "\n",
    "# Performance evaluation\n",
    "print('Performance evaluation in terms of AUC:')\n",
    "print('-' * 100)\n",
    "print(f'Ensemble \\t {roc_auc_score(y_true=y_test, y_score=ensemble_prob):.3f}')\n",
    "print('-' * 100)\n",
    "for i, p in enumerate(probs):\n",
    "    print(f'Base model {i} \\t {roc_auc_score(y_true=y_test, y_score=p):.3f} \\t ({base_learners[i]})')\n",
    "print('-' * 100)\n",
    "\n",
    "\n",
    "# Also report the pairwise correlation among the base learners\n",
    "pd.DataFrame({\"Logit\": probs[0], \"Shallow Tree\": probs[1], \"Deep Tree\": probs[2]}).corr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b504809",
   "metadata": {},
   "source": [
    "<a id=\"bagging-rf\"></a>\n",
    "\n",
    "# 2. Bagging and Random Forest\n",
    "\n",
    "## 2.1 Concept Recap\n",
    "*Bagging (Bootstrap Aggregating)*:\n",
    "\n",
    "In bagging, we train multiple instances of the same model class on different bootstrap samples (randomly sampled with replacement) of the original training set.\n",
    "The final prediction is usually made by majority voting (classification) or averaging (regression).\n",
    "\n",
    "*Random Forest (RF)*:\n",
    "\n",
    "RF is an extension of bagging, which uses decision trees as base learners. RF also uses feature sub-sampling at each split to decorrelate individual trees. Much empirical evidence suggests that RF is an effective and robust algorithm for a wide range of prediction tasks. It is also relatively simple to tune.\n",
    "\n",
    "## 2.2 Programming Demo: Random Forest\n",
    "To illustrate RF, we train a classification model, measure its AUC, and plot the ROC curve. To showcase the effectiveness of Random Forest, we add the results of the previous demo, our heterogeneous ensemble, to the ROC plot. Recall from the lecture that RF exhibits some hyperparameter, which may deserve some tuning. For the demo, we use the default hyperparameter settings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f78f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "# Instantiate and train RandomForest\n",
    "rf_model = RandomForestClassifier(random_state=fix_seed)  # Random number seed. Recall that RF is stochastic due to bootstrap sampling and random subspace)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "rf_probs = rf_model.predict_proba(X_test)[:,1]\n",
    "print(f\"Random Forest AUC: {roc_auc_score(y_true=y_test, y_score=rf_probs):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86153c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RF performance and compare to benchmarks\n",
    "f, ax = plt.subplots(figsize=(8,6))\n",
    "RocCurveDisplay.from_estimator(estimator=rf_model, X=X_test, y=y_test, ax=ax)\n",
    "RocCurveDisplay.from_predictions(y_true=y_test, y_pred=ensemble_prob, ax=ax, name=\"Simple avg. ensemble\")\n",
    "for i, p in enumerate(probs):\n",
    "    RocCurveDisplay.from_predictions(y_true=y_test, y_pred=p, ax=ax, name=f\"Base model {i}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='red')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7d027b",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Unlike RF, the Bagging algorithm is not specific to Decision Trees. It can be applied to any base learner.\n",
    "\n",
    "Here is your task:<br>\n",
    "- Train two Bagging classifiers, one using logistic regression and one using decision trees to train the base models. This allows for a comparison of which algorithm, logistic regression or decision tree, benefits more from bagging. An implementation of the Bagging algorithm is available in the `sklearn.ensemble` module. \n",
    "- For each Bagging classifier, use 50 base learners and set the random state to 888. \n",
    "  - For the logistic regression base learner, set `max_iter=1000`, and `random_state=888`\n",
    "  - For the decision tree base learner, set `max_depth=3`\n",
    "  - Note that these are the same settings as used in the above demo on the simple average ensemble. \n",
    "- Create an ROC chart to compare the models. Specifically:\n",
    "  - Plot the ROC curve for the bagged logistic regression\n",
    "  - Plot the ROC curve for the bagged decision tree\n",
    "  - Plot the ROC curve for the non-bagged logistic regression trained in the first demo. To do this, simply reuse the already stored predictions `probs[0]`\n",
    "  - Also add a ROC curve for the non-bagged decision tree trained. Again, simply reuse the available predictions `probs[1]`\n",
    "- Make sure all ROC curves are shown in one chart by setting the argument `ax` of the RocCurveDisplay.from_estimator()` function, as is already illustrated in the above ROC curve. \n",
    "- Add a legend to the plot to distinguish the different models.\n",
    "    \n",
    "\n",
    "<details> <summary>Hint on bagging </summary> Use the following scaffolding to configure the Bagging classifier. :\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bagging_model = BaggingClassifier(\n",
    "    base_estimator=___, \n",
    "    n_estimators=___, \n",
    "    random_state=___\n",
    ")\n",
    "bagging_model.fit(X_train, y_train)\n",
    "# Evaluate your model\n",
    "\n",
    "```\t\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46649a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c34c96",
   "metadata": {},
   "source": [
    "Examining the result of Exercise 1, what can you say about the performance of the bagged logistic regression and the bagged decision tree compared to the non-bagged models?\n",
    "\n",
    "More interestingly, what factors might explain the observed effectiveness of bagging (or lack thereof)?\n",
    "\n",
    "**Your Answer:**\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39745828",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cce6643",
   "metadata": {},
   "source": [
    "<a id=\"gradient-boosting\"></a>\n",
    "\n",
    "# 3. Gradient Boosting\n",
    "## 3.1 Concept Recap\n",
    "Boosting builds an ensemble of weak learners in a sequential way, where each new learner attempts to correct the errors of the previous ensemble. Gradient Boosting (GB) is a general boosting framework that suggests a principled way of how to implement the *correction of errors* idea. Specifically, GB defines the (negative) gradient of the loss function with respect to the current ensemble to be the target variable for base model training. Note that the the gradient of the loss function equates to model residuals, $y - \\hat{y}$, in the specific case of the least-squares loss function. Then, GB fits base models to predict (i.e., reduce) the residual of the present ensemble. For other loss functions, for example, cross-entropy, the equivalence between gradients and residuals does not hold. Therefore, characterizing GB as a boosting approach that incrementally fits base models to the residuals of the current ensemble is helpful to build understanding, while the description of GB incrementally fitting base models to loss gradients is more general. Either way, the GB algorithm incorporates a learning rate (shrinkage) hyperparameter that controls how strongly each new model influences the ensemble. A smaller learning rate means that each new model has a smaller impact, which can lead to better generalization but requires more base models (and thus more computation).\n",
    "\n",
    "Common GB implementations include:\n",
    "- GradientBoostingClassifier (in sklearn.ensemble)\n",
    "- XGBoost, LightGBM, CatBoost (popular specialized libraries)\n",
    "\n",
    "## 3.2 Programming Demo: Gradient Boosting\n",
    "The following code snippet demonstrates the training and evaluation of a GB classifier using the `GradientBoostingClassifier` implementation from `sklearn.ensemble`. While this implementation suffices for our purposes, it is worth noting that the specialized libraries like XGBoost, LightGBM, and CatBoost are preferred in practice due to their superior performance and scalability. Thus, when working on real-world projects or larger datasets, you should consider using one of these libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd505fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100,\n",
    "                                      learning_rate=0.1,\n",
    "                                      max_depth=3,\n",
    "                                      random_state=fix_seed)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "RocCurveDisplay.from_estimator(estimator=gb_model, X=X_test, y=y_test, ax=ax)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='red')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f23dcb",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Previous demos and exercises did not elaborate on hyperparameters. Hyperparameter tuning is often useful when working with ensemble algorithms, particularly GB.\n",
    "\n",
    "**Here is your task**:\n",
    "\n",
    "Drawing on the GB demo, experiment with different settings for the hyperparameters `learning_rate` and `n_estimators`. Record the AUC values of different models with different hyperparameters to study how the hyperparameters affect performance.\n",
    "\n",
    "One way to achieve this involves looping over different candidate settings for each hyperparameter and recording the corresponding AUC in each round. Alternatively, [tutorial 8]() on ML theory and practice introduced you the `GridSearchCV` class from `sklearn.model_selection`, which facilitates hyperparameter tuning using grid search. Following a tuning run, the class provides detailed results from the candidate hyperparameter evaluations and can, therefore, also be used to solve this exercise. \n",
    "\n",
    "<details><summary>Hints on <code>GridSearchCV</code></summary> You can draw on the following scaffolding to configure the search grid for your GB classifier\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set the hyperparameter grid\n",
    "gb_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(param_grid=gb_grid, ___)\n",
    "grid_search.fit(___)\n",
    "\n",
    "```\t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221d6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 2\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "B0rxPs4QEGtz",
    "27sCENzmoGcX"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "bads310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
