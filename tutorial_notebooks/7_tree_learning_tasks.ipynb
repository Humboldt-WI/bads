{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97f60c7",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorial_notebooks/7_tree_learning_tasks.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd632d8e",
   "metadata": {},
   "source": [
    "# Tutorial 7 - Classification using Logistic Regression and Decision Trees\n",
    "\n",
    "In this demo notebook, we will recap the logistic regression model and revisit our lecture on decision trees.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f867e0",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "### Standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91904e69",
   "metadata": {},
   "source": [
    "### The HMEQ data set\n",
    "We continue using the \"Home Equity\" data set (HMEQ). To streamline the notebook, and following notebooks using the same data set, we implemented a helper function `get_HMEQ_credit_data` that  loads and preprocesses the data. To use this helper function, you need to import it just as other libraries/modules. More specifically, you need to:\n",
    "- Ensure the file `bads_helper_functions.py`, which is available on our [GitHub](https://github.com/Humboldt-WI/bads), is stored in the same directory from which you run this notebook. \n",
    "- Import the module using `import bads_helper_functions as bads`.\n",
    "\n",
    "Afterwards, you can call the function `get_HMEQ_credit_data` to load a ready-to-use version of the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c383882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bads_helper_functions as bads\n",
    "X, y = bads.get_HMEQ_credit_data()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742e463-9ab9-49ca-a84e-3c1bfb004a90",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9bfc1-ab50-465e-934f-1034f587ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(random_state=888)  # the random_state ensures that we get the same results when re-running this cell multiple times\n",
    "model.fit(X, y)  # Train the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb9920-d1b3-4a19-9dfa-0a14bb7210bc",
   "metadata": {},
   "source": [
    "Note that the `sklearn` implementation does not provide an informative summary, as did the library `statsmodels`, which we used to [illustrate regression analysis](https://github.com/Humboldt-WI/bads/blob/master/tutorial_notebooks/4_predictive_analytics_tasks.ipynb). You can still access the estimated coefficients and the intercept using the attributes `coef_` and `intercept_` of the fitted model. However, $R^2$, p-values or confidence intervals are not available. In brief, this is because `sklearn` is designed to support prediction. Let's demonstrate how to do this, that is compute predictions using the trained model. For simplicity, we compute prediction for the training data. You already learnt that this is inappropriate and that we should use the *holdout* method instead. We will talk about model evaluation in a future notebook. Here, we keep things simple and compute predictions for the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2856dfd-fe11-4ed1-803e-63f66c58b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Estimated coefficients:\\n\", model.coef_)  # The coefficients of the model \n",
    "print(\"\\nIntercept coefficients:\\n\", model.intercept_)  # The intercept of the model   \n",
    "yhat = model.predict(X)  # simple way to compute predictions using logistic regression and any other machine learning model in sklearn \n",
    "print(\"\\nPredictions:\\n\", yhat)  # The predictions of the model   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b594ac70-c675-4eae-951a-2210ee923dc8",
   "metadata": {},
   "source": [
    "### Diagnosing predictions\n",
    "The above output hints at an issue with our predictions. We discuss this part in the tutorial and *debug* the predictions to fully understand what is going on when we call the function `predict()` and when this function is useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f394a7cb-f1d2-4981-b0e0-7c73eb457a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed in class...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8173f614-b287-4e1b-b4a3-2937c88e1383",
   "metadata": {},
   "source": [
    "### Visualizing the logistic regression\n",
    "We complete our examination of the logistic regression model with a visualization of its behavior. Given that plotting is resticted to low dimensional data, we consider only two features of our data set in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127568a4-102e-4b5a-9b2b-9a7d522421fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate low dimensional logit model for visualization\n",
    "x1 = \"YOJ\"\n",
    "x2 = \"DEBTINC\"\n",
    "model2 = LogisticRegression(random_state=888).fit(X[[x1, x2]], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c727e2-8442-4b32-b857-a747cc36ba62",
   "metadata": {},
   "source": [
    "For visualization, we use another helper function `plot_logit_decision_surface()`. Let's first inspect its interface and understand how we can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf6344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(bads.plot_logit_decision_surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd9a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to call the function plot_logit_decision_surface()\n",
    "bads.plot_logit_decision_surface(model2, X, x2, x1, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f820b3f7",
   "metadata": {},
   "source": [
    "#### Illustration using synthetic data\n",
    "Given our real-world lending data is challenging to work with, we also demonstrate the use of our visualization function using well-behaved synthetic data. To that end, we use the function `make_blobs()`, which we learnt about several weeks ago in our [clustering tutorial](https://github.com/Humboldt-WI/bads/blob/master/tutorial_notebooks/3_descriptive_analytics_solutions.ipynb). This function allows us to generate a well-behaved two-dimensional data set with two classes that logistic regression can distinguish easily. The resulting plot will help us understand the decision boundary of the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea635951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "Xsyn, ysyn = make_blobs(n_samples=250, centers=2, cluster_std=1, n_features=2, random_state=88)\n",
    "Xsyn = pd.DataFrame(Xsyn, columns=[\"x1\", \"x2\"])\n",
    "model_syn = LogisticRegression(random_state=88).fit(Xsyn, ysyn)\n",
    "bads.plot_logit_decision_surface(model_syn, Xsyn, \"x2\", \"x1\", ysyn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc535c0-2689-48f6-9670-d73c08fb67bd",
   "metadata": {},
   "source": [
    "# Decision trees\n",
    "Once you are familiar with `sklearn`, switching to another model is straightforward. We will now introduce decision trees, including how to train them,  how to visualize grown trees, and how to compute predictions. For training and prediction, you already know all you need. The functions `fit()` and `predict()` are the same as for logistic regression. Likewise, you can obtain probabilistic predictions using the function `predict_proba()`. For visualization, you can use the function `plot_tree`. All these functions are part of the module `sklearn.tree`.\n",
    "\n",
    "These information should be sufficient to get started. Here is your task: \n",
    "\n",
    "## Exercise tree learning\n",
    "1. Train a decision tree on the HMEQ data set. Set the maximum depth of the tree to 3.\n",
    "2. Visualize the tree using the function `plot_tree()`.\n",
    "3. Compute probabilistic predictions for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1148d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution:"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "B0rxPs4QEGtz",
    "27sCENzmoGcX"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "bads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
