{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JW8q2Hxl3thA"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorial_notebooks/3_descriptive_analytics_tasks.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_55XGJ5Krxox"
   },
   "source": [
    "# Chapter 2 - Foundations of descriptive analytics\n",
    "The lecture has introduced you to the field of descriptive analytics. Out of the space of methods for descriptive analytics, we concentrated on cluster analysis. Given this focus, today's tutorial notebook will revisit the **kMeans** algorithm and deepen our understanding of how it segments a given data set. This will also help us to introduce more Python functionality and further develop our understanding of Python programming and relevant libraries. \n",
    "\n",
    "Here is the outline of the demo notebook:\n",
    "*   Generating data for cluster analysis \n",
    "*   kMeans Algorithm using sklearn\n",
    "*   Finding the optimal \"k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-wD7vGxHFa5"
   },
   "source": [
    "# Cluster Analysis\n",
    "\n",
    "Cluster analysis is a good example of unsupervised machine learning. Unsupervised learning algorithms group observations based on their similarities in features values. In the end, humans need to characterize found clusters and how they differ from one another. That task, however, is often challenging and **not supported by the algorithm**. We must rely on domain expertise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8BjFh5UsQel"
   },
   "source": [
    "## Data Generation for cluster analysis\n",
    "\n",
    "We begin with illustrating clustering on synthetic data. Abstracting from a specific application by using synthetic data allows us to focus on algorithmic steps and illustrate a vanilla unsupervised learning workflow. \n",
    "\n",
    "To generate data, we use the function `make_blob()` from the `sklean.datasets` namespace. In a nutshell, we generate data from two Gaussian distributions. The function allows specifying the parameters of these Gaussians. This way, we can decide how similar or how dissimilar we want our data to be. Have a look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) to appreciate the full functionality of the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "whNm-Zlud3WV"
   },
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_daZwJTuzrj"
   },
   "outputs": [],
   "source": [
    "# Generate data in two dimensions using make_blobs:\n",
    "\n",
    "# 1. Import the function to generate data\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# 2. Print out the documentation of the function to see how it is to be used\n",
    "\n",
    "make_blobs?  # appending a ? to the function name will print the function documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mini-Task\n",
    "Based on the documentation of `make_blobs()` write code to call the function. Specifically, \n",
    "1. Introduce two variables that store the number of synthetic data points and their dimensionality. \n",
    "2. Set these variables to create a synthetic data set of 500 points with dimension 2\n",
    "3. Call the function. Set its arguments `n_samples` and `n_features` to the variables defined step 1\n",
    "4. Store the output of the function in other variable `data`. \n",
    "\n",
    "Make sure you understand the output. If in doubt, note that the function documentation comments on the output that is returned to the caller\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed in the tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mini-Task\n",
    "Provided you solved the previous task, you now have access to a variable `data` storing your synthetic data set. To better understand that data, \n",
    "- create a scatter plot depicting your two-dimensional data,\n",
    "- whereby data points that belong to different clusters should have different color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed in the tutorial\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mini-Task\n",
    "The scatter plots shows that, without explicitly deciding on the number of clusters, you *somehow* created synthetic data comprising three clusters. Your next task is to revise your function call to exert more control. Specifically, \n",
    "- Overwrite the variable `data` by calling once more the function `make_blobs()`\n",
    "- Configure the function call such that you can explicitly determine \n",
    "    - the number of clusters, and\n",
    "    - the spread of each cluster.\n",
    "- Set the number of clusters to 2\n",
    "\n",
    "In the end, you should have created 2-dimensional synthetic data with two clusters. Verify this by creating another scatter plot with color coding of data points from different clusters. \n",
    "\n",
    ">Hint: examine the function documentation and, specifically, the argument `center`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed in the tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSq6MXgyMCsp"
   },
   "source": [
    "Based on a visual inspection, any clustering method including *kMeans* should be able to cluster the data. More specifically, in our synthetic data, we **know** exactly **how many clusters exist** and **which data point belongs to which cluster**. This is precisely the information that would not be availble in a realistic setting. Rather, the very job of clustering would be to find out how many clusters exist and which data point belongs to which cluster. This is the beauty of synthetic data. Knowing the desired - *optimaö* - outcome of clustering allows us to assess the output of a clustering method. \n",
    "\n",
    "Before moving on with *kMeans*, recall that this algorithm is based on *centroids*. A centroid is the midpoint of a cluster. \n",
    "\n",
    "##### Mini-Task\n",
    "Let's exploit the fact that we know the true clusters in our synthetic data. Specifically:\n",
    "- compute the mean of each cluster (note this is a 2-d vector)\n",
    "- store the means in a list called `true_centroids`\n",
    "\n",
    "Solving this tasks allows us to later compare the true centroids to the centroids that *kMeans* has found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ytt_jOhfM8JB",
    "outputId": "462a6111-1091-458a-dd51-de27400a7a1d"
   },
   "outputs": [],
   "source": [
    "# To be completed in the tutorium\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlHawULKMCsx"
   },
   "source": [
    "Are the means consistent with your expectation? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRPjqtQpHhBr"
   },
   "source": [
    "# The kMeans algorithm\n",
    "\n",
    "The kMeans algorithm is an established and widely used method for clustering. Much more sophisticated algorithms exist, many of which are readily available in Python libraries. However, it is good practice to start simple. For the purpose of BADS, knowing kMeans and what clustering in general can do for us is enough. \n",
    "\n",
    "kMeans belongs to the family of non-hierarchical cluster analysis methods. We discussed its functioning in the lecture using, amongst others, the below illustration. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/kmeans.png\" width=\"1200\" height=\"800\" alt=\"kMeans Algorithm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9h1igA87vxUz"
   },
   "source": [
    "## kMeans with sklearn\n",
    "\n",
    "As with most popular machine learning algorithms, the library `sklearn` provides an implementation. We simply have to import the `KMeans` function from `sklearn.cluster`.\n",
    "\n",
    "The documentation for this function can be found here: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html. Once you create a `KMeans` object, you can use many methods and attributes to fit, predict and evaluate your model.\n",
    "\n",
    "##### Mini-Task\n",
    "- Execute the *kMeans* algorithm based on the above information\n",
    "    - Set the parameter k to 2 (we know our data has 2 clusters)\n",
    "    - Ensure the algorithms works like explained on the above illustration, that is by starting with selecting *k* centroids **at random**\n",
    "- Compare the centroids estimated by *kMeans* with the true_centroids determined above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Ngg58FxZNnSJ"
   },
   "outputs": [],
   "source": [
    "# To be completed in the tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2pwM6iieG269",
    "outputId": "0a4e4424-edaf-47f0-e4f5-8aab069cc480"
   },
   "outputs": [],
   "source": [
    "# Here are a few more examples of useful properties that you can query from the fitted kMans object.\n",
    "# Note this code will not work unless you solved the previous tasks\n",
    "\n",
    "print(f'Number of kMeans iterations: {km.n_iter_}') # the number of iterations required for stability in this solution\n",
    "print(f'Value of the objective function: {km.inertia_:.2f}') # a value similar to total distance for the clusters\n",
    "print('Estimated labels for the first 10 observations: ', km.labels_[:10])  \n",
    "\n",
    "# note that km.labels_ and estimated_clusters are equivalent\n",
    "if np.all(km.labels_ == estimated_clusters):\n",
    "    print('We just proved that km.labels_ == estimated_clusters')\n",
    "else:\n",
    "    print('Ups, something went wrong. The condition m.labels_ == estimated_clusters should evaluate to true.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How good is the cluster assignment?\n",
    "After running *kMeans*, you can access the cluster assignment for every data point through the property `km.labels_`. \n",
    "Assuming you solved the previous mini-tasks and have created a trained kMeans clustering object, the following code should show you the cluster assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.labels_  # access cluster assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be tempting to do a quick comparison of estimated clusters to true clusters, which we know for our synthetic data. Here is an example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aRxdZ0J0MCtx",
    "outputId": "6bda4246-d688-4a7b-8975-1e97d5e48bec"
   },
   "outputs": [],
   "source": [
    "y == km.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this easy comparison is not likely to work. We discuss in the tutorial why it does not work and how we could fix the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoMdrnT5rnKP"
   },
   "source": [
    "### Elbow method\n",
    "The above part has introduced all concepts we need to apply the elbow method. Recall that the elbow method is a heuristic to determine the number of clusters, which, in reality, we would not know.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/kmeans_elbow_method.png\" width=\"1200\" height=\"800\" alt=\"kMeans Algorithm\">\n",
    "\n",
    "To apply this method, we can make use of the `.inertia_` attribute of a fitted `KMeans` object. \n",
    "\n",
    "#### Mini-Task\n",
    "- Create a list that stores candidate values for the meta-parameter *k* (i.e., number of clusters)\n",
    "    - Considers settings $k=1, 2, ..., 5$\n",
    "- Iterate over your list and run *kMeans* for each candidate setting of *k*. \n",
    "- Store, from each run of *kMeans, the value of the objective function, that is the property `km.inertia_`, where `km` stands for your fitted `KMeans` object\n",
    "- Create a line plot of *kMeans*´objective values against the corresponding number of cluster (i.e., associated candidate settings of *k*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4PR63XZXq3jN",
    "outputId": "0eef857a-346b-40df-98e9-74c128c50990"
   },
   "outputs": [],
   "source": [
    "# To be completed in the tutorial\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kH_U6SEJMCt3"
   },
   "source": [
    "We can see the sharpest kink at K=2. This is not surprising. We set up our synthetic data generation such that the data points originate from two distinct clusters. \n",
    "\n",
    "### Further studies\n",
    "To deepen your understanding of this demo, you could add a coding cell and copy key parts of the demo to this cell, including the generation of synthetic data, its visualization in a scatter plot, the execution of kMeans across different settings of *k* and the elbow curve. This would allow you to easily rerun all steps with different data sets, and to investigate changes in the data affect *kMeans*, and how such effects can be mitigated by adjusting meta-parameters of the *kMeans* algorithm.\n",
    "\n",
    "Another idea would be to examine how the *kMeans* algorithm itself can be implemented. While `sklean`provides a powerful, ready-to-use implementation, the *kMeans* algorithm is quite easy to understand and implement from scratch. Recall that the lecture slides provide a pseudo-code description on which you could draw. Implementing an algorithm yourself is highly useful to fully understand how it works. You can also find many from-scratch-implementation on the internet. We also provide one in our [GitHub repo](https://github.com/Humboldt-WI/bads/blob/master/algorithms_from_scratch/kmeans.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkxJUyq4hMWG"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "*kMeans* is just one out of many powerful clustering algorithms. However, each cluster method performs descriptive analytics in that it groups data points into homogeneous subgroups. Hence, the output that you can expect from any cluster method is pretty much the same that we illustrated here for the case of *kMeans*. \n",
    "\n",
    "Here is a quick overview of useful `sklearn` functions for synthetic data creation and `KMeans`:\n",
    "\n",
    "| Goal | `sklearn` Implementation |\n",
    "| --- | --- |\n",
    "| Generate distinct groups of data | `make_blobs(n_samples=n, centers=k, cluster_std=std)` |\n",
    "| Generate kMeans | `KMeans(n_clusters=k)` |\n",
    "| Fit and predict model on new data | `model_name.fit_predict(X)` |\n",
    "| Predict model on new data | `model_name.predict(X)` |\n",
    "| Get centroid coordinates | `model_name.cluster_centers` |\n",
    "| Total distance in model | `model_name.inertia_` |\n",
    "| Number of iterations to achieve stability | `model_name.n_iter_` |\n",
    "| Labels for training data| `model_name.labels_` |\n",
    "\n",
    "Check the documentation for each function to see all the potential additional options which can be very useful for your specific needs."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Bdho4OZir6O7",
    "PdBezwLheDGy",
    "LBLlvv2AIfB1",
    "xoMdrnT5rnKP"
   ],
   "name": "2_nb_descriptive_analytics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "bads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
