{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iGz0qnmS-UK"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorial_notebooks/5_data_prep_tasks.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHScrtl_S-UK"
   },
   "source": [
    "# Tutorial 5 - Pandas library for data preparation\n",
    "The notebook revisits our lecture on EDA and data preparation. In this scope, you will further deepen your understanding of **Pandas**, the goto library for working with tabular data in Python. We will exemplify two core Pandas classes *data series* and *data frame*. To that end, the demo notebook introduces a real-world data associated with credit scoring. \n",
    "\n",
    "Here is the outline for today:\n",
    "- The HMEQ data set\n",
    "- The Pandas library\n",
    "- Explanatory data analysis\n",
    "- Data preparation\n",
    "\n",
    "Before moving on, let's import some of our standard library so that we have them ready when we need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33ZdGrYCS-UK"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# This code will suppress warning message  \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BKS1jVVS-UK"
   },
   "source": [
    "## The HMEQ data set\n",
    "Our data set, called the  \"Home Equity\" or, in brief, HMEQ data set, is provided by [Credit Risk Analytics.Net](http://www.creditriskanalytics.net). It comprises  information about a set of borrowers, which are categorized along demographic variables and variables concerning their business relationship with the lender. A binary target variable called 'BAD' is  provided and indicates whether a borrower has repaid her/his debt. You can think of the data as a standard use case of binary classification. Specifically, the data set consists of 5,960 observations and 13 features including a binary target variable. The variables are defined as follows:\n",
    "\n",
    "- BAD: the target variable, 1=default; 0=non-default \n",
    "- LOAN: amount of the loan request\n",
    "- MORTDUE: amount due on an existing mortgage\n",
    "- VALUE: value of current property\n",
    "- REASON: DebtCon=debt consolidation; HomeImp=home improvement\n",
    "- JOB: occupational categories\n",
    "- YOJ: years at present job\n",
    "- DEROG: number of major derogatory reports\n",
    "- DELINQ: number of delinquent credit lines\n",
    "- CLAGE: age of oldest credit line in months\n",
    "- NINQ: number of recent credit inquiries\n",
    "- CLNO: number of credit lines\n",
    "- DEBTINC: debt-to-income ratio\n",
    "\n",
    "The features describe the financial situation of a borrower. We will keep using the data set for many tutorials and tasks. So it makes sense to familiarize yourself with the features. Make sure you understand what type of information they provide and what this information might reveal about the risk of defaulting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFgoPwxcS-UL"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dhX-K3MS-UL"
   },
   "source": [
    "# Foundations of the Pandas Library\n",
    "Pandas is a key library within in the Python ecosystem. Given time restrictions, our tutorial will not provide a comprehensive introduction. Below, we provide a few useful demos for self-study. The core of this tutorial session focuses on use cases concerning explanatory data analysis and data preparation. \n",
    "\n",
    "## Loading data from the WWW\n",
    "The `Pandas` library supports various ways to load data from, e.g., your hard disk, a server in your network, the internet, and so on. Here, we consider the easiest setting, which is loading data from the web. All we need for this is an URL. The following code loads the data directly from the [BADS repository](https://github.com/Humboldt-WI/bads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CFQPyjiS-UN",
    "outputId": "17a8c028-8539-4a22-9c1c-359146d2761a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd  # import library\n",
    "\n",
    "# Load the data directly from GitHub\n",
    "data_url = 'https://raw.githubusercontent.com/Humboldt-WI/bads/master/data/hmeq.csv'\n",
    "df = pd.read_csv(data_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eyeballing data \n",
    "The Pandas data frame provides a ton of useful functions for data handling. We begin with showcasing some standard functions that one needs every time when working with data. \n",
    "\n",
    "The first step is typically to preview the data. The `head` function displays the first rows of the data frame. By default, it shows five rows. You can also specify how many rows you want to see. Similar functions are `tail`, which shows the last rows of the data frame.\n",
    "\n",
    "```python\n",
    "# Get a preview of the data\n",
    "df.head(10)  # print ten rows \n",
    "# Alternatively \n",
    "df.tail(10)  # print last ten rows\n",
    "# You can also simply print the entire data frame\n",
    "print(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another key property, for Pandas data frames and other data structures, is their shape. The shape indicates how many rows and columns the data frame has. This is useful to check whether the data has been loaded correctly. Recall that, in a machine learning, we refer to these as *observations* (rows) and *features* (columns).      \n",
    "\n",
    "```python   \n",
    "# Get the shape of the data frame\n",
    "df.shape  # returns (5960, 14)\n",
    "```\n",
    "Below, we illustrate some ways to use the `.shape` attribute. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PH9g48kDS-UN",
    "outputId": "4d778539-f9ee-4e39-9bbe-22063bc53025"
   },
   "outputs": [],
   "source": [
    "# Query some properties of the data\n",
    "print(f'Dimensionality of the data is {df.shape}')  # .shape returns a tupel\n",
    "print(f'The data set has {df.shape[0]} observations and {df.shape[1]} features')     # .shape returns a tupel, which we can index\n",
    "print(f'The total number of elements is {df.size}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.info()` function provides a more technical summary of the data frame. It shows the number of non-null entries per column, the data type of each column, and the memory usage of the data frame. This is particularly useful to get a quick overview of the data structure and to identify any missing values.\n",
    "\n",
    "```python  \n",
    "# Get a technical summary of the data frame\n",
    "df.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DdYdSXezS-UN",
    "outputId": "964fceae-a2a3-4a05-f5fb-5a4cf04755f1"
   },
   "outputs": [],
   "source": [
    "# Try out the .info() method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnNVr6oXS-UO"
   },
   "source": [
    "Finally, eyeballing the data in a machine learning context involves examining key descriptive statistics. To compute these, we can use the `.describe()` method. By default, it computes statistics for all numerical features. If you want to include categorical features, you can set the `include` parameter to `'all'`.\n",
    "\n",
    "```python   \n",
    "# Get descriptive statistics\n",
    "df.describe()  # for numerical features\n",
    "# Alternatively, include all features\n",
    "df.describe(include='all')  # for all features\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3_MZA2rS-UO",
    "outputId": "1a0b20f0-3173-427c-a9bb-4cf55500223e"
   },
   "outputs": [],
   "source": [
    "# Produce summary statistics (to R-programmers: this is equivalent to the famous R function summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating data (self-study)\n",
    "We discussed indexing and slicing in the contexts of Python `lists` and other containers like dictionaries. In `Pandas`, `Numpy`, and other libraries, indexing/slicing are equally important and work in similar ways. Here, we provide a few more demos on common ways to use indexing in `Pandas`. A web search for \"pandas data frame indexing\" will provide many additional insights if you are interested. Likewise, feel free to skip this part if you already feel comfortable with data frame indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic indexing of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing a single column by name\n",
    "df['BAD']\n",
    "# Alternatively, you can access a single column using dot-notation\n",
    "df.BAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the *R* programmers: we can index our data in a way similar to *R*. Note the use of `loc[]`. This is a special type of syntax you need to memorize. Also note that we specify the columns we want to index using a `list`. Hence the inner box bracket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-style indexing of selected rows and columns\n",
    "df.loc[0:4, [\"BAD\", \"LOAN\"]]  # select row 0, 1, 2, 3 and for those rows only the columns BAD and LOAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access columns by a numerical index using .iloc\n",
    "df.iloc[0:4, 0]\n",
    "df.iloc[0:4, [0, 3, 5]]\n",
    "df.iloc[0:4, np.arange(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few cautionary notes on numerical indexing in Python. The function `iloc()` considers the index of the data frame. In the above output, this is the left-most column without header. We have not defined a custom row index and Python uses consecutive integer numbers by default. However, a data frame could also have a custom index. In such a case, calls to `iloc()` need to refer to the custom index. It is good practice to eyeball a data frame and verify the way in which rows are indexed prior to using `iloc()`.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other common forms of indexing and subset selection\n",
    "It is also common practice to select rows based on comparisons of feature values using. You can achieve this using `.loc`. Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.BAD == 1, :]  # Get all observations with target variable BAD = 1. The : means you want to retrieve all columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"NINQ\"]>12, [\"LOAN\", \"VALUE\", \"NINQ\"]]  # Another example where we select only a subset of the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with high-dimensional data sets, you will often perform certain actions only with columns or a specific data type. To that end, you should know the function `select_dtypes`. This function works with data types as they are defined in Pandas. Here are a few examples, which make use of the function arguments `include` and `exclude` to specify which columns we want to access:\n",
    "```python\n",
    "# Select all numerical features \n",
    "num_df = df.select_dtypes(include=['number'])\n",
    "# Select all categorical features\n",
    "cat_df = df.select_dtypes(include=['object'])\n",
    "# Select all features that are not numerical\n",
    "non_num_df = df.select_dtypes(exclude=['number'])\n",
    "# Select all numerical and boolean features\n",
    "num_bool_df = df.select_dtypes(include=['number', 'bool'])\n",
    "``` \n",
    "\n",
    "Recall that the function `.info()` provides information about the data types of each feature. This is useful to identify the correct data types to be used with `select_dtypes`.\n",
    "\n",
    "It is also worth mentioning that `.select_dtypes` returns a data frame. Hence, you can further process the returned data frame using all standard data frame functions. However, be careful when making changes to the returned data frame. These changes will not be reflected in the original data frame. If you want to make changes and in several other use cases, it is convenient to not create a copy of selected columns of a data frame, but to index the original data frame directly. To achieve this, you need to combine `.select_dtypes` with indexing. Here is an example:\n",
    "```python   \n",
    "# Select all numerical features from the original data frame and return an index\n",
    "ix_num_df = df.select_dtypes(include=['number']).columns\n",
    "# Use the index to access the original data frame\n",
    "num_df_original = df[ix_num_df]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out the above examples to create different views of the data frame. For example, try to access only the columns that store whole numbers (i.e., integer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9KB26FrS-UR"
   },
   "source": [
    "## Manipulating data (self-study)\n",
    "We often have to manipulate data. For example, imputing missing values as part of data preparation (see later) will require us to change the data stored in a data frame. `Pandas` supports many ways to manipulate data. Let's introduce a few common options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5EaoWGmS-UR"
   },
   "source": [
    "### Using in-build Pandas functions\n",
    "Many functions that `Pandas`provide result in data changes. One example is the `sort_values` function, which we demonstrate below. By default, functions like `sort_values` **do not alter the data** in a `DataFrame` directly. Instead, they return a new `DataFrame` in which the data was changed, just as the `.select_dtypes` function discussed above. Here is an example. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df.sort_values(by=\"LOAN\", ascending=False)  # We can specify the column by which to sort and the order; next to other arguments\n",
    "df_sorted.head(10)  # Print a preview of the data; like above when introducing the method .head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlQ9W9kxS-US"
   },
   "source": [
    "Note the row index (left-most column). The index tells us that the order of the rows is different. That was to be expected because we sorted the the feature *LOAN*. In the original data, which we store in the variable `df`, we still have the original row order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of the demo was to show that, by default, `Pandas` function will not alter the `DataFrame` directly. Therefore, you see many codes of this form: \n",
    "```\n",
    "new_data_frame = old_data_frame.someFunction()\n",
    "``` \n",
    "Occasionally, you can overwrite this default behavior. Some `Pandas` functions provide an argument `inplace`. Setting this argument to `True` would then alter a `Data Frame` directly. To try this out, you can run the following code: \n",
    "```python\n",
    "# Create a copy of the original data frame to preserve it\n",
    "df_copy = df.copy()\n",
    "# Print the first rows of the copied data frame\n",
    "print(df_copy.head())  # This is only to show the original row order\n",
    "# Sort the copied data frame in place\n",
    "df_copy.sort_values(by='VALUE', inplace=True)\n",
    "# Print the first rows of the sorted data frame again and verify the order has changed\n",
    "print(df_copy.head())  # The order should now be different\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with the inplace argument. We recommend you first create a copy of the original data frame to avoid changing it. \n",
    "df_copy = df.copy()\n",
    "# Now play with df_copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NW4D58fUS-UT"
   },
   "source": [
    "### The apply function\n",
    "If you have used R, you will know the `apply()` function. It kinda does what the name suggests. It let's you define a function and apply that function to every element in a data frame. Combine that with indexing and you obtain a powerful way to selectively alter your data. \n",
    "<br>\n",
    "We provide some demos in the following, where, for simplicity, we consider only the numerical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlC_-KNbS-UT"
   },
   "outputs": [],
   "source": [
    "df_numerical = df.select_dtypes(float) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-itB_qwS-UT"
   },
   "source": [
    "Silly example: say you want to square the values of all your features. You can achieve this by calling `.apply()` on a `DataFrame` providing a suitable function as argument. In this - silly - example, we can use the in-built `Numpy` function `square`. However, we could also use a custom function, or define the function directly within the call to `.apply`. The latter is a more advanced Python concept known as *lambda function*. Web search for it if interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AiQxmyzlS-UT",
    "outputId": "d93ce20b-14d3-4b37-fad4-512fb65aae02"
   },
   "outputs": [],
   "source": [
    "# All three examples below are equivalent\n",
    "\n",
    "# Using apply together with an existing function\n",
    "df_squared1 = df_numerical.apply(np.square)  # note that the reference the function. Thus it is np.square and not np.square(). When adding brackets, we call the function. \n",
    "\n",
    "# Using apply together with a custom function\n",
    "def my_square(x):\n",
    "    return x*x\n",
    "\n",
    "df_squared2 = df_numerical.apply(my_square)\n",
    "\n",
    "# Using apply together with a lamda function\n",
    "df_squared3 = df_numerical.apply(lambda x: x * x) # you can define a function directly like here, we have a square function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-task: write some code to verify all three data frames are identical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFnWNqP1S-UU"
   },
   "source": [
    "So this was apply in action. Writing your own custom function and then feeding every column of a data frame or a selection thereof - by indexing - let you perform some powerful operations. We will see more meaningful use cases as we go along (spoiler alert: we use `apply()` for outlier handling below) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Altering data types\n",
    "We can also manipulate the way in which Pandas stores the data. This is a technical change, which does not matter for this small dataset. However, when working with more realistic data, efficient storage can safe a lot of memory on your computer and speed up computations. Therefore, we conclude the self-study part with a demo of some useful ways to alter data types in a data frame. First, let's recall how our data is stored at the moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features *JOB* and *REASON* are stored as data type `object`. This is the most general data type in Python. A variable of this type can store any piece of data, numbers, text, dates, times, ... This generality has a price. First, storing data as data type `object` consumes a lot of memory. Second, we cannot access specific functionality that is available for a specific data type only. Functions to manipulate text are an example. These are available for data of type `string` but not for data of type `object`. \n",
    "<br>\n",
    "In our case, the two features that Pandas stores as objects are actually categorical variables. We can easily verify this using, e.g., functions like `.value_counts()`.\n",
    "```python   \n",
    "# Check the unique values in the JOB feature\n",
    "df['JOB'].value_counts()\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try calling the .value_counts() function on the feature REASON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing our two \"object features\" are categories, we should alter their data type accordingly. To that end, we make use of the function `.astype()`, which facilitates converting one data type into another. Note that we cannot alter the data type of a feature directly. Instead, we need to re-assign the converted feature back to the data frame as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code categories properly \n",
    "df['REASON'] = df['REASON'].astype('category')\n",
    "df['JOB'] = df['JOB'].astype('category')\n",
    "df.info()  # verify the conversion was successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it does not really matter for this tiny data set, note that the conversion from object to category has reduced the amount of memory that the data frame consumes. On my machine, we need 524.2 KB after the translation, whereas we needed more than 600 KB for the original data frame. If you work with millions of observations the above conversion will result in a significant reduction of memory consumption. If memory consumption is an issue, we could a significant further reduction by reducing the precision of the numerical variables. Downcasting from float64 to float32 bit might is likely ok for predictive modeling. Also, the target variable is stored as an integer but we know that it has only two states. Thus, we can convert the target to a boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target variable has only two states so that we can store it as a boolean\n",
    "df['BAD'] = df['BAD'].astype('bool')\n",
    "\n",
    "# For simplicity, we also convert LOAN to a float so that all numeric variables are of type float\n",
    "df['LOAN'] = df['LOAN'].astype(np.float64)\n",
    "\n",
    "# Last, let's change all numeric variables from float64 to float32 to reduce memory consumption further\n",
    "# Note the combination of .select_dtypes and indexing, already discussed above\n",
    "num_vars = df.select_dtypes(include=np.float64).columns\n",
    "df[num_vars] = df[num_vars].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invest some time to understand the above codes. Our examples start to combine multiple pieces of functionality. For example, the above demo uses indexing, functions, and function arguments to perform tasks. Keep practicing and you will become familiar with the syntax.\n",
    "<br>\n",
    "Finally, let's verify our changes once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory consumption after the conversions\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, our type conversions reduced memory consumption by more than a half. You might want to bear this potential in mind when using your computer to process larger data sets. Should you be interested in some more information on memory efficiency, have a look at this post at [TowardDataScience.com](https://towardsdatascience.com/pandas-save-memory-with-these-simple-tricks-943841f8c32). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanatory data analysis\n",
    "Now that we are familiar with *Pandas* fundamentals, we can proceed with explanatory data analysis (EDA). EDA involves descriptive statistics and various visualizations to depict the distribution of individual features, associations between features, and so on. Pandas provides several functions for such use cases or provides interfaces to other libraries, such as *Numpy* or libraries for plotting such as *Matplotlib* and *Seaborn*. We illustrate the corresponding functionality by going through a typical EDA pipeline, which will also help us to better understand the HMEQ dataset. More specifically, we will cover the following steps:\n",
    "- Univariate analysis of categorical features\n",
    "- Univariate analysis of numerical features\n",
    "- Multivariate analysis of numerical features\n",
    "- Multivariate analysis of categorical features\n",
    "- Interactions between numerical and categorical features    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate analysis of categorical variables\n",
    "In this part, we will examine  our target variable 'BAD', as well as the two categorical variables 'REASON' and 'JOB' individually. Firstly, we will count how many observations belong to each category of a variable using the function `.value_counts()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"BAD\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also count the category levels in the other categorical variables REASON and JOB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Visualizations are often easier to interpret than tables. For categories, count plots and stacked count plots are common EDA vehicles. To create a count plot in Seaborn, we can use the `countplot()` function as follows:\n",
    "\n",
    "```python\n",
    "sns.countplot(x='BAD', data=df)\n",
    "```\n",
    "Try this out by creating a count plot for the target variable 'BAD'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of target variable BAD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-use the `.select_dtype()` function from above to create a count plot for each categorical variable in the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df.select_dtypes(include=[\"category\"])  # select the features to plot based on their data type\n",
    "for feature in categories.columns:\n",
    "    sns.countplot(x=feature, data=df)  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate analysis of numeric variables\n",
    "Let us now take a closer look at the numeric variables and their distribution by means of histograms. Creating a histogram is easily achieved using the `.hist()` function, which Pandas offers. Try it out by calling the function on your entire data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create one histogram for each numeric variable and illustrate how to set the number of bins\n",
    "df.select_dtypes(include=np.number).hist(bins=20, figsize=(12,8))\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice feature of `.hist()` is that it creates a histogram for each numeric variable in the data frame in one go. This is particularly useful when working with high-dimensional data sets. However, the function offers only limited options to customize the plots. Therefore, we recommend using dedicated plotting libraries such as `Seaborn` or `Matplotlib` when you want to examine a single variable at a time. For example, to create a histogram of the feature 'LOAN' using the `Seaborn` library, you can use: \n",
    "```python\n",
    "    sns.histplot(data=df, x='LOAN')\n",
    "    plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogramms for a few more features. \n",
    "# Explore the arguments that the histplot function offers to customize the plots\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another crucial plot in data science is the boxplot. Creating a boxplot using Pandas is very easy:\n",
    "```python\n",
    "    df.boxplot(column='LOAN')\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "Alternatively, we could again use *Seaborn* to create the boxplot:\n",
    "```python\n",
    "    sns.boxplot(x='LOAN', data=df)\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "Decide for yourself which version you prefer. Both are fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try creating a boxplot for the feature 'VALUE' using either Pandas or Seaborn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we provide a more sophisticated demo in which we create histograms for all numerical features in the data frame using a loop. Note how we again use `.select_dtypes()` to identify the numerical features. Also, we introduce the `.subplots()` function from Matplotlib to create a grid of plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns\n",
    "num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Determine the size of the grid of plots (e.g., 3 columns)\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(num_cols) / n_cols))\n",
    "\n",
    "# subplots function in action: here we define the grid\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "\n",
    "# Flatten axes for easy indexing of individual plots\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, col in zip(axes, num_cols):\n",
    "    sns.boxplot(data=df, y=col, ax=ax)\n",
    "    ax.set_title(col)\n",
    "\n",
    "# Turn off unused axes if any\n",
    "for ax in axes[len(num_cols):]:\n",
    "    ax.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate analysis of numeric variables\n",
    "\n",
    "After gaining more knowledge about the variables individually, it is important to examine their relationships more closely. In data science, this is a good way of identifying redundant information as well as variable interactions. \n",
    "Next, we will plot a heatmap. It shows the correlation for all numeric variables. Highly correlated variables are redundant as they convey the same pieces of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr= df.select_dtypes(include=np.number).corr()\n",
    "sns.heatmap(corr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate analysis of categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categories and the binary target in particular are also useful to examine sub-groups. For example, we could calculate the mean of a/all numeric variables for good and bad borrowers. Enter `.groupby()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"BAD\").mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another standard operation when exploring categorical variables is to check cross-tabulations. Considering, for example, the variables `Reason`and `Job`, we can create a cross-tab as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.REASON, df.JOB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cross-tab can be particularly informative when it includes the target variable. This way, we can spot whether certain category levels of the other (independent) variable are especially prominent with good or bad borrowers. In this use case, we would also want to switch from showing counts (as above) to showing relative frequencies. We achieve this by augmenting our call to `crosstab()` with the argument `normalize='index'`. Have a look into the [documentation of the function](https://pandas.pydata.org/docs/reference/api/pandas.crosstab.html) to understand why we select the option `'index'` for the function argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = pd.crosstab(df.JOB, df.BAD, normalize='index')\n",
    "job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can also report the same information in a graphical way. A common way to display categorical variables is the stacked count plot. Let us analyze the variables `REASON` and `JOB` and how they are linked to the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason = df.groupby(['BAD', 'REASON'], observed=True).size().reset_index().pivot(columns='BAD', index='REASON', values=0)\n",
    "                                                                       \n",
    "reason.plot(kind='bar', stacked=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactions between numeric and categorical variables \n",
    "Next, we can have a look at the distribution of our categories across the numerical variables. Violin plots are a great way to do so. The *seaborn* library makes creating these plots very easy. Below, we illustrate two options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the category 'REASON' and create one plot for each numeric variable\n",
    "for col  in df.select_dtypes(include=np.number).columns:\n",
    "    plt.figure()\n",
    "    sns.violinplot(x='REASON', y=col, hue='BAD',\n",
    "                   split=True, inner=\"quart\",\n",
    "                   data= df)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat this for the variable 'JOB' but using a different approach. Again, the insighty that you can derive from the visualizations should be similar. Simply decide which version you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for category JOB\n",
    "fig, axs = plt.subplots(3,3, figsize=(15, 10))\n",
    "plt.tight_layout(pad=0.5, w_pad=4, h_pad=1.0)  \n",
    "x = df.JOB\n",
    "\n",
    "sns.violinplot(x=x, y=\"LOAN\",  data=df,ax=axs[0,0])\n",
    "sns.violinplot(x=x, y=\"MORTDUE\", data=df,ax=axs[0,1])\n",
    "sns.violinplot(x=x, y=\"VALUE\", data=df,ax=axs[0,2])\n",
    "sns.violinplot(x=x, y=\"YOJ\", data=df,ax=axs[1,0])\n",
    "sns.violinplot(x=x, y=\"DEROG\", data=df,ax=axs[1,1])\n",
    "sns.violinplot(x=x, y=\"CLAGE\", data=df,ax=axs[1,2])\n",
    "sns.violinplot(x=x, y=\"NINQ\", data=df,ax=axs[2,0])\n",
    "sns.violinplot(x=x, y=\"CLNO\", data=df,ax=axs[2,1])\n",
    "sns.violinplot(x=x, y=\"DEBTINC\", data=df,ax=axs[2,2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7uvFSUsS-UU"
   },
   "source": [
    "# Data preparation\n",
    "Data preparation is a mega-topic. It will accompany us throughout the whole course. I this part, we focus on some typical issues in our data and demonstrate how to perform standard data prep tasks using `Pandas`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ypE7GuOS-UW"
   },
   "source": [
    "## Missing values\n",
    "You might have already noticed that our data contains many missing values. This is common when working with real data. Likewise, handling missing values is a standard task in data preparation. `Pandas` provides the function `.isna()` as entry point to the corresponding functionality and helps with identifying the relevant cases.\n",
    "\n",
    "*Note*: `Pandas` also supports an equivalent function called `.isnull()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeahGMfES-UW",
    "outputId": "9141fa4f-01b3-4f3f-fa63-5e744bcb1c5f"
   },
   "outputs": [],
   "source": [
    "# Boolean mask of same size as the data frame to access missing values via indexing\n",
    "missing_mask = df.isna()\n",
    "\n",
    "print(f'Dimension of the mask: {missing_mask.shape}')\n",
    "print(f'Dimension of the data frame: {df.shape}')\n",
    "\n",
    "missing_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aBde7TBS-UW"
   },
   "source": [
    "We can now count the number of missing values per row or per column or in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_DgQ7QHS-UW",
    "outputId": "b931d0ae-fece-4d25-d8d9-0a352b4da582"
   },
   "outputs": [],
   "source": [
    "# missing values per row\n",
    "miss_per_row = missing_mask.sum(axis=1)\n",
    "print('Missing values per row:\\n', miss_per_row)\n",
    "\n",
    "# missing values per column\n",
    "miss_per_col = missing_mask.sum(axis=0)\n",
    "print('Missing values per column:\\n', miss_per_col )\n",
    "\n",
    "# count the total number of missing values\n",
    "n_total_missing = missing_mask.sum().sum()\n",
    "print(f'Total number of missing values: {n_total_missing}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_9jyQY_S-UW"
   },
   "source": [
    "It can be useful to visualize the *missingness* in a data set by means of a heatmap. Note how the below example gives you a good intuition of how and where the data set is affected by missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggtNOCJKS-UW",
    "outputId": "c66c163d-4ae4-46ec-978a-49cca891d75d"
   },
   "outputs": [],
   "source": [
    "sns.heatmap(df.isna())  # quick visualization of the missing values in our data set\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vnw8ySRgS-UW"
   },
   "source": [
    "### Categorical features\n",
    "Let's start with the two categorical features. The heatmap suggests that `REASON` exhibits more missing values than `JOB`. We will treat them differently for the sake of illustration. Now that we start altering our data frame more seriously, it is a good idea to make a copy of the data so that we can easily go back to a previous state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuSqLNgzS-UW"
   },
   "outputs": [],
   "source": [
    "# copy data\n",
    "df_orig = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7nqgRZuS-UX"
   },
   "source": [
    "#### Adding a new category level\n",
    "One way to treat missing values in a categorical feature is to introduce a new category level \"IsMissing\". We will demonstrate this approach for the feature *REASON*. \n",
    "<br>One feature of the category data type in Pandas is that category levels are managed. We cannot add levels directly. Thus, before assigning the missing values our new category level *IsMissing*, we first need to introduce this level. We basically tell our data frame that *IsMissing* is another suitable entry for *REASON* next to the levels that already exist in the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdBWbWtfS-UX",
    "outputId": "b0338de2-ff64-4ff7-d922-1142f7f92879"
   },
   "outputs": [],
   "source": [
    "# Variable REASON: we treat missing values as a new category level.\n",
    "# First we need to add a new level\n",
    "df.REASON = df.REASON.cat.add_categories(['IsMissing'])\n",
    "\n",
    "# Now we can do the replacement\n",
    "df.REASON[df.REASON.isnull() ] = \"IsMissing\"\n",
    "df.REASON.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXAjFQFtS-UX",
    "outputId": "b5d3bf61-c04e-4d4f-a5f0-d345f8d3b184"
   },
   "outputs": [],
   "source": [
    "df.REASON.isna().sum()  # verify that no more missing values exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tf_LOVmPS-UX"
   },
   "source": [
    "#### Mode replacement\n",
    "For the feature *JOB*, which is multinomial, we replace missing values with the mode. Please note that this is a crude way to handle missing values. I'm not endorsing it! But you should have at least seen a demo. Here it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTNUN63yS-UX",
    "outputId": "a5ecc720-55f7-48b2-967a-27b387f50ad1"
   },
   "outputs": [],
   "source": [
    "# Determine the mode\n",
    "mode_of_job = df.JOB.mode()\n",
    "print(mode_of_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4M32XJxQS-UY",
    "outputId": "a6d86590-c2d8-4716-adcf-9fa8e258b3ac"
   },
   "outputs": [],
   "source": [
    "# replace missing values with the mode\n",
    "df.JOB[df.JOB.isnull() ] = df.JOB.mode()[0]  # the index [0] is necessary as the result of calling mode() is a Pandas Series\n",
    "# verify that no more missing values exist\n",
    "df.JOB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4SvhbFnS-UY",
    "outputId": "ecb4b0ef-ca74-4890-cc24-0b46e0738747"
   },
   "outputs": [],
   "source": [
    "# Verify more seriously that missing value replacement was successful\n",
    "if df.REASON.isnull().any() == False and df.JOB.isnull().any() == False:\n",
    "    print('well done!')\n",
    "else:\n",
    "    print('ups')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXZaNFpGS-UZ"
   },
   "source": [
    "### Numerical features\n",
    "We have a lot of numerical features. To keep things simple, we simply replace all missing values with the median. Again, this is  a crude approach that should be applied with care; if at all. \n",
    "\n",
    "Regarding the implementation of media replacement, we could write a loop that treats every feature one-by-one:\n",
    "```python\n",
    "num_cols = df.select_dtypes(include=['number']).columns\n",
    "for col in num_cols:\n",
    "    median = df[col].median()\n",
    "    df[col].fillna(median, inplace=True)\n",
    "```\n",
    "However, Pandas provides a more elegant way to achieve the same result without writing an explicit loop:\n",
    "```python\n",
    "df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "``` \n",
    "\n",
    "A more *data science* way to impute missing values is to the `SimpleImputer`class from *sklearn* \n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[num_cols] = imputer.fit_transform(df[num_cols])\n",
    "```\n",
    "\n",
    "Try out any of the above options to impute missing values in the numerical features. Then run the following code to verify that all missing values have been handled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in numerical columns using median replacement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify there are no more missing values in the data\n",
    "n_total_missing = df.isna().sum().sum()\n",
    "if  n_total_missing == 0:\n",
    "    print('Well done, no more missing values!')\n",
    "else:\n",
    "    print(f'Ups! There are still {n_total_missing} missing values.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "Our analysis of the features' distributions indicated that at least some of the feature exhibit outliers. As discussed in class, it can be useful to truncate outliers. There are multiple ways to do so. One common approach involves computing a *fence* of plausible values and declaring values outside this *fence* as outliers. This is the essence of Tuky's rule, defining upper and lower fences as 1.5 (or 3) times the inter-quartile range (IQR) above the 75th percentile and below the 25th percentile, respectively. Values outside these fences are considered outliers. Let us first illustrate this idea for one feature `LOAN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine distribution one more time\n",
    "feature = \"LOAN\"\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "sns.histplot(data=df, x=feature, ax=axes[0])\n",
    "sns.boxplot(data=df, y=feature, ax=axes[1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute Q1, Q3, and IQR\n",
    "q1 = df[feature].quantile(0.25)\n",
    "q3 = df[feature].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "# compute boundaries\n",
    "outlier_factor = 1.5  # common thresholds are 1.5 and 3 for mild and heavy outliers, respectively\n",
    "lower_fence = q1 - outlier_factor*iqr\n",
    "upper_fence = q3 + outlier_factor*iqr\n",
    "\n",
    "# Sanitize the computed values: lower_fence must not be smaller than the minimum value and upper_fence must not be larger than the maximum value observed for the feature\n",
    "lower_fence = np.max([lower_fence, np.min(df[feature])])\n",
    "upper_fence = np.min([upper_fence, np.max(df[feature])])\n",
    "\n",
    "print(f\"Common values of feature {feature} should fall into the interval [{lower_fence}, {upper_fence}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the effect of outlier truncation\n",
    "new_feature = df[feature].clip(lower=lower_fence, upper=upper_fence)\n",
    "\n",
    "# plot the distribution again\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "sns.histplot(x=new_feature, ax=axes[0])\n",
    "sns.boxplot(y=new_feature, ax=axes[1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the boxplot before and after outlier truncation, we can see that the extreme values have been removed. This may seem good news as it makes the distribution more compact. However, a look at the histogram points to an issue with our outlier truncation approach, which now shows a peak at its new maximum. Intuitively, this would adversely affect predictive modeling as all outliers are now mapped to the same value. \n",
    "\n",
    "Lesson learnt: in practice, we should be careful when applying outlier truncation. Alternative options such as discretizing features may work better. Also, when using truncation, we should not blindly follow the rule underlying the boxplot, but carefully select the boundary values. The problem observed above for `LOAN` could be avoided by increasing our boundary of *plausible* values and truncating few values, e.g., only those outside the .99 percentile. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretization of Numeric Variables\n",
    "\n",
    "As seen in the previous demo, outlier truncation can be risky. Mapping numerical features into categories is an alternative approach to avoid adverse effects from outliers while also avoiding the risk to introduce spurious patters through outlier truncation. Unsurprisingly, the *Pandas* library provides useful functions to discretize variables. However, we will cover those in the exercise part. Below, we take a more judgmental approach, discretizing the features `DEROG` and `DELINQ` because of their distribution. Let's first recall their distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution again\n",
    "features = [\"DEROG\", \"DELINQ\"]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(8, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(features):\n",
    "    sns.histplot(data=df, x=col, ax=axes[2*i])\n",
    "    sns.boxplot(data=df, x=col, ax=axes[2*i + 1])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, `DEROG` and `DELINQ` both display a large number of zeros. Hence, we could consider one category level *isZero* and another *IsGreaterThenZero*. This would give a binary variable. We can also introduce more category levels to obtain a more fine-grained categorical representation of the original numbers. Let's examine the number of unique entries in these two features in more detail, using the `.value_counts()` functions introduced earlier in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DELINQ.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DEROG.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How you proceed from here is based on judgment. We will exemplify two different techniques that could be considered in the focal case. For one feature, we will create a dummy, indicating whether or not the value of the variable is 0. For the other feature, we will group into three categories. As \"DELINQ\" shows fewer observations for the value 0, we will use this variable to divide into three groups: 0, 1 & >1. We will add the new categorical variables to our data frame so that we can later decide which representation to use for predictive analytics. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DEROG_dummy'] = 0 #set default to 0\n",
    "df.loc[(df['DEROG'] == 0), 'DEROG_dummy'] = 1 #change to 1 if value of \"DEROG\" is 0 \n",
    "df['DEROGzero'] = df['DEROG_dummy'].astype('bool')\n",
    "df.DEROG_dummy.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DELINQ_cat'] = '1+' # set default value to +1 for new variable \n",
    "df.loc[(df['DELINQ'] == 1), 'DELINQ_cat'] = '1' # change this value to 1, if value of DELINQ is 1\n",
    "df.loc[(df['DELINQ'] == 0), 'DELINQ_cat'] = '0'\n",
    "df['DELINQ_cat'] = df['DELINQ_cat'].astype('category')  # convert to categorical\n",
    "df.DELINQ_cat.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdaOAlZTTEZT"
   },
   "source": [
    "# Summary of useful Pandas functions\n",
    "\n",
    "Many useful tricks with `Pandas` (here `df` is a pandas DataFrame and `col` is one of its columns):\n",
    "\n",
    "| Goal | Possible Code |\n",
    "| --- | --- |\n",
    "| Get df column (column name must have no spaces) | `df.col` |\n",
    "| Get df column | `df[\"col\"]` |\n",
    "| Example condition: only select rows where `col1 > 1` | `df[\"col\"] > 1` |\n",
    "| Use index names to select rows and columns | `df.loc[row_list, col_list]` |\n",
    "| Use index numbers to select rows and columns | `df.iloc[row_list, col_list]` |\n",
    "| Get df column based on a condition | `df.loc[condition, ['col2','col3',...]]`|\n",
    "| Group df by values of `col` | `df.groupby(\"col\")` |\n",
    "| Perform function on `col2` for each group of `col1` | `df.groupby(\"col1\")[\"col2\"].fun()` |\n",
    "| Find value counts of each value in `col` | `df.groupby(['col']).size()`| \n",
    "| Get column mean and ignore null values | `df[\"col\"].mean(skipna=True)` |\n",
    "| Get column mode | `df[\"col\"].mode()` |\n",
    "| Get column median | `df[\"col\"].median()` |\n",
    "| Get rows of the 95th quantile of `col` | `df[\"col\"].quantile(q=0.95)` |\n",
    "| Filter `df` with a boolean condition | `df.query(condition)` |\n",
    "| Create tally of `col2` by values of `col1` | `pd.crosstab(df['col1'], df['col2']`) |\n",
    "| Pivot rows and columns | `df.pivot(index='col1', columns='col2', values='col3')` | \n",
    "| Sort values by `col` and save `df` in this order | `df.sort_values(by='col', inplace=True)` |\n",
    "| Apply function to each column of `df` | `df.apply(fun)` |\n",
    "| Save `df` as CSV in working directory | `df.to_csv('./file_name.csv', index=False)` |\n",
    "| Count the number of times each value occurs | `df['col'].value_counts()` |\n",
    "| Change column's data type | `df['col'] = df['col'].astype('type')` |\n",
    "| Create boolean matrix of `df` where `True` indicates null value | `df.isnull()` | \n",
    "| Create boolean matrix of `df` where `True` indicates null value | `df.isna()` | \n",
    "| Create copy of df | `df_copy = df.copy()` |\n",
    "| Add new category to categorical variable | `df.col.cat.add_categories(['New C'], inplace=True)` |\n",
    "| Replace null values with `\"IsMissing\"` | `df.col[df.col.isnull()] = \"IsMissing\"` |\n",
    "| Fill missing values with median and save `df` | `df['col'].fillna(median_value, inplace=True)` |\n",
    "| Calculate time at execution (must import `time` library) | `time.time()` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dependency of loan amount and credit risk\n",
    "Examine the dependency between the loan amount (i.e., feature `LOAN`) and the default risk. You find  information on the latter in the column `df[\"BAD\"]`. A value of 1 indicates that a borrower is a defaulter (i.e., bad risk). Specifically:\n",
    "1. Calculate the average of the feature `LOAN`\n",
    "2. Calculate the average `LOAN` amount separately for bad and good risk using logical indexing. \n",
    "3. Interpret the results of your analysis. Is there a dependency between `LOAN` and default risk?\n",
    "4. Re-calculate the average `LOAN` amount for good and bad risks. This time, make use of the function `group_by`, which exists for data frames.  \n",
    "5. Extend the previous task by computing the group-wise median for all numerical features in the data frame\n",
    "\n",
    "\n",
    "**Extension:** a nice extension of subtasks 1 to 3 would be to secure your interpretation with a statistical hypothesis test. Perhaps you know a suitable test. If not, run a web search for, e.g., *statistical test for difference in means python*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solutions to the exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scaling numerical \n",
    "Another common data preparation task is scaling numerical features. The goal is to ensure that all features have the same scale. This is important for many machine learning algorithms. The lecture introduced two common scaling methods: min-max scaling and z-score scaling.\n",
    "The `sklearn` library provides implementations of both approaches in the classes `MinMaxScaler` and `StandardScaler`, which are part of the module `preprocessing`. Expericence their functionality to solving the following exercises.\n",
    "\n",
    "1. Import the class `MinMaxScaler` and `StandardScaler` from the module `preprocessing` in the library `sklearn`.\n",
    "2. Familiarize yourself with the functioning of the `StandardScaler` using its documentation and other sources (e.g., web search). \n",
    "3. Test the `StandardScaler` by applying it to the numerical features `LOAN`. Afterwards, the scaled feature should have a mean of 0 and a standard deviation of 1. Write a few lines of code to verify this.\n",
    "4. The use of the `MinMaxScaler` is similar to the `StandardScaler`. Apply the `MinMaxScaler` to all other numerical features in the data set. More specifically, \n",
    "- Create a new data frame that contains only the numerical features.\n",
    "- Remove the feature `LOAN` from that data frame; as we already scaled it in task 3.\n",
    "- Apply the `MinMaxScaler` to the new data frame.\n",
    "- Write a few lines of code to verify that the scaling was successful. To that end, recall what the 'MinMaxScaler' does.\n",
    "- Combine the scaled features with the feature `LOAN` and the categorical features in a new `DataFrame`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solutions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discretizing numerical features\n",
    "Discretizing numerical features is another common data preparation task. The goal is to convert continuous numerical features into discrete bins or categories. This can be useful for certain types of analysis and modeling. The `pandas` library provides the `cut` and `qcut` functions for this purpose.\n",
    "\n",
    "1. Familiarize yourself with the `cut` and `qcut` functions in the `pandas` library using their documentation and other sources (e.g., web search).\n",
    "2. Use the `cut` function to discretize the `LOAN` feature into 5 equal-width bins. Assign meaningful labels to each bin (e.g., 'Very Low', 'Low', 'Medium', 'High', 'Very High').\n",
    "3. Verify the binning by displaying the first few rows of the data frame and checking the `LOAN` feature.\n",
    "4. Use the `qcut` function to discretize the `MORTDUE` feature into 4 quantile-based bins. Assign meaningful labels to each bin (e.g., 'Q1', 'Q2', 'Q3', 'Q4').\n",
    "5. Verify the binning by displaying the first few rows of the data frame and checking the `MORTDUE` feature.\n",
    "6. Create a new data frame that includes the discretized `LOAN` and `MORTDUE` features along with the other original features.\n",
    "7. Write a custom function that takes a data frame and a list of numerical features as input and returns a new data frame with all specified features discretized into a given number of bins using the `cut` function. Test your function on the numerical features in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solutions "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "4_nb_data_preparation (2).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "bads310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
