{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorials/11_nb_imbalance_n_costs.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The notebook is under development and not ready for use by our students. Please ignore this notebook for the time being."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11 - Imbalanced and cost-sensitive learning\n",
    "Welcome to chapter 10 of [Business Analytics and Data Science](). In this week, we will revisit the lecture on imbalanced and cost-sensitive learning. Imbalance and asymmetric error costs occur frequently in business applications. Our well-known credit scoring exemplifies just this. Banks approve credit applications selectively and have achieved high sophistication in default prediction. Consequently, the observed default rates are often low, which implies that a credit scoring data set will typically exhibit class imbalance. Good customers represent the majority while defaulting clients represent the minority class. Using strategies from the reals of imbalanced learning, we can enhance the recognition of the minority class during classifier training.\n",
    "\n",
    "Likewise, it is a known fact that error costs are asymmetric. Approaching credit risk prediction as a binary classification problem, the two possible errors are accepting a client who will default (false positive error) and rejecting a client who would repay had we approved the credit (false negative error). Just to be sure, here we define the good payers as the *positive* class. Thus, accepting a bad risk is a false positive error. The classifier predicts a client to be a good payer (i.e., positive), and we therefore accept the client, whereas the client defaults eventually (i.e., turns out to be a negative client). So the classifier has made a false positive error. Likewise, *false negative* means that the classifier predicts a client to belong to the negative class, that is the bad risks, whereas the client is actually a good payer. In other words, the classifier predicts the negative class and this prediction turns out to be false; hence false negative. False positive errors are more costly than false negative errors because they imply an actual loss. A false negative error, on the other hand, implies an opportunity costs from not lending to - and not earning interest -  a good payer. The literature on cost-sensitive learning has developed approaches to address asymmetric error costs during the *training* and/or the *evaluation* of a classifier. \n",
    "\n",
    "Time to explore some instruments for class imbalance and cost-sensitivity. Since the corresponding literature routinely considers classification, we will also stick to this form of predictive modeling. Thus, the tutorial will not touch on imbalance and cost-asymmetry in regression. \n",
    "\n",
    "The outline of the tutorial is as follows:\n",
    "- Preliminaries\n",
    "-\n",
    "-\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "We begin with the usual preparations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data  (4172, 1) (4172, 18) (1788, 1) (1788, 18)\n"
     ]
    }
   ],
   "source": [
    "# Import standard packages. \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Load the data for this tutorial directly from GitHub\n",
    "data_url = 'https://raw.githubusercontent.com/Humboldt-WI/bads/master/data/hmeq_modeling.csv'\n",
    "\n",
    "df = pd.read_csv(data_url, index_col='index')\n",
    "\n",
    "# Extract target variable and feature matrix \n",
    "X = df.drop(['BAD'], axis=1) \n",
    "y = df[['BAD']].values\n",
    "\n",
    "# Data partitioning\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Select random state to make results reproducable\n",
    "rnd_state = 888 \n",
    "\n",
    "# Create training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = rnd_state)  # 30% of the data as hold-out\n",
    "\n",
    "# Make yourself familiar with these vectors\n",
    "print('Shape of the data ', y_train.shape, X_train.shape, y_test.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOAN</th>\n",
       "      <th>MORTDUE</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>YOJ</th>\n",
       "      <th>CLAGE</th>\n",
       "      <th>NINQ</th>\n",
       "      <th>CLNO</th>\n",
       "      <th>DEBTINC</th>\n",
       "      <th>DEROGzero</th>\n",
       "      <th>REASON_HomeImp</th>\n",
       "      <th>REASON_IsMissing</th>\n",
       "      <th>JOB_Office</th>\n",
       "      <th>JOB_Other</th>\n",
       "      <th>JOB_ProfExe</th>\n",
       "      <th>JOB_Sales</th>\n",
       "      <th>JOB_Self</th>\n",
       "      <th>DELINQcat_1</th>\n",
       "      <th>DELINQcat_1+</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.832283</td>\n",
       "      <td>-1.295882</td>\n",
       "      <td>-1.335526</td>\n",
       "      <td>0.266788</td>\n",
       "      <td>-1.075278</td>\n",
       "      <td>-0.065054</td>\n",
       "      <td>-1.297476</td>\n",
       "      <td>0.137456</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.810666</td>\n",
       "      <td>-0.013474</td>\n",
       "      <td>-0.672699</td>\n",
       "      <td>-0.236615</td>\n",
       "      <td>-0.723092</td>\n",
       "      <td>-0.826792</td>\n",
       "      <td>-0.756608</td>\n",
       "      <td>0.137456</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.789048</td>\n",
       "      <td>-1.654549</td>\n",
       "      <td>-1.839275</td>\n",
       "      <td>-0.668103</td>\n",
       "      <td>-0.368769</td>\n",
       "      <td>-0.065054</td>\n",
       "      <td>-1.189302</td>\n",
       "      <td>0.137456</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.789048</td>\n",
       "      <td>-0.159552</td>\n",
       "      <td>-0.202559</td>\n",
       "      <td>-0.236615</td>\n",
       "      <td>-0.061033</td>\n",
       "      <td>-0.065054</td>\n",
       "      <td>-0.107566</td>\n",
       "      <td>0.137456</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.767431</td>\n",
       "      <td>0.791699</td>\n",
       "      <td>0.311107</td>\n",
       "      <td>-0.811933</td>\n",
       "      <td>-1.088528</td>\n",
       "      <td>-0.826792</td>\n",
       "      <td>-0.756608</td>\n",
       "      <td>0.137456</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5955</th>\n",
       "      <td>2.545249</td>\n",
       "      <td>-0.384589</td>\n",
       "      <td>-0.181135</td>\n",
       "      <td>1.057849</td>\n",
       "      <td>0.558823</td>\n",
       "      <td>-0.826792</td>\n",
       "      <td>-0.540260</td>\n",
       "      <td>0.354834</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5956</th>\n",
       "      <td>2.545249</td>\n",
       "      <td>-0.462591</td>\n",
       "      <td>-0.119037</td>\n",
       "      <td>1.057849</td>\n",
       "      <td>0.390638</td>\n",
       "      <td>-0.826792</td>\n",
       "      <td>-0.648434</td>\n",
       "      <td>0.312440</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5957</th>\n",
       "      <td>2.545249</td>\n",
       "      <td>-0.478000</td>\n",
       "      <td>-0.119331</td>\n",
       "      <td>0.914020</td>\n",
       "      <td>0.436639</td>\n",
       "      <td>-0.826792</td>\n",
       "      <td>-0.648434</td>\n",
       "      <td>0.261479</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5958</th>\n",
       "      <td>2.545249</td>\n",
       "      <td>-0.584642</td>\n",
       "      <td>-0.143317</td>\n",
       "      <td>0.770191</td>\n",
       "      <td>0.457322</td>\n",
       "      <td>-0.826792</td>\n",
       "      <td>-0.540260</td>\n",
       "      <td>0.057266</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5959</th>\n",
       "      <td>2.545249</td>\n",
       "      <td>-0.629882</td>\n",
       "      <td>-0.209363</td>\n",
       "      <td>0.914020</td>\n",
       "      <td>0.530515</td>\n",
       "      <td>-0.826792</td>\n",
       "      <td>-0.540260</td>\n",
       "      <td>0.096008</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5960 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           LOAN   MORTDUE     VALUE       YOJ     CLAGE      NINQ      CLNO  \\\n",
       "index                                                                         \n",
       "0     -1.832283 -1.295882 -1.335526  0.266788 -1.075278 -0.065054 -1.297476   \n",
       "1     -1.810666 -0.013474 -0.672699 -0.236615 -0.723092 -0.826792 -0.756608   \n",
       "2     -1.789048 -1.654549 -1.839275 -0.668103 -0.368769 -0.065054 -1.189302   \n",
       "3     -1.789048 -0.159552 -0.202559 -0.236615 -0.061033 -0.065054 -0.107566   \n",
       "4     -1.767431  0.791699  0.311107 -0.811933 -1.088528 -0.826792 -0.756608   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "5955   2.545249 -0.384589 -0.181135  1.057849  0.558823 -0.826792 -0.540260   \n",
       "5956   2.545249 -0.462591 -0.119037  1.057849  0.390638 -0.826792 -0.648434   \n",
       "5957   2.545249 -0.478000 -0.119331  0.914020  0.436639 -0.826792 -0.648434   \n",
       "5958   2.545249 -0.584642 -0.143317  0.770191  0.457322 -0.826792 -0.540260   \n",
       "5959   2.545249 -0.629882 -0.209363  0.914020  0.530515 -0.826792 -0.540260   \n",
       "\n",
       "        DEBTINC  DEROGzero  REASON_HomeImp  REASON_IsMissing  JOB_Office  \\\n",
       "index                                                                      \n",
       "0      0.137456       True               1                 0           0   \n",
       "1      0.137456       True               1                 0           0   \n",
       "2      0.137456       True               1                 0           0   \n",
       "3      0.137456       True               0                 1           0   \n",
       "4      0.137456       True               1                 0           1   \n",
       "...         ...        ...             ...               ...         ...   \n",
       "5955   0.354834       True               0                 0           0   \n",
       "5956   0.312440       True               0                 0           0   \n",
       "5957   0.261479       True               0                 0           0   \n",
       "5958   0.057266       True               0                 0           0   \n",
       "5959   0.096008       True               0                 0           0   \n",
       "\n",
       "       JOB_Other  JOB_ProfExe  JOB_Sales  JOB_Self  DELINQcat_1  DELINQcat_1+  \n",
       "index                                                                          \n",
       "0              1            0          0         0            0             0  \n",
       "1              1            0          0         0            0             1  \n",
       "2              1            0          0         0            0             0  \n",
       "3              1            0          0         0            0             0  \n",
       "4              0            0          0         0            0             0  \n",
       "...          ...          ...        ...       ...          ...           ...  \n",
       "5955           1            0          0         0            0             0  \n",
       "5956           1            0          0         0            0             0  \n",
       "5957           1            0          0         0            0             0  \n",
       "5958           1            0          0         0            0             0  \n",
       "5959           1            0          0         0            0             0  \n",
       "\n",
       "[5960 rows x 18 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalance, accuracy, and how the two don't go together\n",
    "We have made the point in the lecture that one problem of class imbalance related to the fact that standard indicators of classification performance provide misleading signals when the data is imbalanced. Let's start by demonstrating the issue with a little experiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "1. Introduce imbalance ratio\n",
    "2. Create a plot of classification accuracy (y-axis) versus imbalance ratio (x-axis) using logistic regression and our toy data from tutorial 5 (code already below). You need to repeat the standard training/test workflow multiple times using different data samples (i.e., so that the imbalance increases)  \n",
    "3. Briefly discuss the plot. At some point, logit should become a naive classifier. This is what we want to show. If it doesn't work, you can use a tree instead of logit.\n",
    "4. Have a 2nd plot like the first one but display for each imbalance ratio the accuracy of a naive classifier that always predict the majority class.\n",
    "5. State an exercise task to repeat the analysis using different classifiers and the HMEQ data set.\n",
    "6. Showcase how AUC is robust by charting AUC versus imbalance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7O1rqLbODmyT"
   },
   "outputs": [],
   "source": [
    "def toy_data(n=1000, mu1=[1,1], mu2=[4, 4], sig1=1, sig2=1):\n",
    "    \"\"\" Customer function to generate linearly seperable toy data. The code has been discussed in more detail in Tutorial #3.\n",
    "        \n",
    "        The arguments represent, respectively, the size of the data, the mean vectors of the two Gaussians from which we\n",
    "        sample class 1 and class 2 data points, and their standard deviations.\n",
    "    \"\"\"\n",
    "    \n",
    "    class1_x1 = np.random.normal(loc=mu1[0], scale=sig1, size=n)\n",
    "    class1_x2 = np.random.normal(loc=mu1[1], scale=sig1, size=n)\n",
    "\n",
    "    class2_x1 = np.random.normal(loc=mu2[0], scale=sig2, size=n)\n",
    "    class2_x2 = np.random.normal(loc=mu2[1], scale=sig2, size=n)\n",
    "\n",
    "    y1 = np.repeat(0, n)\n",
    "    y2 = np.repeat(1, n)\n",
    "\n",
    "    class1 = np.vstack((class1_x1, class1_x2)).T\n",
    "    class2 = np.vstack((class2_x1, class2_x2)).T\n",
    "\n",
    "    X = np.vstack((class1,class2))\n",
    "    y = np.concatenate((y1,y2))\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ym5oBSTDmyX",
    "outputId": "18793545-4632-4d84-dba7-1592920ad39c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X (2000, 2)\n",
      "Shape of y (2000,)\n"
     ]
    }
   ],
   "source": [
    "# Create and plot the data\n",
    "X, y = toy_data()\n",
    "\n",
    "# Always useful to remind oneself of the dimensions of a data set\n",
    "print(\"Shape of X {}\".format(X.shape))  \n",
    "print(\"Shape of y {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remedying class imbalance via resampling\n",
    "We introduced *resampling* as one model-agnostic way to address class imbalance. The functioning of simple under/oversampling is trivial. So let us focus on slightly more sophisticated resampling techniques. The *SMOTE* algorithm is clearly one of the best-known strategies. It is widely used in the industry and considered as a benchmark approach to new imbalanced learners in virtually any academic paper. So for let's take a deep-dive into SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE from scratch\n",
    "\n",
    "# TODO \n",
    "1. Code to build SMOTE from scratch\n",
    "  - ideally the implementation should also support categories\n",
    "  - however, it this proves too difficult, we can restrict the part of numerical variables\n",
    "2. Showcasing the from-scratch version of SMOTE with a little example\n",
    "  - You can create an imbalanced toy data set and how a classifier trained on that data performs worse than a classifier trained on the same data + SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries for imbalanced learning\n",
    "\n",
    "# TODO\n",
    "The idea in this part is to showcase library implementation of SMOTE and other resampling algorithms. I guess **imbalanced-lean** is the goto library so let's stick to that one. Refer to the documentation for a number of useful examples. \n",
    "\n",
    "For our demo, we can use the HMEQ data set and try out how different classifiers perform in conjunction with different resampling strategies supported by the library (i.e., SMOTE + others). It may be that HMEQ is a poor choice. You will have to first increase the class imbalance by removing bad cases. Afterwards, you can hopefully find a classifier that experiences problems with the data and performs better after some resampling techniques. If not, we might have to switch to another data set.\n",
    "\n",
    "It is not straightforward how to get the resampling right. For the demo described above, you can simply train/test split the data and make sure to only resample the training set while leaving the test set untouched. Afterwards, you can refer to https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/ where demos of resampling + cross-validation are available. Between, the page is very nice, as usual for Jason, so you might want to draw on the demos for the tutorial. I'm sure you already saw the page ;)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost-sensitive learning\n",
    "\n",
    "- briefly discuss similarity between class imbalance and cost-sensitive learning\n",
    "\n",
    "We have already established in the beginning of the notebook that credit scoring is a good environment to think about error costs. Therefore, the rest of the tutorial will focus on our good old HMEQ data set. \n",
    "\n",
    "- state of cost-matrix for the following exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost-minimal threshold\n",
    "We have discussed Bayesian decision theory in the lecture and arrived at an expression for the cost-optimal classification threshold. \n",
    "\n",
    "- reproduce equation from slide 37\n",
    "\n",
    "\n",
    "# TODO\n",
    "- calculate the optimal cut-off for the cost-matrix stated above (use whatever cost ratio gives nice results. Yet, false positive error must be higher than false negative errors as explained in the intro)\n",
    "- calculate error costs on the HMEQ test set of a logit model when using a default threshold and when using the optimal threshold. Supposedly the optimal threshold will give better result. Correct?\n",
    "- Next introduce a strong benchmark than the default cut-off. Use a validation set drawn from the training data or cross-validate the training data to tune the cut-off empirical. That is, empirically determine the threshold that gives the lowest error costs on the validation set and check how this classifier performs (in costs) on the test set. Does it better than the Bayes optimal threshold? \n",
    "- The logit model is expected to give well-calibrated predictions. Exercise for the students is to repeat the analysis using a tree, which will supposedly not provide calibrated predictions.\n",
    "- Maybe we can show a reliability plot at some point to show the calibration of the logit model.\n",
    "\n",
    "I leave it to you how you implement the threshold tuning. I guess a function to do it is available in sklearn. If yes, please demonstrate its use. Or else, you could use some functionality of the Yellowbrick lib (e.g., https://www.scikit-yb.org/en/latest/api/classifier/threshold.html).\n",
    "\n",
    "A manual implementation can come in addition to a demo of a sklearn or other official package, but could also be left as an exercise. Again, Jason has a potentially useful tutorial: https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost-sensitive classifier learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "In this part, I'd simply like some demos of training sklearn classifiers with weights. Linear models and trees support this, and maybe others as well. I envision a list of models that are supported and a demo using HMEQ. For example, train a classifier with and without class weights (i.e., costs) and compare which one works better. When saying 'work', I think of costs as performance measure. Actually you could also showcase other performance measures like AUC and discuss how they differ agree with costs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (adams)",
   "language": "python",
   "name": "pycharm-feb95198"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
